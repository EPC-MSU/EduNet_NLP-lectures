{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Машинный перевод</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача машинного перевода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ранее мы рассматривали в основном задачу классификации: каждому тексту сопоставляется метка класса из заданного списка.\n",
    "\n",
    "Другая важная задача обработки естественного языка — машинный перевод: последовательности на одном языке сопоставляется последовательность на другом языке, передающая тот же смысл.\n",
    "\n",
    "Рассмотрим задачу перевода с английского на французский язык:\n",
    "- Исходное предложение: I am a student.\n",
    "- Целевое предложение: Je suis étudiant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-content/L04/out/machine_translation.png\" width=\"600\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что нужно для перевода текста?\n",
    "1. Понимать отдельные слова\n",
    "2. Понимать взаимодействие между словами (синтаксис)\n",
    "3. Переводить слова (с учетом контекста)\n",
    "4. Составлять осмысленный и связный текст\n",
    "\n",
    "Для перевода текста можно разбить исходное предложение на несколько фрагментов, затем переводить его по фразам.\n",
    "\n",
    "Однако у такого подхода есть проблемы:\n",
    "- одному слову в исходном предложении (*forced*) может соответствовать несколько слов в целевом предложении (*a forcé*)\n",
    "- порядок слов может меняться (*exceptional measures* vs. *des mesures exceptionnelles*)\n",
    "\n",
    "Следовательно, возникают сложности с построением выравнивания слов (word alignment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-content/L04/out/word_alignment.png\" width=\"800\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другая возможность — прочитать все исходное предложение целиком, проанализировать его значение, а затем произвести перевод.\n",
    "\n",
    "В этом случае модель состоит из двух блоков:\n",
    "- кодировщик: строит векторное представление для исходного предложения (кодирует);\n",
    "- декодировщик: генерирует целевое предложение на основе этого векторного представления (раскодирует)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i.ibb.co/h2srmnP/encoder-decoder.png\" width=\"800\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://blog.paperspace.com/introduction-to-neural-machine-translation-with-bahdanaus-attention/\">Introduction to Neural Machine Translation</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве блоков кодировщика и декодировщика могут использоваться нейронные сети с различной архитектурой.\n",
    "1. Изначально это были рекуррентные нейронные сети, которые хорошо подходят для обработки последовательных данных;\n",
    "2. Позже была предложена архитектура Трансформер, которая лежит в основе современных языковых моделей.\n",
    "\n",
    "В сегодняшней лекции мы рассмотрим обе эти архитектуры и их применение в задаче машинного перевода."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/rnn_transformer.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://shchegrikovich.substack.com/p/rnn-vs-transformers-or-how-scalability\">RNN vs Transformers</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Рекуррентные нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Рекуррентные нейронные сети (recurrent neural networks, RNN)** — это класс нейронных сетей, которые применяются для обработки последовательных данных.\n",
    "\n",
    "Они применяются в широком перечне задач: от **распознавания речи** до **генерации подписей** к изображениям.\n",
    "\n",
    "Эти задачи объединяет необходимость работы с контекстом и извлечения информации, сформированной при обработке одной части данных, для обработки других частей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/sequence_data.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://cbare.github.io/2019-01-27/deep-learning-sequence-models.html\">Deep Learning - Sequence Models</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Рекуррентная ячейка и рекуррентный слой"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим работу рекуррентных сетей применительно к текстовым данным.\n",
    "\n",
    "Каждое слово $x_t$ в предложении последовательно обрабатывается с помощью **ячейки RNN**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/RNN_cell.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"http://vbystricky.ru/2021/05/rnn_lstm_gru_etc.html\">RNN, LSTM, GRU</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. На вход ячейки поступает входной вектор $x_t$ —  эмбеддинг слова с индексом $t$. Он имеет фиксированный размер $k$.\n",
    "\n",
    "2. Ячейка принимает еще один параметр $h_{t-1}$ — **скрытое состояние** или **память** (hidden state). Это вектор, хранящий информацию о предшествующем контексте. Он тоже имеет фиксированный размер $n$.\n",
    "\n",
    "3. Вектор $x_t$ умножается на матрицу $W^{nk}$ (размер $n \\times k$), которая содержит обучаемые веса: $W^{nk} \\cdot x_t$. Получаем новый вектор размера $n$.\n",
    "\n",
    "4. Вектор $h_{t-1}$ умножается на другую матрицу весов $W^{nn}$ (размер $n \\times n$): $ W^{nn} \\cdot h_{t-1} $. Получаем новый вектор размера $n$.\n",
    "\n",
    "5. Получившиеся векторы имеют одинаковый размер, их можно сложить: $W^{nk} \\cdot x_t + W^{nn} \\cdot h_{t-1}$.\n",
    "\n",
    "6. К получившемуся вектору применим функцию активации — гиперболический тангенс: $tanh(W^{nk} \\cdot x_t + W^{nn} \\cdot h_{t-1})$. Это и будет новым скрытым состоянием $h_t$. Оно зависит от предыдущего состояния $h_{t-1}$ и текущего элемента последовательности $x_t$.\n",
    "\n",
    "7. Рассчитанное скрытое состояние $h_t$ является представлением текущего слова $x_t$ с учетом предшествующего контекста и передается в качестве старого скрытого состояния для следующего слова $x_{t+1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обработки всей последовательности слов используется **слой RNN**, где ячейка применяется ко всем словам в цикле."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/RNN_layer.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"http://vbystricky.ru/2021/05/rnn_lstm_gru_etc.html\">RNN, LSTM, GRU</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда первый токен $x_0$ подается в ячейку, вектор памяти $h$ инициализируется нулями.\n",
    "\n",
    "Вектор памяти $h_0$ для первого токена $x_0$ передается в следующую ячейку, обрабатывающую второй токен $x_1$. Вектор $h_0$ также является контекстуализированным представлением токена $x_0$ — $y_0$.\n",
    "\n",
    "Для каждого последующего токена вектор памяти учитывает контекст из предыдущих токенов.\n",
    "\n",
    "Вектор $y_t$ является не только контекстуализированным представлением для последнего токена $x_t$, но и вектором всего предложения, т.к. содержит информацию обо всех токенах.\n",
    "\n",
    "В зависимости от типа задачи, можно использовать контекстуализированные вектора для каждого слова или только вектор всего предложения (последнего слова)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN слой в PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В PyTorch есть слой — `torch.nn.RNN` [🛠️[doc]](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html), который реализует логику, описанную выше.\n",
    "\n",
    "Также есть сущность `torch.nn.RNNCell` [🛠️[doc]](https://pytorch.org/docs/stable/generated/torch.nn.RNNCell.html), которая реализует вычисления на одном такте времени.\n",
    "\n",
    "Слой `nn.RNN` фактически является оберткой, которая вызывает `nn.RNNCell` в цикле по длине входной последовательности.\n",
    "\n",
    "Параметры слоя `nn.RNN`:\n",
    "\n",
    "* **`input_size`** — размерность $\\large x_t$, целое число.\n",
    "\n",
    "* **`hidden_size`** — размерность $\\large h_t$, целое число. Фактически это количество нейронов в рекуррентном слое.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Техническая особенность рекуррентных слоев в PyTorch: по умолчанию ожидаемые размерности входа такие:\n",
    "\n",
    "**`[длина последовательности, размер батча, размерность входа]`**\n",
    "\n",
    "Однако если при создании слоя указать `batch_first=True`, то можно подавать входные значения в более привычном формате, когда размер батча стоит на первом месте:\n",
    "\n",
    "**`[размер батча, длина последовательности, размерность входа]`**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "rnn = torch.nn.RNN(input_size=3, hidden_size=2, batch_first=True)\n",
    "\n",
    "dummy_batched_seq = torch.randn((16, 57, 3))  # batch_size, seq_len, input_size\n",
    "out, h = rnn(dummy_batched_seq)\n",
    "\n",
    "print(\"Input shape:\".ljust(20), f\"{dummy_batched_seq.shape}\")\n",
    "print(\"Out shape:\".ljust(20), f\"{out.shape}\")\n",
    "print(\"Last hidden state shape:\".ljust(20), f\"{h.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При вызове слой возвращает два объекта:\n",
    "* `out` — последовательность скрытых состояний,\n",
    "* `h` — скрытое состояние на последнем такте.\n",
    "\n",
    "Мы указали `batch_first=True`, при этом `out` сохранил последовательность размерностей, как у входа, а вот у `h` размерность батча встала на второе место."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_batch_first = h.permute(1, 0, 2)\n",
    "\n",
    "print(f\"h is last out: {(h_batch_first == out[:, -1:, :]).all().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Многослойные RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN-блоки можно объединять в слои, накладывая их друг на друга. Для этой операции в `torch.nn.RNN` есть аргумент `num_layers`, с помощью которого можно указать количество слоёв."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-content/L04/out/rnn_multiple_layers.png\" width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn((16, 6, 3))  # batch_size, seq_len, input_size\n",
    "rnn = torch.nn.RNN(input_size=3, hidden_size=2, num_layers=2, batch_first=True)\n",
    "\n",
    "out, h = rnn(dummy_input)\n",
    "\n",
    "print()\n",
    "print(\"Out:\\n\", out.shape)  # Hidden states for all elements from top layer\n",
    "print(\"h:\\n\", h.shape)  # Hidden states for last element for all layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проблемы RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теоретически, можно было бы сразу пропустить все данные через сеть и затем вычислить градиент, однако возникнут следующие проблемы:\n",
    "\n",
    " - большие последовательности не поместятся в памяти,\n",
    " - так как цепочка будет очень длинной, возникнет затухание/взрыв градиента,\n",
    " - по мере прохождения сигнала по цепи контекст затирается.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Допустим, у нас есть длинная последовательность. Если мы сразу предсказываем, то в каждый момент времени нужно распространить Loss. И все ячейки нужно обновить во время backpropogation. Все градиенты нужно посчитать. Возникают проблемы, связанные с нехваткой памяти.\n",
    "\n",
    "Есть специальные тесты для проверки, контекст какой длины использует RNN при предсказании. Если мы делаем предсказание только в последней ячейке, может оказаться, что используется, скажем, информация только о последних 10 словах предложения.\n",
    "\n",
    "Функция активации Tanh постепенно затирает контекст."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/backprop_through_time.png\" width=\"700\"><center>\n",
    "\n",
    "<center><em>Source: <a href=\"http://cs231n.stanford.edu/slides/2021/lecture_10.pdf\">CS231n: Recurrent Neural Network</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затухающий/взрывающийся градиент (Vanishing/exploding gradient) — явления затухающего и взрывающегося градиента часто встречаются в контексте RNN. И при большой длине последовательности это становится критичным. Причина в том, что зависимость величины градиента от числа слоёв экспоненциальная, поскольку веса умножаются многократно.\n",
    "\n",
    "$dL ∝ (W)^N:$\n",
    "\n",
    "$W > 1 \\rightarrow$ взрыв, $W < 1 \\rightarrow$ затухание."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet_NLP-content/L04/out/simple_rnn_backprop.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Один из путей решения проблемы — **градиентное отсечение** (Gradient truncating) — метод, который ограничивает максимально допустимое значение градиента, позволяя избежать градиентного взрыва.\n",
    "\n",
    "А от затухания градиента может помочь **пропускание** **градиента по частям**, на сколько-то шагов по времени назад или вперёд, а не через всю нейросеть. Да, градиент будет не совсем точно считаться, и мы будем терять в качестве. Но это экономит память."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/truncated_backprop.png\"  width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"http://cs231n.stanford.edu/slides/2021/lecture_10.pdf\">CS231n: Recurrent Neural Network</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM (Long Short-Term Memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычная RNN имела множество проблем, в том числе в ней очень быстро затухала информация о предыдущих элементах последовательности. Помимо этого были проблемы с затуханием/взрывом градиента.\n",
    "\n",
    "Эти проблемы были частично решены в LSTM, предложенной в [Long Short-Term Memory (Hochreiter & Schmidhuber, 1997) 🎓[article]](http://www.bioinf.jku.at/publications/older/2604.pdf).\n",
    "\n",
    "В обычной RNN-ячейке был только один путь передачи информации. На каждом шаге мы конкатенировали предыдущее скрытое состояние с текущим входом и пропускали их через линейный слой и активацией:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<html>\n",
    "<head>\n",
    "<style>\n",
    "table, th, td {\n",
    "  border: 1px solid black;\n",
    "  border-collapse: collapse;\n",
    "}\n",
    "<font size=\"2\" face=\"Times New Romans\" >\n",
    "\n",
    "\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "<table >\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "<center><img src = \"https://edunet.kea.su/repo/EduNet_NLP-content/L04/out/simple_rnn_h_state.png\" width=\"500\"></center>\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "<table >\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "$\\large h_t = \\tanh(W \\cdot [h_{t-1}, x_t])$\n",
    "\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "\n",
    "\n",
    "</table>\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При этом информация о предыдущих элементах последовательности очень быстро затухает и теряется общая информация о предложении.\n",
    "\n",
    "Структура ячейки LSTM намного сложнее. Здесь есть целых 4 линейных слоя, каждый из которых выполняет разные задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<html>\n",
    "<head>\n",
    "<style>\n",
    "table, th, td {\n",
    "  border: 1px solid black;\n",
    "  border-collapse: collapse;\n",
    "}\n",
    "<font size=\"2\" face=\"Times New Romans\" >\n",
    "\n",
    "\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "<table >\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-content/L04/out/lstm_chain.png\" width=\"500\"></center>\n",
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-content/L04/out/lstm_chain_notation.png\" width=\"700\"></center>\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "<table >\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "$\\large f_t = σ(W_f \\cdot [h_{t-1}, x_t])\\ \\ \\ \\ $\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "$$\\large \\text{forget  gate}$$\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "$\\large i_t = σ(W_i \\cdot [h_{t-1}, x_t])$\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "$$\\large \\text{input gate}$$\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "$\\large o_t = σ(W_o \\cdot [h_{t-1}, x_t])$\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "$$\\large \\text{output gate}$$\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "$\\large c^\\prime_t = \\tanh(W_c \\cdot [h_{t-1}, x_t])$\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "$$\\large \\text{candidate cell state}$$\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "$\\large c_t = f_t\\otimes c_{t-1} + i_t \\otimes c^\\prime_t$\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "$$\\large \\text{cell state}$$\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "$\\large h_t = o_t\\otimes \\tanh(c_t)$\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "$$\\large  \\text{hidden state}$$\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Главное нововведение: в LSTM добавлен путь $c$, который по задумке должен этот общий контекст сохранять."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-content/L04/out/lstm_c_state_highway.png\" width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другими словами, путь $c$ (cell state, иногда называется highway, магистраль)  помогает нейросети сохранять важную информацию, встретившуюся в какой-то момент в последовательности в прошлом, все время, пока эта информация требуется.\n",
    "\n",
    "По формулам также видно, как возросла сложность."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отличие от RNN состоит в том, что кроме $h$ возвращается еще и $c$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "lstm = nn.LSTM(input_size=3, hidden_size=2, batch_first=True)\n",
    "input = torch.randn(16, 57, 3)  # batch_size, seq_len, input_size\n",
    "out, (h, c) = lstm(input)  # h and c returned in tuple\n",
    "\n",
    "print(\"Input shape:\".ljust(15), input.shape)\n",
    "print(\"Shape of h\".ljust(15), h.shape)  # 1, batch_size, hidden_size\n",
    "print(\"Shape of c\".ljust(15), c.shape)  # 1, batch_size, hidden_size\n",
    "print(\"Output shape:\".ljust(15), out.shape)  # batch_size, seq_len, hidden_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU (Gated Recurrent Unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самая известная модификация LSTM — GRU. Она более компактна за счет сильных упрощений в сравнении со стандартной LSTM.\n",
    "\n",
    "Главные изменения: объединены forget и input gates, слиты $h_t$ и $c_t$, которые в обычной LSTM только участвовали в формировании друг друга."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<html>\n",
    "<head>\n",
    "<style>\n",
    "table, th, td {\n",
    "  border: 1px solid black;\n",
    "  border-collapse: collapse;\n",
    "}\n",
    "<font size=\"2\" face=\"Times New Romans\" >\n",
    "\n",
    "\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "<table >\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-content/L04/out/gru_basic_block.png\" width=\"500\"></center>\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "<table >\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "$\\large z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t])$\n",
    "\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "$\\large r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t])$\n",
    "\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "$\\large \\tilde h_t = \\tanh(W \\cdot [r_t \\otimes h_{t-1}, x_t])$\n",
    "\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "$\\large h_t = (1-z_t) \\otimes h_{t-1} + z_t \\otimes \\tilde h_t$\n",
    "\n",
    "</td>\n",
    "\n",
    "\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = nn.GRU(input_size=3, hidden_size=2, batch_first=True)\n",
    "input = torch.randn(16, 57, 3)  # batch_size, seq_len, input_size\n",
    "out, h = gru(input)\n",
    "\n",
    "print(\"Input shape:\".ljust(15), input.shape)\n",
    "print(\"Shape of h:\".ljust(15), h.shape)  # 1, batch_size, hidden_size\n",
    "print(\"Output shape:\".ljust(15), out.shape)  # batch_size, seq_len, hidden_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Практический опыт исследователей: иногда лучше работает GRU, иногда — LSTM. Точный рецепт успеха сказать нельзя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Типы задач"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Один к одному: на вход подается один элемент, на выходе получаем вероятность класса — классификация изображений.\n",
    "\n",
    "- Один ко многим: на вход приходит один элемент, а на выходе получаем целую последовательность — генерация текста по слову, текстовой подписи по изображению.\n",
    "\n",
    "- Многие к одному: на вход сети подается последовательность, а в качестве выхода получаем один вектор вероятностей для классов — классификация текстов (анализ тональности, определение языка).\n",
    "\n",
    "- Многие ко многим: преобразовать последовательность в последовательность\n",
    "  - количество выходов равно количеству входов — тегирование (именованные сущности, части речи).\n",
    "  - количество выходов сети не обязательно равно количеству входов — машинный перевод, суммаризация."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/RNN_tasks.png\" width=\"800\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://dotnettutorials.net/lesson/recurrent-neural-network/\">Recurrent Neural Network (RNNs)</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN для машинного перевода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Машинный перевод относится к задаче преобразования одной последовательности в другую, длина которых может быть любой и не обязательно должна совпадать: \"многие ко многим\" или sequence-to-sequence (сокращенно seq2seq).\n",
    "\n",
    "Модель для машинного перевода состоит из блока <font color=\"blue\"> кодировщика </font> и <font color=\"red\"> декодировщика</font>. Ниже представлена более подробная схема."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/seq2seq.png\" width=\"600\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://blog.paperspace.com/introduction-to-neural-machine-translation-with-bahdanaus-attention/\">Introduction to Neural Machine Translation</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <table >\n",
    "     <tr>\n",
    "       <td>\n",
    "\n",
    "Архитектура <font color=\"blue\">кодировщика</font>:\n",
    "\n",
    "- на вход первой ячейки RNN поступает вектор слова \"I\"<br>\n",
    "и нулевое скрытое состояние,\n",
    "- вектор слова \"I\" обрабатывается внутри первой ячейки RNN;\n",
    "- на вход второй ячейки поступает вектор слова \"am\"<br>и измененный вектор слова \"I\" как скрытое состояние;\n",
    "- вектор слова \"am\" обрабатывается внутри второй ячейки RNN;\n",
    "- $\\cdots$\n",
    "- вектор слова \"student\" обрабатывается внутри четвертой ячейки RNN;\n",
    "- измененный вектор слова \"student\" (= вектор предложения)<br>поступает на вход декодера как скрытое состояние.\n",
    "\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "\n",
    "Архитектура <font color=\"red\">декодировщика</font>:\n",
    "- на вход первой ячейки RNN поступает<br>вектор спецсимвола начала предложения <br>и вектор исходного предложения из энкодера как скрытое состояние;\n",
    "-  вектор спецсимвола начала предложения<br>обрабатывается внутри первой ячейки RNN;\n",
    "- на вход второй ячейки поступает вектор слова \"Je\"<br>и измененный вектор спецсимвола начала как скрытое состояние;\n",
    "- вектор слова \"Je\" обрабатывается внутри второй ячейки RNN;\n",
    "- $\\cdots$\n",
    "- вектор слова \"étudiant\" обрабатывается внутри четвертой ячейки RNN;\n",
    "- измененные векторы каждого слова<br>передаются на линейный слой, применяется softmax;\n",
    "- на выходе получаем 4 вектора, длина которых равна длине словаря, —<br>это распределение вероятностей для следующего элемента<br>при условии текущей последовательности.\n",
    "\n",
    "\n",
    "</td>\n",
    "     </tr>\n",
    "    </table>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак,\n",
    "- <font color=\"blue\">кодировщик</font> используется для получения конекстуализированного вектора исходного предложения;\n",
    "- <font color=\"red\">декодировщик</font> используется для генерации целевого предложения;\n",
    "- вектор исходного предложения используется в качестве скрытого состояния на первом шаге генерации.\n",
    "\n",
    "**Проблема Sequence-to-Sequence**: использование только конечного скрытого состояния энкодера для представления всей входной последовательности приводит к потере информации, особенно с начала последовательности;\n",
    "- по мере того, как энкодер обрабатывает входную последовательность, ожидается, что в конечном скрытом состоянии будет собрана вся необходимая информация;\n",
    "- когда последовательность становится длиннее, этому единственному состоянию труднее сохранять всю необходимую информацию из предыдущих частей последовательности.\n",
    "\n",
    "Чтобы модель могла фокусироваться на различных частях входной последовательности и сохранять больше информации на протяжении всего процесса кодирования и декодирования, был предложен механизм внимания (attention mechanism).\n",
    "\n",
    "[[paper] 🎓 Bahdanau D., Cho K., Bengio Y. (2014). Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/seq2seq_attention.png\" width=\"600\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://blog.paperspace.com/introduction-to-neural-machine-translation-with-bahdanaus-attention/\">Introduction to Neural Machine Translation</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Архитектура энкодера не меняется, все изменения касаются архитектуры декодера.\n",
    "- на вход первой ячейки RNN также поступает вектор спецсимвола начала предложения и вектор исходного предложения из энкодера как скрытое состояние;\n",
    "-  вектор спецсимвола начала предложения обрабатывается внутри первой ячейки RNN;\n",
    "- для вектора спецсимвола начала предложения и векторов каждого слова исходной последовательности считается мера сходства — это и есть веса внимания (attention weights);\n",
    "- если векторы некоторого слова целевого и исходного предложения схожи, то при переводе (генерации) данного слова декодер больше \"обращает внимания\" на него;\n",
    "- вектор каждого слова исходного предложения умножается на свой вес внимания, затем все векторы складываются, получаем контекстный вектор (context vector);\n",
    "- вектор спецсимвола начала предложения и контекстный вектор конкатенируются;\n",
    "- конкатенированный вектор передается на линейный слой, применяется softmax;\n",
    "- получаем вектор, длина которого равна длине словаря, — это распределение вероятностей для следующего элемента при условии спецсимвола начала предложения;\n",
    "- на вход второй ячейки поступает вектор слова \"Je\" и измененный вектор спецсимвола начала как скрытое состояние;\n",
    "- вектор слова \"Je\" обрабатывается внутри второй ячейки RNN;\n",
    "- считаются веса внимания для слова \"Je\" и всех слов целевого предложения;\n",
    "- $\\cdots$\n",
    "- для каждого слова получаем распределение вероятностей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/attention_equation.jpg\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://www.google.com/url?sa=i&url=https%3A%2F%2Fai.plainenglish.io%2Fintroduction-to-attention-mechanism-bahdanau-and-luong-attention-e2efd6ce22da&psig=AOvVaw1NlMR6N0XSxl6zYQwhAlYw&ust=1723824501708000&source=images&cd=vfe&opi=89978449&ved=0CAUQtaYDahcKEwiAhL3usPeHAxUAAAAAHQAAAAAQDw\">Introduction to Attention Mechanism</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Механизм внимания позволил исправить некоторые проблемы применения рекуррентных сети, однако ряд недостатков остается:\n",
    "- не способны запоминать дистантные зависимости\n",
    "- возникает взрыв и затухание градиента\n",
    "- вычисления проводятся последовательно и занимают много времени\n",
    "\n",
    "Что если убрать рекуррентность и оставить только механизм внимания?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Архитектура Трансформер"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Архитектура, в которой отсутствует рекуррентность и используется только механизм внимания, получила название Трансформер.\n",
    "\n",
    "[[paper] 🎓 Vaswani A. et al. (2017).Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "Как и в случае с рекуррентной сетью seq2seq, Трансформер состоит из блока энкодера и блока декодера.\n",
    "- Энкодер обрабатывает исходную последовательность и кодирует ее.\n",
    "- Декодер обрабатывает целевую последовательность с учетом информации из энкодера. Выход из декодера является предсказанием модели.\n",
    "\n",
    "Блок энкодера состоит из 6 энкодеров, расположенных друг за другом. Блок декодера – это стек декодеров, представленных в том же количестве."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/transformer.jpg\" width=\"860\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все энкодеры идентичны по структуре, хотя и имеют разные веса. Каждый можно разделить на два подслоя.\n",
    "- Входная последовательность, поступающая в энкодер, сначала проходит через слой внутреннего внимания (self-attention), помогающий энкодеру посмотреть на другие слова во входном предложении во время кодирования конкретного слова.\n",
    "- Выход слоя внутреннего внимания отправляется в нейронную сеть прямого распространения (feed-forward neural network).\n",
    "\n",
    "Декодер также содержит эти два слоя, но между ними есть слой внимания, который помогает декодеру фокусироваться на релевантных частях исходного предложения (аналогично механизму внимания в моделях seq2seq)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/enc_dec_tr.png\" width=\"650\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как и в случае других алгоритмов, каждое слово входной и целевой последовательности преобразуется в вектор.\n",
    "\n",
    "После того как слова входящего предложения преобразовались в эмбеддинги, каждый из них по отдельности проходит через слои энкодера. Одна из основных особенностей Трансформера состоит в том, что каждое слово идет по своей собственной траектории."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/encoder.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы попробуем создать и обучить модель архитектуры Трансформер, основываясь на тьюториале:\n",
    "\n",
    "[[doc] 🛠️ The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Механизм внутреннего внимания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть мы хотим перевести предложение: *The animal didn't cross the street because it was too tired*. Местоимение *it* может относиться к улице (*street*) или к животному (*animal*). Когда модель обрабатывает слово *it*, слой внутреннего внимания помогает понять, что *it* относится к *animal*.\n",
    "\n",
    "По мере того как модель обрабатывает каждое слово входной последовательности, внутреннее внимание позволяет \"взглянуть\" на другие слова и лучше закодировать данное слово. Механизм внутреннего внимания – это метод, который Трансформер использует, чтобы смоделировать \"понимание\" других релевантных слов при обработке конкретного слова.\n",
    "\n",
    "Во время кодирования *it* в энкодере №5 часть механизма внимания фокусируется на *The animal* и использует фрагмент его представления для кодирования *it*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/self_attention.png\" width=\"400\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### На примере векторов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При вычислении внутреннего внимания используются: вектор запроса $query$, вектор ключа $key$ и вектор значения $value$. Они создаются с помощью умножения эмбеддинга слова последовательности на три матрицы весов (линейных слоя) $W^Q, W^K, W^V$. Размер новых векторов – 64, размер исходных векторов – 512.\n",
    "\n",
    "$q_i=x_iW^Q$\n",
    "\n",
    "$k_i=x_iW^K$\n",
    "\n",
    "$v_i=x_iW^V$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/vector_attention.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее вычисляется коэффициент внутреннего внимания $score$ для $i$-го слова по отношению к каждому слову в предложении. Коэффициент определяет, насколько нужно сфокусироваться на других частях предложения во время кодирования слова в $i$-й позиции. Коэффициент подсчитывается с помощью скалярного произведения вектора запроса $q$ $i$-го слова и вектора ключа $k$ каждого слова.\n",
    "\n",
    "$score_{ij}=q_i \\cdot k_j$\n",
    "\n",
    "📌 Какие векторы нужно перемножить, чтобы посчитать коэффициент внимания слова *Thinking* по отношению к слову *Machines*? По отношению к самому себе?\n",
    "\n",
    "На следующем шаге коэффициенты делятся на квадратный корень из $d_k$ – размера векторов ключа $k$. К получившимся значениям применяется функция активации softmax, чтобы коэффициенты в сумме давали 1. Полученный софтмакс-коэффициент определяет, в какой мере каждое из слов предложения \"фокусируется\" на другом слове.\n",
    "\n",
    "$softmax.score_{ij}=softmax(\\frac{score_i}{\\sqrt d_k})$\n",
    "\n",
    "После этого каждый вектор значения $v$ умножается на софтмакс-коэффициент, получаем взвешенные векторы. Идея в том, что нужно сохранять без изменений значения слов, на которых мы фокусируемся, и отвести на второй план нерелевантные слова (умножив их на небольшие значения, например, 0.001). Затем  взвешенные векторы складываются. Результат (взвешенная сумма) представляет собой выход слоя внутреннего внимания для $i$-го слова.\n",
    "\n",
    "$sum_i=\\sum_{j=1}^nv_j \\cdot softmax.score_{ij}$\n",
    "\n",
    "Полученный вектор передается дальше в нейронную сеть прямого распространения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/attention_example.png\" width=\"600\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### На примере матриц"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично, матрицы запроса $Q$, ключа $K$ и значения $V$ вычисляются с помощью умножения эмбеддингов матрицы $X$ на матрицы весов (линейные слои) $W^Q, W^K, W^V$. Каждая строка в матрице $X$ соответствует слову в предложении."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/matrix_attention.png\" width=\"450\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Последующие этапы для вычисления выхода слоя внутреннего внимания могут быть отражены в одной формуле."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/attention_score.png\" width=\"600\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию `attention` для подсчета внутреннего внимания.\n",
    "\n",
    "$Attention(Q, K, V) = softmax\\large(\\frac{QK^T}{\\sqrt{d_k}})V$\n",
    "\n",
    "Для умножения матриц $Q, K, V$ используется метод `torch.matmul()`.\n",
    "\n",
    "Транспонирование осуществляется с помощью метода `.transpose`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"\"\"\n",
    "    Compute 'Scaled Dot Product Attention'\n",
    "    \"\"\"\n",
    "    d_k = query.size(-1)\n",
    "    # compute the attention scores by using torch.matmul\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    # compute the result as the values weighted by attention probabilities (again, using torch.matmul)\n",
    "    result = torch.matmul(p_attn, value)\n",
    "    return result, p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, attentions = attention(\n",
    "    torch.tensor([[0, 0], [0, 1], [1, 1]], dtype=torch.float),\n",
    "    torch.tensor([[100, 0], [0, 100], [0, 0]], dtype=torch.float),\n",
    "    torch.tensor([[1, 0], [0, 1], [0, 0]], dtype=torch.float),\n",
    ")\n",
    "print(results)\n",
    "print(attentions)\n",
    "\n",
    "assert np.allclose(results[0].numpy(), [1/3, 1/3])  # the first query attends to all keys equally\n",
    "assert np.allclose(results[1].numpy(), [0, 1])      # the second query attends only to the second key\n",
    "assert np.allclose(results[2].numpy(), [1/2, 1/2])  # the third query attends to the first and second key equally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Множественное внимание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В Трансформере механизм внутреннего внимания совершенствуется с помощью добавления множественного внимания (multi-headed attention). Это повышает способность модели фокусироваться на разных словах. В случае множественного внимания мы располагаем отдельными матрицами весов $W^Q, W^K, W^V$  для каждой \"головы\", что в результате дает разные матрицы $Q, K, V$. Как и ранее, матрица $X$ умножается на веса $W^Q, W^K, W^V$ для получения матриц $Q, K, V$.\n",
    "\n",
    "Трансформер использует 8 \"голов\" внимания, так что в итоге у нас получается 8 наборов для каждого энкодера и декодера. Сделав те же вычисления внутреннего внимания 8 раз с разными матрицами весов, в результате получим 8 разных матриц."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/multihead_attention.png\" width=\"600\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако слой сети прямого распространения не ожидает, что к нему поступит 8 матриц – он ждет всего одну, в которую нам и необходимо сжать полученные матрицы. Чтобы это сделать, можно конкатенировать и затем умножить их на дополнительные веса матрицы $W^O$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/concatenate.png\" width=\"800\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем отобразить все \"головы\" внимания на одной картинке.\n",
    "\n",
    "📌 Какие головы больше фокусируются на каких словах?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/multihead_viz.png\" width=\"400\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для создание множественного внимания необходим класс `MultiHeadedAttention`.\n",
    "\n",
    "Каждая голова внимания независимо вычисляет внимание (скалярное произведение) для своих матриц $Q,K,V$. Следовательно, каждая голова может \"обращать внимание\" на разные части предложения.\n",
    "\n",
    "$MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch\n",
    "        x, self.attn = attention(query, key, value, mask=mask,\n",
    "                                 dropout=self.dropout)\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear\n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кодирование позиции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из-за того, что мы избавили от рекуррентной нейронной сети, информация о порядке слов перестала учитываться моделью. В классе `MultiHeadAttention` операции применяются к размерности 2 (признаки слов), но не к размерности 1 (слова последовательности).\n",
    "\n",
    "Для того чтобы модель понимала порядок слов, вводятся векторы кодирования позиции (positional encoding). Они суммируются с векторами каждого слова."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/positional_encoding.png\" width=\"850\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Информация о позиции кодируется с помощью тригонометрических функций: синуса и косинуса.\n",
    "\n",
    "Пусть есть входная последовательность длины $L$, нужно закодировать позицию некоторого элемента $pos$.\n",
    "\n",
    "$pos$ — индекс слова в исходной последовательности, $0 \\leq pos < L$\n",
    "\n",
    "$d$ — размер эмеддингов входного слоя\n",
    "\n",
    "$i$ — номер координаты эмбеддинга входного слоя, $0 \\leq i < \\large \\frac{d}{2}$\n",
    "\n",
    "$p_0$ — первое слово последовательности, $d$ — длина вектора, $i$ — координаты вектора."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/pe1.png\" width=\"500\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://www.youtube.com/watch?v=dichIcUZfOw\">Visual Guide to Transformer Neural Networks</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Слова с разным номером позиции будут иметь разные значения значения на оси $y$. Однако для некоторых позиций ($p_0$ и $p_6$) значения совпадают, поскольку синус — периодическая функция."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/pe2.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://www.youtube.com/watch?v=dichIcUZfOw\">Visual Guide to Transformer Neural Networks</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получить разные значения можно благодаря параметру $i$. При изменении значения $i$ меняется частота, что приводит к получению разных значений для $p_0$ и $p_6$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/pe3.png\" width=\"800\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://www.youtube.com/watch?v=dichIcUZfOw\">Visual Guide to Transformer Neural Networks</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим класс `PositionalEncoding` для кодирования позиции.\n",
    "\n",
    "$PE_{(pos, 2i)}=sin(pos/10000^{2i/d_{model}})$\n",
    "\n",
    "$PE_{(pos, 2i+1)}=cos(pos/10000^{2i/d_{model}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "pe = PositionalEncoding(20, 0)\n",
    "y = pe.forward(Variable(torch.zeros(1, 100, 20)))\n",
    "plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())\n",
    "plt.legend([\"dim %d\"%p for p in [4,5,6,7]])\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Слои Embedding и преобразование Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично другим моделям преобразования последовательностей, мы используем обученные эмбеддинги для преобразования входных и выходных токенов в векторы размерности $d_{\\text{model}}$. Мы также используем линейное преобразование и функцию softmax для преобразования выходных данных декодера в прогнозируемые вероятности следующего токена. В нашей модели мы используем одну и ту же весовую матрицу между слоями эмбеддингов и линейным преобразованием, выполненным до softmax. В слоях эмбеддингов мы умножаем эти веса на $\\sqrt{d_{\\text{model}}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cеть прямого распространения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "За слоем множественного внимания в энкодере и декодере следует сеть прямого распространения. Она состоит из двух линейных слоев и функции активации ReLU между ними. Размер входных и выходных данных — 512, промежуточный размер — 2048 (в 4 раза больше)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Энкодер"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можем перейти к реализации блока энкодера. Он состоит из стека $N=6$ идентичных слоев."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/enc_details.png\" width=\"400\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый подслой (внутреннее внимание, полносвязный) включает остаточное соединение (residual connection), за которым следует этап нормализации слоя ([[doc] 🛠️ layer normalization](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.htm)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/connection_normalization.png\" width=\"500\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эмбеддинг каждого элемента последовательности дублируется: одна копия поступает на вход слоя (внимания или сети прямого распространения), другая копия не меняется и прибавляется к выходу слоя. Это позволяет избежать затухания градиента.\n",
    "\n",
    "После применяется нормализация. Веса могут принимать значения в разном диапазоне, но скорость обучение одинаковая. Из-за этого возникает сложность с обновлением весов.\n",
    "\n",
    "Нормализация применяется по предложениям: применяется к первому признаку каждого слова в первом предложении, затем ко второму признаку каждого слова в первом предожении и т.д.\n",
    "\n",
    "Среднее: $\\mu_i =\\frac{1}{m}\\sum_{j=1}^mx_{ij}$\n",
    "\n",
    "Стандартное отклонение: $\\sigma_i = \\frac{1}{m}\\sum_{j=1}^m(x_{ij}-\\mu_i)$\n",
    "\n",
    "Нормализованное значение: $\\hat x_{ij} = \\frac {x_{ij}-\\mu_i}{\\sigma_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-content/L04/out/layer_normalization.png\" width=\"400\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы используем остаточное соединение вокруг каждого из двух подслоев, далее следует нормализация слоя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Return a2 * x_normalized + b2,\n",
    "        where x_normalized is calculated by subtracting row-wise means from x and dividing the result by row-wise standard deviation + eps.\n",
    "        standard deviation is calculated with Bessel's correction (the default in Pytorch)\n",
    "        \"\"\"\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln = LayerNorm(2, eps=0)\n",
    "with torch.no_grad():\n",
    "    result = ln(torch.tensor([[0.0, 1], [100, 101], [100, 200]])).numpy()\n",
    "\n",
    "# becasue of Bessel's correction, standard deviation is pulled to 0. Here we un-pull it back.\n",
    "print(result)\n",
    "result_unnormalized = result / np.sqrt(0.5)\n",
    "assert (result_unnormalized[:, 0] == -1).all()\n",
    "assert (result_unnormalized[:, 1] == 1).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть результат каждого подслоя — $\\mathrm{LayerNorm}(x + \\mathrm{Sublayer}(x))$, где $\\mathrm{Sublayer}(x)$ — это функция, реализуемая самим подслоем. Мы применяем dropout к выходным данным каждого подслоя, прежде чем они складываются и нормализуются.\n",
    "\n",
    "Чтобы упростить эти остаточное соединение, все подслои в модели, а также слой эмбеддингов выдают выходные данные с размерностью $d_{\\text{model}}=512$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый слой состоит из двух подслоев. Первый представляет собой механизм множественного внимания, а второй — полносвязную сеть прямого распространения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Декодер"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Блок декодера устроен очень похоже, но есть несколько отличий.\n",
    "- Декодер принимает два аргумента: целевое предложение и выход энкодера.\n",
    "- Используется два слоя множественного внимания\n",
    "  - маскированное множественное внимания для обработки целевого предложения\n",
    "  - множественное внимание между энкодером и декодером для сравнения целевого и исходного предложения\n",
    "- Второй слой принимает в качестве матриц $K$ и $V$ использует выход энкодера.\n",
    "\n",
    "Декодер выполняет задачу языкового моделирования и предсказывает вероятность следующего слова. Следовательно, при обработке целевой последовательности он не может \"заглядывать\" вперед и анализировать контекст, который еще не был сгенерирован. Поэтому используется маскирование всех позиций после текущей, их векторы заполняются значениями `-inf`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/enc_dec_details.png\" width=\"800\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Декодер также состоит из стека из $N=6$ идентичных слоев."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В дополнение к двум подслоям на каждом слое энкодера, декодер содержит третий подслой, который применяет механизм множественного внимания к выходным данным энкодера. Как и в случае с энкодером, мы используем остаточные соединения вокруг каждого из подслоев с последующей нормализации слоя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"Follow Figure 1 (right) for connections.\"\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы также модифицируем подслой внутреннего внимания в стеке декодера, добавляя маскирование. Это необходимо, чтобы не учитывать токены в следующих позициях при анализе текущей и предыдущей позиций. Маскирование гарантирует, что предсказания для позиции $i$ могут зависеть только от выходных данных на позициях, меньших, чем $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Маска внимания показывает позицию (строка), на которую разрешено смотреть каждому слову (столбец). Слова блокируются для того, чтобы во время обучения можно было обратить внимание на будущие слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(subsequent_mask(20)[0])\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Трансформер"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим функцию от гиперпараметров для построения полной модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/transformer_full.png\" width=\"1000\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html\">Sequence to Sequence and Attention</a></em></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture.\n",
    "    Base for this and many other models.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        return self.decode(self.encode(src, src_mask), src_mask,\n",
    "                            tgt, tgt_mask)\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "\n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    \"Construct a model from hyperparameters\"\n",
    "    c = copy.deepcopy  # use it for attn, ffn, and position in the model layers\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    # insert correct arguments into the EncoderDecoder constructor.\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab)\n",
    "    )\n",
    "\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small example model.\n",
    "tmp_model = make_model(10, 30, 2)\n",
    "assert sum(p.numel() for p in tmp_model.parameters()) == 14750750"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.numel() for p in tmp_model.encoder.parameters()) + sum(p.numel() for p in tmp_model.decoder.parameters()) + sum(p.numel() for p in tmp_model.tgt_embed.parameters()) + sum(p.numel() for p in tmp_model.src_embed.parameters()) + sum(p.numel() for p in tmp_model.generator.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом разделе описывается режим обучения для нашей модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Деление на батчи и маскирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вначале определим формат батча, который содержит исходное и целевое предложения для обучения, а также создание масок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"Object for holding a batch of data with mask during training.\"\n",
    "    def __init__(self, src, trg=None, pad=0):\n",
    "        self.src = src\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        if trg is not None:\n",
    "            self.trg = trg[:, :-1]\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            self.trg_mask = \\\n",
    "                self.make_std_mask(self.trg, pad)\n",
    "            self.ntokens = (self.trg_y != pad).data.sum()\n",
    "\n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & Variable(\n",
    "            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "        return tgt_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим батчи с помощью текстовой функции torch, которая гарантирует, что размер нашего батча после паддинга до максимального размера батча не превысит порогового значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global max_src_in_batch, max_tgt_in_batch\n",
    "def batch_size_fn(new, count, sofar):\n",
    "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
    "    global max_src_in_batch, max_tgt_in_batch\n",
    "    if count == 1:\n",
    "        max_src_in_batch = 0\n",
    "        max_tgt_in_batch = 0\n",
    "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
    "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
    "    src_elements = count * max_src_in_batch\n",
    "    tgt_elements = count * max_tgt_in_batch\n",
    "    return max(src_elements, tgt_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Цикл обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем создадим функцию для обучения, подсчета значения функции потерь и обновления параметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def run_epoch(data_iter, model, loss_compute):\n",
    "    \"Standard Training and Logging Function\"\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        out = model.forward(batch.src, batch.trg,\n",
    "                            batch.src_mask, batch.trg_mask)\n",
    "        loss = loss_compute(out, batch.trg_y, batch.ntokens)\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "        if i % 50 == 1:\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
    "                    (i, loss / batch.ntokens, tokens / elapsed))\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "    return total_loss / total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оптимизатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать оптимизатор `Adam` [🛠️[doc]](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) с параметрами $\\beta_1=0.9$, $\\beta_2=0.98$ и $\\epsilon=10^{-9}$.  Скорость обучения будет варьироваться в соответствии с формулой:\n",
    "$$\n",
    "lrate = d_{\\text{model}}^{-0.5} \\cdot\n",
    "  \\min({step\\_num}^{-0.5},\n",
    "    {step\\_num} \\cdot {warmup\\_steps}^{-1.5})\n",
    "$$\n",
    "\n",
    "Это соответствует линейному увеличению скорости обучения на первых этапах $warmup\\_steps$ и последующему ее уменьшению пропорционально обратному квадратному корню из номера шага. Будем использовать значение $warmup\\_steps=4000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "\n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "\n",
    "def get_std_opt(model):\n",
    "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
    "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример графиков для различных размеров модели и гиперпараметров оптимизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Three settings of the lrate hyperparameters.\n",
    "opts = [NoamOpt(512, 1, 4000, None),\n",
    "        NoamOpt(512, 1, 8000, None),\n",
    "        NoamOpt(256, 1, 4000, None)]\n",
    "plt.plot(np.arange(1, 20000), [[opt.rate(i) for opt in opts] for i in range(1, 20000)])\n",
    "plt.legend([\"512:4000\", \"512:8000\", \"256:4000\"])\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Регуляризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сглаживание меток — это метод, используемый для регуляризации процесса обучения нейронной сети. Он помогает избежать переобучения за счет уменьшения зависимости модели от правильных меток во время процесса обучения. Это работает за счет повышения уверенности модели в «неправильных» ярлыках.\n",
    "\n",
    "Во время обучения будем использовать сглаживание значений меток с параметром $\\varepsilon_{ls}=0.1$. Вместо one-hot распределения по одной целевой группе мы создаем распределение, в котором наибольшую вероятность имеет правильное слово, а остальные значения вероятности распределяются по всему словарю."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "q_i =\n",
    "\\left\\{\n",
    "    \\begin {aligned}\n",
    "         & 1 - \\varepsilon \\quad & \\text{if } i=y, \\\\\n",
    "         & \\varepsilon/(K-1) \\quad & \\text{otherwise}                  \n",
    "    \\end{aligned}\n",
    "\\right.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/label_smoothing.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://arxiv.org/abs/2011.12562\">Delving Deep into Label Smoothing</a></em></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False)\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
