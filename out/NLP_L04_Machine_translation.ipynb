{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Машинный перевод</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача машинного перевода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ранее мы рассматривали в основном задачу классификации: каждому тексту сопоставляется метка класса из заданного списка.\n",
    "\n",
    "Другая важная задача обработки естественного языка — машинный перевод: последовательности на одном языке сопоставляется последовательность на другом языке, передающая тот же смысл.\n",
    "\n",
    "Рассмотрим задачу перевода с английского на французский язык:\n",
    "- Исходное предложение: I am a student.\n",
    "- Целевое предложение: Je suis étudiant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://i.postimg.cc/bwXzwx5c/machine-translation.jpg\" width=\"600\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что нужно для перевода текста?\n",
    "1. Понимать отдельные слова\n",
    "2. Понимать взаимодействие между словами (синтаксис)\n",
    "3. Переводить слова (с учетом контекста)\n",
    "4. Составлять осмысленный и связный текст\n",
    "\n",
    "Для перевода текста можно разбить исходное предложение на несколько фрагментов, затем переводить его по фразам.\n",
    "\n",
    "Однако у такого подхода есть проблемы:\n",
    "- одному слову в исходном предложении (*forced*) может соответствовать несколько слов в целевом предложении (*a forcé*)\n",
    "- порядок слов может меняться (*exceptional measures* vs. *des mesures exceptionnelles*)\n",
    "\n",
    "Следовательно, возникают сложности с построением выравнивания слов (word alignment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://i.postimg.cc/SNK4dyXs/word-alignment.png\" width=\"300\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другая возможность — прочитать все исходное предложение целиком, проанализировать его значение, а затем произвести перевод.\n",
    "\n",
    "В этом случае модель состоит из двух блоков:\n",
    "- кодировщик: строит векторное представление для исходного предложения (кодирует);\n",
    "- декодировщик: генерирует целевое предложение на основе этого векторного представления (раскодирует)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i.ibb.co/h2srmnP/encoder-decoder.png\" width=\"800\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве блоков кодировщика и декодировщика могут использоваться нейронные сети с различной архитектурой.\n",
    "1. Изначально это были рекуррентные нейронные сети, которые хорошо подходят для обработки последовательных данных;\n",
    "2. Позже была предложена архитектура Трансформер, которая лежит в основе современных языковых моделей.\n",
    "\n",
    "В сегодняшней лекции мы рассмотрим обе эти архитектуры и их применение в задаче машинного перевода."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/rnn_transformer.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://shchegrikovich.substack.com/p/rnn-vs-transformers-or-how-scalability\">RNN vs Transformers</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Рекуррентные нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Рекуррентные нейронные сети (recurrent neural networks, RNN)** — это класс нейронных сетей, которые применяются для обработки последовательных данных.\n",
    "\n",
    "Они применяются в широком перечне задач: от **распознавания речи** до **генерации подписей** к изображениям.\n",
    "\n",
    "Эти задачи объединяет необходимость работы с контекстом и извлечения информации, сформированной при обработке одной части данных, для обработки других частей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/sequence_data.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://cbare.github.io/2019-01-27/deep-learning-sequence-models.html\">Deep Learning - Sequence Models</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Рекуррентная ячейка и рекуррентный слой"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим работу рекуррентных сетей применительно к текстовым данным.\n",
    "\n",
    "Каждое слово $x_t$ в предложении последовательно обрабатывается с помощью **ячейки RNN**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/RNN_cell.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"http://vbystricky.ru/2021/05/rnn_lstm_gru_etc.html\">RNN, LSTM, GRU</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. На вход ячейки поступает входной вектор $x_t$ —  эмбеддинг слова с индексом $t$. Он имеет фиксированный размер $k$.\n",
    "\n",
    "2. Ячейка принимает еще один параметр $h_{t-1}$ — **скрытое состояние** или **память** (hidden state). Это вектор, хранящий информацию о предшествующем контексте. Он тоже имеет фиксированный размер $n$.\n",
    "\n",
    "3. Вектор $x_t$ умножается на матрицу $W^{nk}$ (размер $n \\times k$), которая содержит обучаемые веса: $W^{nk} \\cdot x_t$. Получаем новый вектор размера $n$.\n",
    "\n",
    "4. Вектор $h_{t-1}$ умножается на другую матрицу весов $W^{nn}$ (размер $n \\times n$): $ W^{nn} \\cdot h_{t-1} $. Получаем новый вектор размера $n$.\n",
    "\n",
    "5. Получившиеся векторы имеют одинаковый размер, их можно сложить: $W^{nk} \\cdot x_t + W^{nn} \\cdot h_{t-1}$.\n",
    "\n",
    "6. К получившемуся вектору применим функцию активации — гиперболический тангенс: $tanh(W^{nk} \\cdot x_t + W^{nn} \\cdot h_{t-1})$. Это и будет новым скрытым состоянием $h_t$. Оно зависит от предыдущего состояния $h_{t-1}$ и текущего элемента последовательности $x_t$.\n",
    "\n",
    "7. Рассчитанное скрытое состояние $h_t$ является представлением текущего слова $x_t$ с учетом предшествующего контекста и передается в качестве старого скрытого состояния для следующего слова $x_{t+1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обработки всей последовательности слов используется **слой RNN**, где ячейка применяется ко всем словам в цикле."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/RNN_layer.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"http://vbystricky.ru/2021/05/rnn_lstm_gru_etc.html\">RNN, LSTM, GRU</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда первый токен $x_0$ подается в ячейку, вектор памяти $h$ инициализируется нулями.\n",
    "\n",
    "Вектор памяти $h_0$ для первого токена $x_0$ передается в следующую ячейку, обрабатывающую второй токен $x_1$. Вектор $h_0$ также является контекстуализированным представлением токена $x_0$ — $y_0$.\n",
    "\n",
    "Для каждого последующего токена вектор памяти учитывает контекст из предыдущих токенов.\n",
    "\n",
    "Вектор $y_t$ является не только контекстуализированным представлением для последнего токена $x_t$, но и вектором всего предложения, т.к. содержит информацию обо всех токенах.\n",
    "\n",
    "В зависимости от типа задачи, можно использовать контекстуализированные вектора для каждого слова или только вектор всего предложения (последнего слова)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN слой в PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В PyTorch есть слой — `torch.nn.RNN` [🛠️[doc]](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html), который реализует логику, описанную выше.\n",
    "\n",
    "Также есть сущность `torch.nn.RNNCell` [🛠️[doc]](https://pytorch.org/docs/stable/generated/torch.nn.RNNCell.html), которая реализует вычисления на одном такте времени.\n",
    "\n",
    "Слой `nn.RNN` фактически является оберткой, которая вызывает `nn.RNNCell` в цикле по длине входной последовательности.\n",
    "\n",
    "Параметры слоя `nn.RNN`:\n",
    "\n",
    "* **`input_size`** — размерность $\\large x_t$, целое число.\n",
    "\n",
    "* **`hidden_size`** — размерность $\\large h_t$, целое число. Фактически это количество нейронов в рекуррентном слое.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Техническая особенность рекуррентных слоев в PyTorch: по умолчанию ожидаемые размерности входа такие:\n",
    "\n",
    "**`[длина последовательности, размер батча, размерность входа]`**\n",
    "\n",
    "Однако если при создании слоя указать `batch_first=True`, то можно подавать входные значения в более привычном формате, когда размер батча стоит на первом месте:\n",
    "\n",
    "**`[размер батча, длина последовательности, размерность входа]`**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "rnn = torch.nn.RNN(input_size=3, hidden_size=2, batch_first=True)\n",
    "\n",
    "dummy_batched_seq = torch.randn((16, 57, 3))  # batch_size, seq_len, input_size\n",
    "out, h = rnn(dummy_batched_seq)\n",
    "\n",
    "print(\"Input shape:\".ljust(20), f\"{dummy_batched_seq.shape}\")\n",
    "print(\"Out shape:\".ljust(20), f\"{out.shape}\")\n",
    "print(\"Last hidden state shape:\".ljust(20), f\"{h.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При вызове слой возвращает два объекта:\n",
    "* `out` — последовательность скрытых состояний,\n",
    "* `h` — скрытое состояние на последнем такте.\n",
    "\n",
    "Мы указали `batch_first=True`, при этом `out` сохранил последовательность размерностей, как у входа, а вот у `h` размерность батча встала на второе место."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_batch_first = h.permute(1, 0, 2)\n",
    "\n",
    "print(f\"h is last out: {(h_batch_first == out[:, -1:, :]).all().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Многослойные RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN-блоки можно объединять в слои, накладывая их друг на друга. Для этой операции в `torch.nn.RNN` есть аргумент `num_layers`, с помощью которого можно указать количество слоёв."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-content/L04/out/rnn_multiple_layers.png\" width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn((16, 6, 3))  # batch_size, seq_len, input_size\n",
    "rnn = torch.nn.RNN(input_size=3, hidden_size=2, num_layers=2, batch_first=True)\n",
    "\n",
    "out, h = rnn(dummy_input)\n",
    "\n",
    "print()\n",
    "print(\"Out:\\n\", out.shape)  # Hidden states for all elements from top layer\n",
    "print(\"h:\\n\", h.shape)  # Hidden states for last element for all layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проблемы RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теоретически, можно было бы сразу пропустить все данные через сеть и затем вычислить градиент, однако возникнут следующие проблемы:\n",
    "\n",
    " - большие последовательности не поместятся в памяти,\n",
    " - так как цепочка будет очень длинной, возникнет затухание/взрыв градиента,\n",
    " - по мере прохождения сигнала по цепи контекст затирается.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Допустим, у нас есть длинная последовательность. Если мы сразу предсказываем, то в каждый момент времени нужно распространить Loss. И все ячейки нужно обновить во время backpropogation. Все градиенты нужно посчитать. Возникают проблемы, связанные с нехваткой памяти.\n",
    "\n",
    "Есть специальные тесты для проверки, контекст какой длины использует RNN при предсказании. Если мы делаем предсказание только в последней ячейке, может оказаться, что используется, скажем, информация только о последних 10 словах предложения.\n",
    "\n",
    "Функция активации Tanh постепенно затирает контекст."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/backprop_through_time.png\" width=\"700\"><center>\n",
    "\n",
    "<center><em>Source: <a href=\"http://cs231n.stanford.edu/slides/2021/lecture_10.pdf\">CS231n: Recurrent Neural Network</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затухающий/взрывающийся градиент (Vanishing/exploding gradient) — явления затухающего и взрывающегося градиента часто встречаются в контексте RNN. И при большой длине последовательности это становится критичным. Причина в том, что зависимость величины градиента от числа слоёв экспоненциальная, поскольку веса умножаются многократно.\n",
    "\n",
    "$dL ∝ (W)^N:$\n",
    "\n",
    "$W > 1 \\rightarrow$ взрыв, $W < 1 \\rightarrow$ затухание."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet_NLP-content/L04/out/simple_rnn_backprop.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Один из путей решения проблемы — **градиентное отсечение** (Gradient truncating) — метод, который ограничивает максимально допустимое значение градиента, позволяя избежать градиентного взрыва.\n",
    "\n",
    "А от затухания градиента может помочь **пропускание** **градиента по частям**, на сколько-то шагов по времени назад или вперёд, а не через всю нейросеть. Да, градиент будет не совсем точно считаться, и мы будем терять в качестве. Но это экономит память."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/truncated_backprop.png\"  width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"http://cs231n.stanford.edu/slides/2021/lecture_10.pdf\">CS231n: Recurrent Neural Network</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM (Long Short-Term Memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычная RNN имела множество проблем, в том числе в ней очень быстро затухала информация о предыдущих элементах последовательности. Помимо этого были проблемы с затуханием/взрывом градиента.\n",
    "\n",
    "Эти проблемы были частично решены в LSTM, предложенной в [Long Short-Term Memory (Hochreiter & Schmidhuber, 1997) 🎓[article]](http://www.bioinf.jku.at/publications/older/2604.pdf).\n",
    "\n",
    "В обычной RNN-ячейке был только один путь передачи информации. На каждом шаге мы конкатенировали предыдущее скрытое состояние с текущим входом и пропускали их через линейный слой и активацией:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<html>\n",
    "<head>\n",
    "<style>\n",
    "table, th, td {\n",
    "  border: 1px solid black;\n",
    "  border-collapse: collapse;\n",
    "}\n",
    "<font size=\"2\" face=\"Times New Romans\" >\n",
    "\n",
    "\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "<table >\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "<center><img src = \"https://edunet.kea.su/repo/EduNet_NLP-content/L04/out/simple_rnn_h_state.png\" width=\"500\"></center>\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "<table >\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "$\\large h_t = \\tanh(W \\cdot [h_{t-1}, x_t])$\n",
    "\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "\n",
    "\n",
    "</table>\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При этом информация о предыдущих элементах последовательности очень быстро затухает и теряется общая информация о предложении.\n",
    "\n",
    "Структура ячейки LSTM намного сложнее. Здесь есть целых 4 линейных слоя, каждый из которых выполняет разные задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<html>\n",
    "<head>\n",
    "<style>\n",
    "table, th, td {\n",
    "  border: 1px solid black;\n",
    "  border-collapse: collapse;\n",
    "}\n",
    "<font size=\"2\" face=\"Times New Romans\" >\n",
    "\n",
    "\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "<table >\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-content/L04/out/lstm_chain.png\" width=\"500\"></center>\n",
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-content/L04/out/lstm_chain_notation.png\" width=\"700\"></center>\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "<table >\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "$\\large f_t = σ(W_f \\cdot [h_{t-1}, x_t])\\ \\ \\ \\ $\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "$$\\large \\text{forget  gate}$$\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "$\\large i_t = σ(W_i \\cdot [h_{t-1}, x_t])$\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "$$\\large \\text{input gate}$$\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "$\\large o_t = σ(W_o \\cdot [h_{t-1}, x_t])$\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "$$\\large \\text{output gate}$$\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "$\\large c^\\prime_t = \\tanh(W_c \\cdot [h_{t-1}, x_t])$\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "$$\\large \\text{candidate cell state}$$\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "$\\large c_t = f_t\\otimes c_{t-1} + i_t \\otimes c^\\prime_t$\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "$$\\large \\text{cell state}$$\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "$\\large h_t = o_t\\otimes \\tanh(c_t)$\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "$$\\large  \\text{hidden state}$$\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Главное нововведение: в LSTM добавлен путь $c$, который по задумке должен этот общий контекст сохранять."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-content/L04/out/lstm_c_state_highway.png\" width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другими словами, путь $c$ (cell state, иногда называется highway, магистраль)  помогает нейросети сохранять важную информацию, встретившуюся в какой-то момент в последовательности в прошлом, все время, пока эта информация требуется.\n",
    "\n",
    "По формулам также видно, как возросла сложность."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отличие от RNN состоит в том, что кроме $h$ возвращается еще и $c$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "lstm = nn.LSTM(input_size=3, hidden_size=2, batch_first=True)\n",
    "input = torch.randn(16, 57, 3)  # batch_size, seq_len, input_size\n",
    "out, (h, c) = lstm(input)  # h and c returned in tuple\n",
    "\n",
    "print(\"Input shape:\".ljust(15), input.shape)\n",
    "print(\"Shape of h\".ljust(15), h.shape)  # 1, batch_size, hidden_size\n",
    "print(\"Shape of c\".ljust(15), c.shape)  # 1, batch_size, hidden_size\n",
    "print(\"Output shape:\".ljust(15), out.shape)  # batch_size, seq_len, hidden_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU (Gated Recurrent Unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самая известная модификация LSTM — GRU. Она более компактна за счет сильных упрощений в сравнении со стандартной LSTM.\n",
    "\n",
    "Главные изменения: объединены forget и input gates, слиты $h_t$ и $c_t$, которые в обычной LSTM только участвовали в формировании друг друга."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<html>\n",
    "<head>\n",
    "<style>\n",
    "table, th, td {\n",
    "  border: 1px solid black;\n",
    "  border-collapse: collapse;\n",
    "}\n",
    "<font size=\"2\" face=\"Times New Romans\" >\n",
    "\n",
    "\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "<table >\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-content/L04/out/gru_basic_block.png\" width=\"500\"></center>\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "<table >\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "$\\large z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t])$\n",
    "\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "$\\large r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t])$\n",
    "\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "$\\large \\tilde h_t = \\tanh(W \\cdot [r_t \\otimes h_{t-1}, x_t])$\n",
    "\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "$\\large h_t = (1-z_t) \\otimes h_{t-1} + z_t \\otimes \\tilde h_t$\n",
    "\n",
    "</td>\n",
    "\n",
    "\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = nn.GRU(input_size=3, hidden_size=2, batch_first=True)\n",
    "input = torch.randn(16, 57, 3)  # batch_size, seq_len, input_size\n",
    "out, h = gru(input)\n",
    "\n",
    "print(\"Input shape:\".ljust(15), input.shape)\n",
    "print(\"Shape of h:\".ljust(15), h.shape)  # 1, batch_size, hidden_size\n",
    "print(\"Output shape:\".ljust(15), out.shape)  # batch_size, seq_len, hidden_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Практический опыт исследователей: иногда лучше работает GRU, иногда — LSTM. Точный рецепт успеха сказать нельзя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Типы задач"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Один к одному: на вход подается один элемент, на выходе получаем вероятность класса — классификация изображений.\n",
    "\n",
    "- Один ко многим: на вход приходит один элемент, а на выходе получаем целую последовательность — генерация текста по слову, текстовой подписи по изображению.\n",
    "\n",
    "- Многие к одному: на вход сети подается последовательность, а в качестве выхода получаем один вектор вероятностей для классов — классификация текстов (анализ тональности, определение языка).\n",
    "\n",
    "- Многие ко многим: преобразовать последовательность в последовательность\n",
    "  - количество выходов равно количеству входов — тегирование (именованные сущности, части речи).\n",
    "  - количество выходов сети не обязательно равно количеству входов — машинный перевод, суммаризация."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/RNN_tasks.png\" width=\"800\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://dotnettutorials.net/lesson/recurrent-neural-network/\">Recurrent Neural Network (RNNs)</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN для машинного перевода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Машинный перевод относится к задаче преобразования одной последовательности в другую, длина которых может быть любой и не обязательно должна совпадать: \"многие ко многим\" или sequence-to-sequence (сокращенно seq2seq).\n",
    "\n",
    "Модель для машинного перевода состоит из блока <font color=\"blue\"> кодировщика </font> и <font color=\"red\"> декодировщика</font>. Ниже представлена более подробная схема."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/seq2seq.png\" width=\"600\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://blog.paperspace.com/introduction-to-neural-machine-translation-with-bahdanaus-attention/\">Introduction to Neural Machine Translation</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <table >\n",
    "     <tr>\n",
    "       <td>\n",
    "\n",
    "Архитектура <font color=\"blue\">кодировщика</font>:\n",
    "\n",
    "- на вход первой ячейки RNN поступает вектор слова \"I\"<br>\n",
    "и нулевое скрытое состояние,\n",
    "- вектор слова \"I\" обрабатывается внутри первой ячейки RNN;\n",
    "- на вход второй ячейки поступает вектор слова \"am\"<br>и измененный вектор слова \"I\" как скрытое состояние;\n",
    "- вектор слова \"am\" обрабатывается внутри второй ячейки RNN;\n",
    "- $\\cdots$\n",
    "- вектор слова \"student\" обрабатывается внутри четвертой ячейки RNN;\n",
    "- измененный вектор слова \"student\" (= вектор предложения)<br>поступает на вход декодера как скрытое состояние.\n",
    "\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "\n",
    "Архитектура <font color=\"red\">декодировщика</font>:\n",
    "- на вход первой ячейки RNN поступает<br>вектор спецсимвола начала предложения <br>и вектор исходного предложения из энкодера как скрытое состояние;\n",
    "-  вектор спецсимвола начала предложения<br>обрабатывается внутри первой ячейки RNN;\n",
    "- на вход второй ячейки поступает вектор слова \"Je\"<br>и измененный вектор спецсимвола начала как скрытое состояние;\n",
    "- вектор слова \"Je\" обрабатывается внутри второй ячейки RNN;\n",
    "- $\\cdots$\n",
    "- вектор слова \"étudiant\" обрабатывается внутри четвертой ячейки RNN;\n",
    "- измененные векторы каждого слова<br>передаются на линейный слой, применяется softmax;\n",
    "- на выходе получаем 4 вектора, длина которых равна длине словаря, —<br>это распределение вероятностей для следующего элемента<br>при условии текущей последовательности.\n",
    "\n",
    "\n",
    "</td>\n",
    "     </tr>\n",
    "    </table>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак,\n",
    "- <font color=\"blue\">кодировщик</font> используется для получения конекстуализированного вектора исходного предложения;\n",
    "- <font color=\"red\">декодировщик</font> используется для генерации целевого предложения;\n",
    "- вектор исходного предложения используется в качестве скрытого состояния на первом шаге генерации.\n",
    "\n",
    "**Проблема Sequence-to-Sequence**: использование только конечного скрытого состояния энкодера для представления всей входной последовательности приводит к потере информации, особенно с начала последовательности;\n",
    "- по мере того, как энкодер обрабатывает входную последовательность, ожидается, что в конечном скрытом состоянии будет собрана вся необходимая информация;\n",
    "- когда последовательность становится длиннее, этому единственному состоянию труднее сохранять всю необходимую информацию из предыдущих частей последовательности.\n",
    "\n",
    "Чтобы модель могла фокусироваться на различных частях входной последовательности и сохранять больше информации на протяжении всего процесса кодирования и декодирования, был предложен механизм внимания (attention mechanism).\n",
    "\n",
    "[[paper] 🎓 Bahdanau D., Cho K., Bengio Y. (2014). Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/seq2seq_attention.png\" width=\"600\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://blog.paperspace.com/introduction-to-neural-machine-translation-with-bahdanaus-attention/\">Introduction to Neural Machine Translation</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Архитектура энкодера не меняется, все изменения касаются архитектуры декодера.\n",
    "- на вход первой ячейки RNN также поступает вектор спецсимвола начала предложения и вектор исходного предложения из энкодера как скрытое состояние;\n",
    "-  вектор спецсимвола начала предложения обрабатывается внутри первой ячейки RNN;\n",
    "- для вектора спецсимвола начала предложения и векторов каждого слова исходной последовательности считается мера сходства — это и есть веса внимания (attention weights);\n",
    "- если векторы некоторого слова целевого и исходного предложения схожи, то при переводе (генерации) данного слова декодер больше \"обращает внимания\" на него;\n",
    "- вектор каждого слова исходного предложения умножается на свой вес внимания, затем все векторы складываются, получаем контекстный вектор (context vector);\n",
    "- вектор спецсимвола начала предложения и контекстный вектор конкатенируются;\n",
    "- конкатенированный вектор передается на линейный слой, применяется softmax;\n",
    "- получаем вектор, длина которого равна длине словаря, — это распределение вероятностей для следующего элемента при условии спецсимвола начала предложения;\n",
    "- на вход второй ячейки поступает вектор слова \"Je\" и измененный вектор спецсимвола начала как скрытое состояние;\n",
    "- вектор слова \"Je\" обрабатывается внутри второй ячейки RNN;\n",
    "- считаются веса внимания для слова \"Je\" и всех слов целевого предложения;\n",
    "- $\\cdots$\n",
    "- для каждого слова получаем распределение вероятностей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/attention_equation.jpg\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://www.google.com/url?sa=i&url=https%3A%2F%2Fai.plainenglish.io%2Fintroduction-to-attention-mechanism-bahdanau-and-luong-attention-e2efd6ce22da&psig=AOvVaw1NlMR6N0XSxl6zYQwhAlYw&ust=1723824501708000&source=images&cd=vfe&opi=89978449&ved=0CAUQtaYDahcKEwiAhL3usPeHAxUAAAAAHQAAAAAQDw\">Introduction to Attention Mechanism</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Механизм внимания позволил исправить некоторые проблемы применения рекуррентных сети, однако ряд недостатков остается:\n",
    "- не способны запоминать дистантные зависимости\n",
    "- возникает взрыв и затухание градиента\n",
    "- вычисления проводятся последовательно и занимают много времени\n",
    "\n",
    "Что если убрать рекуррентность и оставить только механизм внимания?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Архитектура Трансформер"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Архитектура, в которой отсутствует рекуррентность и используется только механизм внимания, получила название Трансформер.\n",
    "\n",
    "[[paper] 🎓 Vaswani A. et al. (2017).Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "Как и в случае с рекуррентной сетью seq2seq, Трансформер состоит из блока энкодера и блока декодера.\n",
    "- Энкодер обрабатывает исходную последовательность и кодирует ее.\n",
    "- Декодер обрабатывает целевую последовательность с учетом информации из энкодера. Выход из декодера является предсказанием модели.\n",
    "\n",
    "Блок энкодера состоит из 6 энкодеров, расположенных друг за другом. Блок декодера – это стек декодеров, представленных в том же количестве."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/transformer.jpg\" width=\"860\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все энкодеры идентичны по структуре, хотя и имеют разные веса. Каждый можно разделить на два подслоя.\n",
    "- Входная последовательность, поступающая в энкодер, сначала проходит через слой внутреннего внимания (self-attention), помогающий энкодеру посмотреть на другие слова во входном предложении во время кодирования конкретного слова.\n",
    "- Выход слоя внутреннего внимания отправляется в нейронную сеть прямого распространения (feed-forward neural network).\n",
    "\n",
    "Декодер также содержит эти два слоя, но между ними есть слой внимания, который помогает декодеру фокусироваться на релевантных частях исходного предложения (аналогично механизму внимания в моделях seq2seq)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/enc_dec_tr.png\" width=\"650\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как и в случае других алгоритмов, каждое слово входной и целевой последовательности преобразуется в вектор.\n",
    "\n",
    "После того как слова входящего предложения преобразовались в эмбеддинги, каждый из них по отдельности проходит через слои энкодера. Одна из основных особенностей Трансформера состоит в том, что каждое слово идет по своей собственной траектории."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/encoder.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Механизм внутреннего внимания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть мы хотим перевести предложение: *The animal didn't cross the street because it was too tired*. Местоимение *it* может относиться к улице (*street*) или к животному (*animal*). Когда модель обрабатывает слово *it*, слой внутреннего внимания помогает понять, что *it* относится к *animal*.\n",
    "\n",
    "По мере того как модель обрабатывает каждое слово входной последовательности, внутреннее внимание позволяет \"взглянуть\" на другие слова и лучше закодировать данное слово. Механизм внутреннего внимания – это метод, который Трансформер использует, чтобы смоделировать \"понимание\" других релевантных слов при обработке конкретного слова.\n",
    "\n",
    "Во время кодирования *it* в энкодере №5 часть механизма внимания фокусируется на *The animal* и использует фрагмент его представления для кодирования *it*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/self_attention.png\" width=\"400\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### На примере векторов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При вычислении внутреннего внимания используются: вектор запроса $query$, вектор ключа $key$ и вектор значения $value$. Они создаются с помощью умножения эмбеддинга слова последовательности на три матрицы весов (линейных слоя) $W^Q, W^K, W^V$. Размер новых векторов – 64, размер исходных векторов – 512.\n",
    "\n",
    "$q_i=x_iW^Q$\n",
    "\n",
    "$k_i=x_iW^K$\n",
    "\n",
    "$v_i=x_iW^V$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/vector_attention.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее вычисляется коэффициент внутреннего внимания $score$ для $i$-го слова по отношению к каждому слову в предложении. Коэффициент определяет, насколько нужно сфокусироваться на других частях предложения во время кодирования слова в $i$-й позиции. Коэффициент подсчитывается с помощью скалярного произведения вектора запроса $q$ $i$-го слова и вектора ключа $k$ каждого слова.\n",
    "\n",
    "$score_{ij}=q_i \\cdot k_j$\n",
    "\n",
    "📌 Какие векторы нужно перемножить, чтобы посчитать коэффициент внимания слова *Thinking* по отношению к слову *Machines*? По отношению к самому себе?\n",
    "\n",
    "На следующем шаге коэффициенты делятся на квадратный корень из $d_k$ – размера векторов ключа $k$. К получившимся значениям применяется функция активации softmax, чтобы коэффициенты в сумме давали 1. Полученный софтмакс-коэффициент определяет, в какой мере каждое из слов предложения \"фокусируется\" на другом слове.\n",
    "\n",
    "$softmax.score_{ij}=softmax(\\frac{score_i}{\\sqrt d_k})$\n",
    "\n",
    "После этого каждый вектор значения $v$ умножается на софтмакс-коэффициент, получаем взвешенные векторы. Идея в том, что нужно сохранять без изменений значения слов, на которых мы фокусируемся, и отвести на второй план нерелевантные слова (умножив их на небольшие значения, например, 0.001). Затем  взвешенные векторы складываются. Результат (взвешенная сумма) представляет собой выход слоя внутреннего внимания для $i$-го слова.\n",
    "\n",
    "$sum_i=\\sum_{j=1}^nv_j \\cdot softmax.score_{ij}$\n",
    "\n",
    "Полученный вектор передается дальше в нейронную сеть прямого распространения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/attention_example.png\" width=\"600\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### На примере матриц"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично, матрицы запроса $Q$, ключа $K$ и значения $V$ вычисляются с помощью умножения эмбеддингов матрицы $X$ на матрицы весов (линейные слои) $W^Q, W^K, W^V$. Каждая строка в матрице $X$ соответствует слову в предложении."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/matrix_attention.png\" width=\"450\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Последующие этапы для вычисления выхода слоя внутреннего внимания могут быть отражены в одной формуле."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/attention_score.png\" width=\"600\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию `attention` для подсчета внутреннего внимания.\n",
    "\n",
    "$Attention(Q, K, V) = softmax\\large(\\frac{QK^T}{\\sqrt{d_k}})V$\n",
    "\n",
    "$Q, K, V$ — матрицы размера `batch_size, seq_length, num_features`.\n",
    "\n",
    "Для умножения матриц по батчам используется метод `.bmm`.\n",
    "\n",
    "Транспонирование осуществляется с помощью метода `.transpose`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"\"\"Compute 'Scaled Dot Product Attention'\n",
    "    \"\"\"\n",
    "    d_k = query.size(-1)\n",
    "    # todo: compute the attention scores by using torch.matmul\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    # todo: compute the result as the values weighted by attention probabilities (again, using torch.matmul)\n",
    "    result = torch.matmul(p_attn, value)\n",
    "    return result, p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, attentions = attention(\n",
    "    torch.tensor([[0, 0], [0, 1], [1, 1]], dtype=torch.float),\n",
    "    torch.tensor([[100, 0], [0, 100], [0, 0]], dtype=torch.float),\n",
    "    torch.tensor([[1, 0], [0, 1], [0, 0]], dtype=torch.float),\n",
    ")\n",
    "print(results)\n",
    "print(attentions)\n",
    "\n",
    "assert np.allclose(results[0].numpy(), [1/3, 1/3])  # the first query attends to all keys equally\n",
    "assert np.allclose(results[1].numpy(), [0, 1])      # the second query attends only to the second key\n",
    "assert np.allclose(results[2].numpy(), [1/2, 1/2])  # the third query attends to the first and second key equally"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
