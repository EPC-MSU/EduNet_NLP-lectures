{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">–ú–∞—à–∏–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ó–∞–¥–∞—á–∞ –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–†–∞–Ω–µ–µ –º—ã —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–ª–∏ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –∑–∞–¥–∞—á—É –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: –∫–∞–∂–¥–æ–º—É —Ç–µ–∫—Å—Ç—É —Å–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è –º–µ—Ç–∫–∞ –∫–ª–∞—Å—Å–∞ –∏–∑ –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞.\n",
    "\n",
    "–î—Ä—É–≥–∞—è –≤–∞–∂–Ω–∞—è –∑–∞–¥–∞—á–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ ‚Äî –º–∞—à–∏–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥: –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –æ–¥–Ω–æ–º —è–∑—ã–∫–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –¥—Ä—É–≥–æ–º —è–∑—ã–∫–µ, –ø–µ—Ä–µ–¥–∞—é—â–∞—è —Ç–æ—Ç –∂–µ —Å–º—ã—Å–ª.\n",
    "\n",
    "–†–∞—Å—Å–º–æ—Ç—Ä–∏–º –∑–∞–¥–∞—á—É –ø–µ—Ä–µ–≤–æ–¥–∞ —Å –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –Ω–∞ —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π —è–∑—ã–∫:\n",
    "- –ò—Å—Ö–æ–¥–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ: I am a student.\n",
    "- –¶–µ–ª–µ–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ: Je suis √©tudiant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-content/L04/out/machine_translation.png\" width=\"600\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ß—Ç–æ –Ω—É–∂–Ω–æ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ —Ç–µ–∫—Å—Ç–∞?\n",
    "1. –ü–æ–Ω–∏–º–∞—Ç—å –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞\n",
    "2. –ü–æ–Ω–∏–º–∞—Ç—å –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –º–µ–∂–¥—É —Å–ª–æ–≤–∞–º–∏ (—Å–∏–Ω—Ç–∞–∫—Å–∏—Å)\n",
    "3. –ü–µ—Ä–µ–≤–æ–¥–∏—Ç—å —Å–ª–æ–≤–∞ (—Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)\n",
    "4. –°–æ—Å—Ç–∞–≤–ª—è—Ç—å –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π –∏ —Å–≤—è–∑–Ω—ã–π —Ç–µ–∫—Å—Ç\n",
    "\n",
    "–î–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ —Ç–µ–∫—Å—Ç–∞ –º–æ–∂–Ω–æ —Ä–∞–∑–±–∏—Ç—å –∏—Å—Ö–æ–¥–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤, –∑–∞—Ç–µ–º –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å –µ–≥–æ –ø–æ —Ñ—Ä–∞–∑–∞–º.\n",
    "\n",
    "–û–¥–Ω–∞–∫–æ —É —Ç–∞–∫–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –µ—Å—Ç—å –ø—Ä–æ–±–ª–µ–º—ã:\n",
    "- –æ–¥–Ω–æ–º—É —Å–ª–æ–≤—É –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏ (*forced*) –º–æ–∂–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ª–æ–≤ –≤ —Ü–µ–ª–µ–≤–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏ (*a forc√©*)\n",
    "- –ø–æ—Ä—è–¥–æ–∫ —Å–ª–æ–≤ –º–æ–∂–µ—Ç –º–µ–Ω—è—Ç—å—Å—è (*exceptional measures* vs. *des mesures exceptionnelles*)\n",
    "\n",
    "–°–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, –≤–æ–∑–Ω–∏–∫–∞—é—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Å –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ–º –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è —Å–ª–æ–≤ (word alignment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-content/L04/out/word_alignment.png\" width=\"800\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î—Ä—É–≥–∞—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å ‚Äî –ø—Ä–æ—á–∏—Ç–∞—Ç—å –≤—Å–µ –∏—Å—Ö–æ–¥–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Ü–µ–ª–∏–∫–æ–º, –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏–µ, –∞ –∑–∞—Ç–µ–º –ø—Ä–æ–∏–∑–≤–µ—Å—Ç–∏ –ø–µ—Ä–µ–≤–æ–¥.\n",
    "\n",
    "–í —ç—Ç–æ–º —Å–ª—É—á–∞–µ –º–æ–¥–µ–ª—å —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –¥–≤—É—Ö –±–ª–æ–∫–æ–≤:\n",
    "- –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫: —Å—Ç—Ä–æ–∏—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (–∫–æ–¥–∏—Ä—É–µ—Ç);\n",
    "- –¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫: –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ü–µ–ª–µ–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è (—Ä–∞—Å–∫–æ–¥–∏—Ä—É–µ—Ç)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i.ibb.co/h2srmnP/encoder-decoder.png\" width=\"800\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://blog.paperspace.com/introduction-to-neural-machine-translation-with-bahdanaus-attention/\">Introduction to Neural Machine Translation</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í –∫–∞—á–µ—Å—Ç–≤–µ –±–ª–æ–∫–æ–≤ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞ –∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞ –º–æ–≥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ —Å —Ä–∞–∑–ª–∏—á–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π.\n",
    "1. –ò–∑–Ω–∞—á–∞–ª—å–Ω–æ —ç—Ç–æ –±—ã–ª–∏ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏, –∫–æ—Ç–æ—Ä—ã–µ —Ö–æ—Ä–æ—à–æ –ø–æ–¥—Ö–æ–¥—è—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö;\n",
    "2. –ü–æ–∑–∂–µ –±—ã–ª–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä, –∫–æ—Ç–æ—Ä–∞—è –ª–µ–∂–∏—Ç –≤ –æ—Å–Ω–æ–≤–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.\n",
    "\n",
    "–í —Å–µ–≥–æ–¥–Ω—è—à–Ω–µ–π –ª–µ–∫—Ü–∏–∏ –º—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –æ–±–µ —ç—Ç–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏ –∏—Ö –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤ –∑–∞–¥–∞—á–µ –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/rnn_transformer.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://shchegrikovich.substack.com/p/rnn-vs-transformers-or-how-scalability\">RNN vs Transformers</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –†–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–†–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ (recurrent neural networks, RNN)** ‚Äî —ç—Ç–æ –∫–ª–∞—Å—Å –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.\n",
    "\n",
    "–û–Ω–∏ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –≤ —à–∏—Ä–æ–∫–æ–º –ø–µ—Ä–µ—á–Ω–µ –∑–∞–¥–∞—á: –æ—Ç **—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏** –¥–æ **–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–¥–ø–∏—Å–µ–π** –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º.\n",
    "\n",
    "–≠—Ç–∏ –∑–∞–¥–∞—á–∏ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —Ä–∞–±–æ—Ç—ã —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –æ–¥–Ω–æ–π —á–∞—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö, –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥—Ä—É–≥–∏—Ö —á–∞—Å—Ç–µ–π."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/sequence_data.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://cbare.github.io/2019-01-27/deep-learning-sequence-models.html\">Deep Learning - Sequence Models</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –†–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–∞—è —è—á–µ–π–∫–∞ –∏ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–π —Å–ª–æ–π"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–†–∞—Å—Å–º–æ—Ç—Ä–∏–º —Ä–∞–±–æ—Ç—É —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã—Ö —Å–µ—Ç–µ–π –ø—Ä–∏–º–µ–Ω–∏—Ç–µ–ª—å–Ω–æ –∫ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º.\n",
    "\n",
    "–ö–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ $x_t$ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é **—è—á–µ–π–∫–∏ RNN**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/RNN_cell.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"http://vbystricky.ru/2021/05/rnn_lstm_gru_etc.html\">RNN, LSTM, GRU</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. –ù–∞ –≤—Ö–æ–¥ —è—á–µ–π–∫–∏ –ø–æ—Å—Ç—É–ø–∞–µ—Ç –≤—Ö–æ–¥–Ω–æ–π –≤–µ–∫—Ç–æ—Ä $x_t$ ‚Äî  —ç–º–±–µ–¥–¥–∏–Ω–≥ —Å–ª–æ–≤–∞ —Å –∏–Ω–¥–µ–∫—Å–æ–º $t$. –û–Ω –∏–º–µ–µ—Ç —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä $k$.\n",
    "\n",
    "2. –Ø—á–µ–π–∫–∞ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –µ—â–µ –æ–¥–∏–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä $h_{t-1}$ ‚Äî **—Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ** –∏–ª–∏ **–ø–∞–º—è—Ç—å** (hidden state). –≠—Ç–æ –≤–µ–∫—Ç–æ—Ä, —Ö—Ä–∞–Ω—è—â–∏–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–µ–¥—à–µ—Å—Ç–≤—É—é—â–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ. –û–Ω —Ç–æ–∂–µ –∏–º–µ–µ—Ç —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä $n$.\n",
    "\n",
    "3. –í–µ–∫—Ç–æ—Ä $x_t$ —É–º–Ω–æ–∂–∞–µ—Ç—Å—è –Ω–∞ –º–∞—Ç—Ä–∏—Ü—É $W^{nk}$ (—Ä–∞–∑–º–µ—Ä $n \\times k$), –∫–æ—Ç–æ—Ä–∞—è —Å–æ–¥–µ—Ä–∂–∏—Ç –æ–±—É—á–∞–µ–º—ã–µ –≤–µ—Å–∞: $W^{nk} \\cdot x_t$. –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤—ã–π –≤–µ–∫—Ç–æ—Ä —Ä–∞–∑–º–µ—Ä–∞ $n$.\n",
    "\n",
    "4. –í–µ–∫—Ç–æ—Ä $h_{t-1}$ —É–º–Ω–æ–∂–∞–µ—Ç—Å—è –Ω–∞ –¥—Ä—É–≥—É—é –º–∞—Ç—Ä–∏—Ü—É –≤–µ—Å–æ–≤ $W^{nn}$ (—Ä–∞–∑–º–µ—Ä $n \\times n$): $ W^{nn} \\cdot h_{t-1} $. –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤—ã–π –≤–µ–∫—Ç–æ—Ä —Ä–∞–∑–º–µ—Ä–∞ $n$.\n",
    "\n",
    "5. –ü–æ–ª—É—á–∏–≤—à–∏–µ—Å—è –≤–µ–∫—Ç–æ—Ä—ã –∏–º–µ—é—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä, –∏—Ö –º–æ–∂–Ω–æ —Å–ª–æ–∂–∏—Ç—å: $W^{nk} \\cdot x_t + W^{nn} \\cdot h_{t-1}$.\n",
    "\n",
    "6. –ö –ø–æ–ª—É—á–∏–≤—à–µ–º—É—Å—è –≤–µ–∫—Ç–æ—Ä—É –ø—Ä–∏–º–µ–Ω–∏–º —Ñ—É–Ω–∫—Ü–∏—é –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ ‚Äî –≥–∏–ø–µ—Ä–±–æ–ª–∏—á–µ—Å–∫–∏–π —Ç–∞–Ω–≥–µ–Ω—Å: $tanh(W^{nk} \\cdot x_t + W^{nn} \\cdot h_{t-1})$. –≠—Ç–æ –∏ –±—É–¥–µ—Ç –Ω–æ–≤—ã–º —Å–∫—Ä—ã—Ç—ã–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º $h_t$. –û–Ω–æ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è $h_{t-1}$ –∏ —Ç–µ–∫—É—â–µ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ $x_t$.\n",
    "\n",
    "7. –†–∞—Å—Å—á–∏—Ç–∞–Ω–Ω–æ–µ —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ $h_t$ —è–≤–ª—è–µ—Ç—Å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ–º —Ç–µ–∫—É—â–µ–≥–æ —Å–ª–æ–≤–∞ $x_t$ —Å —É—á–µ—Ç–æ–º –ø—Ä–µ–¥—à–µ—Å—Ç–≤—É—é—â–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Å—Ç–∞—Ä–æ–≥–æ —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–ª–æ–≤–∞ $x_{t+1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è **—Å–ª–æ–π RNN**, –≥–¥–µ —è—á–µ–π–∫–∞ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∫–æ –≤—Å–µ–º —Å–ª–æ–≤–∞–º –≤ —Ü–∏–∫–ª–µ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/RNN_layer.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"http://vbystricky.ru/2021/05/rnn_lstm_gru_etc.html\">RNN, LSTM, GRU</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ö–æ–≥–¥–∞ –ø–µ—Ä–≤—ã–π —Ç–æ–∫–µ–Ω $x_0$ –ø–æ–¥–∞–µ—Ç—Å—è –≤ —è—á–µ–π–∫—É, –≤–µ–∫—Ç–æ—Ä –ø–∞–º—è—Ç–∏ $h$ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è –Ω—É–ª—è–º–∏.\n",
    "\n",
    "–í–µ–∫—Ç–æ—Ä –ø–∞–º—è—Ç–∏ $h_0$ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ $x_0$ –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è –≤ —Å–ª–µ–¥—É—é—â—É—é —è—á–µ–π–∫—É, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—â—É—é –≤—Ç–æ—Ä–æ–π —Ç–æ–∫–µ–Ω $x_1$. –í–µ–∫—Ç–æ—Ä $h_0$ —Ç–∞–∫–∂–µ —è–≤–ª—è–µ—Ç—Å—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ–º —Ç–æ–∫–µ–Ω–∞ $x_0$ ‚Äî $y_0$.\n",
    "\n",
    "–î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞ –≤–µ–∫—Ç–æ—Ä –ø–∞–º—è—Ç–∏ —É—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤.\n",
    "\n",
    "–í–µ–∫—Ç–æ—Ä $y_t$ —è–≤–ª—è–µ—Ç—Å—è –Ω–µ —Ç–æ–ª—å–∫–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ–º –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Ç–æ–∫–µ–Ω–∞ $x_t$, –Ω–æ –∏ –≤–µ–∫—Ç–æ—Ä–æ–º –≤—Å–µ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, —Ç.–∫. —Å–æ–¥–µ—Ä–∂–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–±–æ –≤—Å–µ—Ö —Ç–æ–∫–µ–Ω–∞—Ö.\n",
    "\n",
    "–í –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏, –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ –∏–ª–∏ —Ç–æ–ª—å–∫–æ –≤–µ–∫—Ç–æ—Ä –≤—Å–µ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (–ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ª–æ–≤–∞)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN —Å–ª–æ–π –≤ PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í PyTorch –µ—Å—Ç—å —Å–ª–æ–π ‚Äî `torch.nn.RNN` [üõ†Ô∏è[doc]](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html), –∫–æ—Ç–æ—Ä—ã–π —Ä–µ–∞–ª–∏–∑—É–µ—Ç –ª–æ–≥–∏–∫—É, –æ–ø–∏—Å–∞–Ω–Ω—É—é –≤—ã—à–µ.\n",
    "\n",
    "–¢–∞–∫–∂–µ –µ—Å—Ç—å —Å—É—â–Ω–æ—Å—Ç—å `torch.nn.RNNCell` [üõ†Ô∏è[doc]](https://pytorch.org/docs/stable/generated/torch.nn.RNNCell.html), –∫–æ—Ç–æ—Ä–∞—è —Ä–µ–∞–ª–∏–∑—É–µ—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –Ω–∞ –æ–¥–Ω–æ–º —Ç–∞–∫—Ç–µ –≤—Ä–µ–º–µ–Ω–∏.\n",
    "\n",
    "–°–ª–æ–π `nn.RNN` —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏ —è–≤–ª—è–µ—Ç—Å—è –æ–±–µ—Ä—Ç–∫–æ–π, –∫–æ—Ç–æ—Ä–∞—è –≤—ã–∑—ã–≤–∞–µ—Ç `nn.RNNCell` –≤ —Ü–∏–∫–ª–µ –ø–æ –¥–ª–∏–Ω–µ –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.\n",
    "\n",
    "–ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–ª–æ—è `nn.RNN`:\n",
    "\n",
    "* **`input_size`** ‚Äî —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å $\\large x_t$, —Ü–µ–ª–æ–µ —á–∏—Å–ª–æ.\n",
    "\n",
    "* **`hidden_size`** ‚Äî —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å $\\large h_t$, —Ü–µ–ª–æ–µ —á–∏—Å–ª–æ. –§–∞–∫—Ç–∏—á–µ—Å–∫–∏ —ç—Ç–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–π—Ä–æ–Ω–æ–≤ –≤ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ–º —Å–ª–æ–µ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã—Ö —Å–ª–æ–µ–≤ –≤ PyTorch: –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –æ–∂–∏–¥–∞–µ–º—ã–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –≤—Ö–æ–¥–∞ —Ç–∞–∫–∏–µ:\n",
    "\n",
    "**`[–¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–∞]`**\n",
    "\n",
    "–û–¥–Ω–∞–∫–æ –µ—Å–ª–∏ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Å–ª–æ—è —É–∫–∞–∑–∞—Ç—å `batch_first=True`, —Ç–æ –º–æ–∂–Ω–æ –ø–æ–¥–∞–≤–∞—Ç—å –≤—Ö–æ–¥–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –±–æ–ª–µ–µ –ø—Ä–∏–≤—ã—á–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ, –∫–æ–≥–¥–∞ —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ —Å—Ç–æ–∏—Ç –Ω–∞ –ø–µ—Ä–≤–æ–º –º–µ—Å—Ç–µ:\n",
    "\n",
    "**`[—Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞, –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–∞]`**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "rnn = torch.nn.RNN(input_size=3, hidden_size=2, batch_first=True)\n",
    "\n",
    "dummy_batched_seq = torch.randn((16, 57, 3))  # batch_size, seq_len, input_size\n",
    "out, h = rnn(dummy_batched_seq)\n",
    "\n",
    "print(\"Input shape:\".ljust(20), f\"{dummy_batched_seq.shape}\")\n",
    "print(\"Out shape:\".ljust(20), f\"{out.shape}\")\n",
    "print(\"Last hidden state shape:\".ljust(20), f\"{h.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–∏ –≤—ã–∑–æ–≤–µ —Å–ª–æ–π –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–≤–∞ –æ–±—ä–µ–∫—Ç–∞:\n",
    "* `out` ‚Äî –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–∫—Ä—ã—Ç—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π,\n",
    "* `h` ‚Äî —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–º —Ç–∞–∫—Ç–µ.\n",
    "\n",
    "–ú—ã —É–∫–∞–∑–∞–ª–∏ `batch_first=True`, –ø—Ä–∏ —ç—Ç–æ–º `out` —Å–æ—Ö—Ä–∞–Ω–∏–ª –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π, –∫–∞–∫ —É –≤—Ö–æ–¥–∞, –∞ –≤–æ—Ç —É `h` —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –±–∞—Ç—á–∞ –≤—Å—Ç–∞–ª–∞ –Ω–∞ –≤—Ç–æ—Ä–æ–µ –º–µ—Å—Ç–æ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_batch_first = h.permute(1, 0, 2)\n",
    "\n",
    "print(f\"h is last out: {(h_batch_first == out[:, -1:, :]).all().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ú–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã–µ RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN-–±–ª–æ–∫–∏ –º–æ–∂–Ω–æ –æ–±—ä–µ–¥–∏–Ω—è—Ç—å –≤ —Å–ª–æ–∏, –Ω–∞–∫–ª–∞–¥—ã–≤–∞—è –∏—Ö –¥—Ä—É–≥ –Ω–∞ –¥—Ä—É–≥–∞. –î–ª—è —ç—Ç–æ–π –æ–ø–µ—Ä–∞—Ü–∏–∏ –≤ `torch.nn.RNN` –µ—Å—Ç—å –∞—Ä–≥—É–º–µ–Ω—Ç `num_layers`, —Å –ø–æ–º–æ—â—å—é –∫–æ—Ç–æ—Ä–æ–≥–æ –º–æ–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ—ë–≤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-content/L04/out/rnn_multiple_layers.png\" width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn((16, 6, 3))  # batch_size, seq_len, input_size\n",
    "rnn = torch.nn.RNN(input_size=3, hidden_size=2, num_layers=2, batch_first=True)\n",
    "\n",
    "out, h = rnn(dummy_input)\n",
    "\n",
    "print()\n",
    "print(\"Out:\\n\", out.shape)  # Hidden states for all elements from top layer\n",
    "print(\"h:\\n\", h.shape)  # Hidden states for last element for all layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ü—Ä–æ–±–ª–µ–º—ã RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏, –º–æ–∂–Ω–æ –±—ã–ª–æ –±—ã —Å—Ä–∞–∑—É –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å –≤—Å–µ –¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ —Å–µ—Ç—å –∏ –∑–∞—Ç–µ–º –≤—ã—á–∏—Å–ª–∏—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç, –æ–¥–Ω–∞–∫–æ –≤–æ–∑–Ω–∏–∫–Ω—É—Ç —Å–ª–µ–¥—É—é—â–∏–µ –ø—Ä–æ–±–ª–µ–º—ã:\n",
    "\n",
    " - –±–æ–ª—å—à–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–µ –ø–æ–º–µ—Å—Ç—è—Ç—Å—è –≤ –ø–∞–º—è—Ç–∏,\n",
    " - —Ç–∞–∫ –∫–∞–∫ —Ü–µ–ø–æ—á–∫–∞ –±—É–¥–µ—Ç –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω–æ–π, –≤–æ–∑–Ω–∏–∫–Ω–µ—Ç –∑–∞—Ç—É—Ö–∞–Ω–∏–µ/–≤–∑—Ä—ã–≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞,\n",
    " - –ø–æ –º–µ—Ä–µ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è —Å–∏–≥–Ω–∞–ª–∞ –ø–æ —Ü–µ–ø–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –∑–∞—Ç–∏—Ä–∞–µ—Ç—Å—è.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–æ–ø—É—Å—Ç–∏–º, —É –Ω–∞—Å –µ—Å—Ç—å –¥–ª–∏–Ω–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å. –ï—Å–ª–∏ –º—ã —Å—Ä–∞–∑—É –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º, —Ç–æ –≤ –∫–∞–∂–¥—ã–π –º–æ–º–µ–Ω—Ç –≤—Ä–µ–º–µ–Ω–∏ –Ω—É–∂–Ω–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–∏—Ç—å Loss. –ò –≤—Å–µ —è—á–µ–π–∫–∏ –Ω—É–∂–Ω–æ –æ–±–Ω–æ–≤–∏—Ç—å –≤–æ –≤—Ä–µ–º—è backpropogation. –í—Å–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –Ω—É–∂–Ω–æ –ø–æ—Å—á–∏—Ç–∞—Ç—å. –í–æ–∑–Ω–∏–∫–∞—é—Ç –ø—Ä–æ–±–ª–µ–º—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –Ω–µ—Ö–≤–∞—Ç–∫–æ–π –ø–∞–º—è—Ç–∏.\n",
    "\n",
    "–ï—Å—Ç—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–µ—Å—Ç—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏, –∫–æ–Ω—Ç–µ–∫—Å—Ç –∫–∞–∫–æ–π –¥–ª–∏–Ω—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç RNN –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏. –ï—Å–ª–∏ –º—ã –¥–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç–æ–ª—å–∫–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–π —è—á–µ–π–∫–µ, –º–æ–∂–µ—Ç –æ–∫–∞–∑–∞—Ç—å—Å—è, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, —Å–∫–∞–∂–µ–º, –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ –æ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 10 —Å–ª–æ–≤–∞—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.\n",
    "\n",
    "–§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ Tanh –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ –∑–∞—Ç–∏—Ä–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/backprop_through_time.png\" width=\"700\"><center>\n",
    "\n",
    "<center><em>Source: <a href=\"http://cs231n.stanford.edu/slides/2021/lecture_10.pdf\">CS231n: Recurrent Neural Network</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ó–∞—Ç—É—Ö–∞—é—â–∏–π/–≤–∑—Ä—ã–≤–∞—é—â–∏–π—Å—è –≥—Ä–∞–¥–∏–µ–Ω—Ç (Vanishing/exploding gradient) ‚Äî —è–≤–ª–µ–Ω–∏—è –∑–∞—Ç—É—Ö–∞—é—â–µ–≥–æ –∏ –≤–∑—Ä—ã–≤–∞—é—â–µ–≥–æ—Å—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ RNN. –ò –ø—Ä–∏ –±–æ–ª—å—à–æ–π –¥–ª–∏–Ω–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —ç—Ç–æ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –∫—Ä–∏—Ç–∏—á–Ω—ã–º. –ü—Ä–∏—á–∏–Ω–∞ –≤ —Ç–æ–º, —á—Ç–æ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –≤–µ–ª–∏—á–∏–Ω—ã –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –æ—Ç —á–∏—Å–ª–∞ —Å–ª–æ—ë–≤ —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è, –ø–æ—Å–∫–æ–ª—å–∫—É –≤–µ—Å–∞ —É–º–Ω–æ–∂–∞—é—Ç—Å—è –º–Ω–æ–≥–æ–∫—Ä–∞—Ç–Ω–æ.\n",
    "\n",
    "$dL ‚àù (W)^N:$\n",
    "\n",
    "$W > 1 \\rightarrow$ –≤–∑—Ä—ã–≤, $W < 1 \\rightarrow$ –∑–∞—Ç—É—Ö–∞–Ω–∏–µ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet_NLP-content/L04/out/simple_rnn_backprop.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–¥–∏–Ω –∏–∑ –ø—É—Ç–µ–π —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã ‚Äî **–≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–µ –æ—Ç—Å–µ—á–µ–Ω–∏–µ** (Gradient truncating) ‚Äî –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –¥–æ–ø—É—Å—Ç–∏–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞, –ø–æ–∑–≤–æ–ª—è—è –∏–∑–±–µ–∂–∞—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –≤–∑—Ä—ã–≤–∞.\n",
    "\n",
    "–ê –æ—Ç –∑–∞—Ç—É—Ö–∞–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –º–æ–∂–µ—Ç –ø–æ–º–æ—á—å **–ø—Ä–æ–ø—É—Å–∫–∞–Ω–∏–µ** **–≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –ø–æ —á–∞—Å—Ç—è–º**, –Ω–∞ —Å–∫–æ–ª—å–∫–æ-—Ç–æ —à–∞–≥–æ–≤ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞–∑–∞–¥ –∏–ª–∏ –≤–ø–µ—Ä—ë–¥, –∞ –Ω–µ —á–µ—Ä–µ–∑ –≤—Å—é –Ω–µ–π—Ä–æ—Å–µ—Ç—å. –î–∞, –≥—Ä–∞–¥–∏–µ–Ω—Ç –±—É–¥–µ—Ç –Ω–µ —Å–æ–≤—Å–µ–º —Ç–æ—á–Ω–æ —Å—á–∏—Ç–∞—Ç—å—Å—è, –∏ –º—ã –±—É–¥–µ–º —Ç–µ—Ä—è—Ç—å –≤ –∫–∞—á–µ—Å—Ç–≤–µ. –ù–æ —ç—Ç–æ —ç–∫–æ–Ω–æ–º–∏—Ç –ø–∞–º—è—Ç—å."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/truncated_backprop.png\"  width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"http://cs231n.stanford.edu/slides/2021/lecture_10.pdf\">CS231n: Recurrent Neural Network</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM (Long Short-Term Memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–±—ã—á–Ω–∞—è RNN –∏–º–µ–ª–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–æ –ø—Ä–æ–±–ª–µ–º, –≤ —Ç–æ–º —á–∏—Å–ª–µ –≤ –Ω–µ–π –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä–æ –∑–∞—Ç—É—Ö–∞–ª–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–∞—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –ü–æ–º–∏–º–æ —ç—Ç–æ–≥–æ –±—ã–ª–∏ –ø—Ä–æ–±–ª–µ–º—ã —Å –∑–∞—Ç—É—Ö–∞–Ω–∏–µ–º/–≤–∑—Ä—ã–≤–æ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞.\n",
    "\n",
    "–≠—Ç–∏ –ø—Ä–æ–±–ª–µ–º—ã –±—ã–ª–∏ —á–∞—Å—Ç–∏—á–Ω–æ —Ä–µ—à–µ–Ω—ã –≤ LSTM, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–π –≤ [Long Short-Term Memory (Hochreiter & Schmidhuber, 1997) üéì[article]](http://www.bioinf.jku.at/publications/older/2604.pdf).\n",
    "\n",
    "–í –æ–±—ã—á–Ω–æ–π RNN-—è—á–µ–π–∫–µ –±—ã–ª —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –ø—É—Ç—å –ø–µ—Ä–µ–¥–∞—á–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –ù–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –º—ã –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä–æ–≤–∞–ª–∏ –ø—Ä–µ–¥—ã–¥—É—â–µ–µ —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å —Ç–µ–∫—É—â–∏–º –≤—Ö–æ–¥–æ–º –∏ –ø—Ä–æ–ø—É—Å–∫–∞–ª–∏ –∏—Ö —á–µ—Ä–µ–∑ –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π –∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–µ–π:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<html>\n",
    "<head>\n",
    "<style>\n",
    "table, th, td {\n",
    "  border: 1px solid black;\n",
    "  border-collapse: collapse;\n",
    "}\n",
    "<font size=\"2\" face=\"Times New Romans\" >\n",
    "\n",
    "\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "<table >\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "<center><img src = \"https://edunet.kea.su/repo/EduNet_NLP-content/L04/out/simple_rnn_h_state.png\" width=\"500\"></center>\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "<table >\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "$\\large h_t = \\tanh(W \\cdot [h_{t-1}, x_t])$\n",
    "\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "\n",
    "\n",
    "</table>\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–∏ —ç—Ç–æ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–∞—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä–æ –∑–∞—Ç—É—Ö–∞–µ—Ç –∏ —Ç–µ—Ä—è–µ—Ç—Å—è –æ–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏.\n",
    "\n",
    "–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —è—á–µ–π–∫–∏ LSTM –Ω–∞–º–Ω–æ–≥–æ —Å–ª–æ–∂–Ω–µ–µ. –ó–¥–µ—Å—å –µ—Å—Ç—å —Ü–µ–ª—ã—Ö 4 –ª–∏–Ω–µ–π–Ω—ã—Ö —Å–ª–æ—è, –∫–∞–∂–¥—ã–π –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö –≤—ã–ø–æ–ª–Ω—è–µ—Ç —Ä–∞–∑–Ω—ã–µ –∑–∞–¥–∞—á–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<html>\n",
    "<head>\n",
    "<style>\n",
    "table, th, td {\n",
    "  border: 1px solid black;\n",
    "  border-collapse: collapse;\n",
    "}\n",
    "<font size=\"2\" face=\"Times New Romans\" >\n",
    "\n",
    "\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "<table >\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-content/L04/out/lstm_chain.png\" width=\"500\"></center>\n",
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-content/L04/out/lstm_chain_notation.png\" width=\"700\"></center>\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "<table >\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "$\\large f_t = œÉ(W_f \\cdot [h_{t-1}, x_t])\\ \\ \\ \\ $\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "$$\\large \\text{forget  gate}$$\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "$\\large i_t = œÉ(W_i \\cdot [h_{t-1}, x_t])$\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "$$\\large \\text{input gate}$$\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "$\\large o_t = œÉ(W_o \\cdot [h_{t-1}, x_t])$\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "$$\\large \\text{output gate}$$\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "$\\large c^\\prime_t = \\tanh(W_c \\cdot [h_{t-1}, x_t])$\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "$$\\large \\text{candidate cell state}$$\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "$\\large c_t = f_t\\otimes c_{t-1} + i_t \\otimes c^\\prime_t$\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "$$\\large \\text{cell state}$$\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "$\\large h_t = o_t\\otimes \\tanh(c_t)$\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "$$\\large  \\text{hidden state}$$\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ì–ª–∞–≤–Ω–æ–µ –Ω–æ–≤–æ–≤–≤–µ–¥–µ–Ω–∏–µ: –≤ LSTM –¥–æ–±–∞–≤–ª–µ–Ω –ø—É—Ç—å $c$, –∫–æ—Ç–æ—Ä—ã–π –ø–æ –∑–∞–¥—É–º–∫–µ –¥–æ–ª–∂–µ–Ω —ç—Ç–æ—Ç –æ–±—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–æ—Ö—Ä–∞–Ω—è—Ç—å."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-content/L04/out/lstm_c_state_highway.png\" width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î—Ä—É–≥–∏–º–∏ —Å–ª–æ–≤–∞–º–∏, –ø—É—Ç—å $c$ (cell state, –∏–Ω–æ–≥–¥–∞ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è highway, –º–∞–≥–∏—Å—Ç—Ä–∞–ª—å)  –ø–æ–º–æ–≥–∞–µ—Ç –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –≤–∞–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –≤—Å—Ç—Ä–µ—Ç–∏–≤—à—É—é—Å—è –≤ –∫–∞–∫–æ–π-—Ç–æ –º–æ–º–µ–Ω—Ç –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –ø—Ä–æ—à–ª–æ–º, –≤—Å–µ –≤—Ä–µ–º—è, –ø–æ–∫–∞ —ç—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Ç—Ä–µ–±—É–µ—Ç—Å—è.\n",
    "\n",
    "–ü–æ —Ñ–æ—Ä–º—É–ª–∞–º —Ç–∞–∫–∂–µ –≤–∏–¥–Ω–æ, –∫–∞–∫ –≤–æ–∑—Ä–æ—Å–ª–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç—å."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û—Ç–ª–∏—á–∏–µ –æ—Ç RNN —Å–æ—Å—Ç–æ–∏—Ç –≤ —Ç–æ–º, —á—Ç–æ –∫—Ä–æ–º–µ $h$ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è –µ—â–µ –∏ $c$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "lstm = nn.LSTM(input_size=3, hidden_size=2, batch_first=True)\n",
    "input = torch.randn(16, 57, 3)  # batch_size, seq_len, input_size\n",
    "out, (h, c) = lstm(input)  # h and c returned in tuple\n",
    "\n",
    "print(\"Input shape:\".ljust(15), input.shape)\n",
    "print(\"Shape of h\".ljust(15), h.shape)  # 1, batch_size, hidden_size\n",
    "print(\"Shape of c\".ljust(15), c.shape)  # 1, batch_size, hidden_size\n",
    "print(\"Output shape:\".ljust(15), out.shape)  # batch_size, seq_len, hidden_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU (Gated Recurrent Unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–∞–º–∞—è –∏–∑–≤–µ—Å—Ç–Ω–∞—è –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è LSTM ‚Äî GRU. –û–Ω–∞ –±–æ–ª–µ–µ –∫–æ–º–ø–∞–∫—Ç–Ω–∞ –∑–∞ —Å—á–µ—Ç —Å–∏–ª—å–Ω—ã—Ö —É–ø—Ä–æ—â–µ–Ω–∏–π –≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π LSTM.\n",
    "\n",
    "–ì–ª–∞–≤–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è: –æ–±—ä–µ–¥–∏–Ω–µ–Ω—ã forget –∏ input gates, —Å–ª–∏—Ç—ã $h_t$ –∏ $c_t$, –∫–æ—Ç–æ—Ä—ã–µ –≤ –æ–±—ã—á–Ω–æ–π LSTM —Ç–æ–ª—å–∫–æ —É—á–∞—Å—Ç–≤–æ–≤–∞–ª–∏ –≤ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–∏ –¥—Ä—É–≥ –¥—Ä—É–≥–∞."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<html>\n",
    "<head>\n",
    "<style>\n",
    "table, th, td {\n",
    "  border: 1px solid black;\n",
    "  border-collapse: collapse;\n",
    "}\n",
    "<font size=\"2\" face=\"Times New Romans\" >\n",
    "\n",
    "\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "<table >\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-content/L04/out/gru_basic_block.png\" width=\"500\"></center>\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "<table >\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "$\\large z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t])$\n",
    "\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "$\\large r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t])$\n",
    "\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "$\\large \\tilde h_t = \\tanh(W \\cdot [r_t \\otimes h_{t-1}, x_t])$\n",
    "\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "$\\large h_t = (1-z_t) \\otimes h_{t-1} + z_t \\otimes \\tilde h_t$\n",
    "\n",
    "</td>\n",
    "\n",
    "\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = nn.GRU(input_size=3, hidden_size=2, batch_first=True)\n",
    "input = torch.randn(16, 57, 3)  # batch_size, seq_len, input_size\n",
    "out, h = gru(input)\n",
    "\n",
    "print(\"Input shape:\".ljust(15), input.shape)\n",
    "print(\"Shape of h:\".ljust(15), h.shape)  # 1, batch_size, hidden_size\n",
    "print(\"Output shape:\".ljust(15), out.shape)  # batch_size, seq_len, hidden_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –æ–ø—ã—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–µ–π: –∏–Ω–æ–≥–¥–∞ –ª—É—á—à–µ —Ä–∞–±–æ—Ç–∞–µ—Ç GRU, –∏–Ω–æ–≥–¥–∞ ‚Äî LSTM. –¢–æ—á–Ω—ã–π —Ä–µ—Ü–µ–ø—Ç —É—Å–ø–µ—Ö–∞ —Å–∫–∞–∑–∞—Ç—å –Ω–µ–ª—å–∑—è."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –¢–∏–ø—ã –∑–∞–¥–∞—á"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- –û–¥–∏–Ω –∫ –æ–¥–Ω–æ–º—É: –Ω–∞ –≤—Ö–æ–¥ –ø–æ–¥–∞–µ—Ç—Å—è –æ–¥–∏–Ω —ç–ª–µ–º–µ–Ω—Ç, –Ω–∞ –≤—ã—Ö–æ–¥–µ –ø–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∞ ‚Äî –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.\n",
    "\n",
    "- –û–¥–∏–Ω –∫–æ –º–Ω–æ–≥–∏–º: –Ω–∞ –≤—Ö–æ–¥ –ø—Ä–∏—Ö–æ–¥–∏—Ç –æ–¥–∏–Ω —ç–ª–µ–º–µ–Ω—Ç, –∞ –Ω–∞ –≤—ã—Ö–æ–¥–µ –ø–æ–ª—É—á–∞–µ–º —Ü–µ–ª—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å ‚Äî –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –ø–æ —Å–ª–æ–≤—É, —Ç–µ–∫—Å—Ç–æ–≤–æ–π –ø–æ–¥–ø–∏—Å–∏ –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é.\n",
    "\n",
    "- –ú–Ω–æ–≥–∏–µ –∫ –æ–¥–Ω–æ–º—É: –Ω–∞ –≤—Ö–æ–¥ —Å–µ—Ç–∏ –ø–æ–¥–∞–µ—Ç—Å—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –∞ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –≤—ã—Ö–æ–¥–∞ –ø–æ–ª—É—á–∞–µ–º –æ–¥–∏–Ω –≤–µ–∫—Ç–æ—Ä –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –¥–ª—è –∫–ª–∞—Å—Å–æ–≤ ‚Äî –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤ (–∞–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏, –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞).\n",
    "\n",
    "- –ú–Ω–æ–≥–∏–µ –∫–æ –º–Ω–æ–≥–∏–º: –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å\n",
    "  - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã—Ö–æ–¥–æ–≤ —Ä–∞–≤–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –≤—Ö–æ–¥–æ–≤ ‚Äî —Ç–µ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ (–∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏, —á–∞—Å—Ç–∏ —Ä–µ—á–∏).\n",
    "  - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã—Ö–æ–¥–æ–≤ —Å–µ—Ç–∏ –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —Ä–∞–≤–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –≤—Ö–æ–¥–æ–≤ ‚Äî –º–∞—à–∏–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥, —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/RNN_tasks.png\" width=\"800\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://dotnettutorials.net/lesson/recurrent-neural-network/\">Recurrent Neural Network (RNNs)</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú–∞—à–∏–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ –∑–∞–¥–∞—á–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –¥—Ä—É–≥—É—é, –¥–ª–∏–Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –º–æ–∂–µ—Ç –±—ã—Ç—å –ª—é–±–æ–π –∏ –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–ª–∂–Ω–∞ —Å–æ–≤–ø–∞–¥–∞—Ç—å: \"–º–Ω–æ–≥–∏–µ –∫–æ –º–Ω–æ–≥–∏–º\" –∏–ª–∏ sequence-to-sequence (—Å–æ–∫—Ä–∞—â–µ–Ω–Ω–æ seq2seq).\n",
    "\n",
    "–ú–æ–¥–µ–ª—å –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –±–ª–æ–∫–∞ <font color=\"blue\"> –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞ </font> –∏ <font color=\"red\"> –¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞</font>. –ù–∏–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω–∞—è —Å—Ö–µ–º–∞."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/seq2seq.png\" width=\"600\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://blog.paperspace.com/introduction-to-neural-machine-translation-with-bahdanaus-attention/\">Introduction to Neural Machine Translation</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <table >\n",
    "     <tr>\n",
    "       <td>\n",
    "\n",
    "–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ <font color=\"blue\">–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞</font>:\n",
    "\n",
    "- –Ω–∞ –≤—Ö–æ–¥ –ø–µ—Ä–≤–æ–π —è—á–µ–π–∫–∏ RNN –ø–æ—Å—Ç—É–ø–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä —Å–ª–æ–≤–∞ \"I\"<br>\n",
    "–∏ –Ω—É–ª–µ–≤–æ–µ —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ,\n",
    "- –≤–µ–∫—Ç–æ—Ä —Å–ª–æ–≤–∞ \"I\" –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ –ø–µ—Ä–≤–æ–π —è—á–µ–π–∫–∏ RNN;\n",
    "- –Ω–∞ –≤—Ö–æ–¥ –≤—Ç–æ—Ä–æ–π —è—á–µ–π–∫–∏ –ø–æ—Å—Ç—É–ø–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä —Å–ª–æ–≤–∞ \"am\"<br>–∏ –∏–∑–º–µ–Ω–µ–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä —Å–ª–æ–≤–∞ \"I\" –∫–∞–∫ —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ;\n",
    "- –≤–µ–∫—Ç–æ—Ä —Å–ª–æ–≤–∞ \"am\" –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ –≤—Ç–æ—Ä–æ–π —è—á–µ–π–∫–∏ RNN;\n",
    "- $\\cdots$\n",
    "- –≤–µ–∫—Ç–æ—Ä —Å–ª–æ–≤–∞ \"student\" –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ —á–µ—Ç–≤–µ—Ä—Ç–æ–π —è—á–µ–π–∫–∏ RNN;\n",
    "- –∏–∑–º–µ–Ω–µ–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä —Å–ª–æ–≤–∞ \"student\" (= –≤–µ–∫—Ç–æ—Ä –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)<br>–ø–æ—Å—Ç—É–ø–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥ –¥–µ–∫–æ–¥–µ—Ä–∞ –∫–∞–∫ —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ.\n",
    "\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "\n",
    "–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ <font color=\"red\">–¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞</font>:\n",
    "- –Ω–∞ –≤—Ö–æ–¥ –ø–µ—Ä–≤–æ–π —è—á–µ–π–∫–∏ RNN –ø–æ—Å—Ç—É–ø–∞–µ—Ç<br>–≤–µ–∫—Ç–æ—Ä —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–∞ –Ω–∞—á–∞–ª–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è <br>–∏ –≤–µ–∫—Ç–æ—Ä –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏–∑ —ç–Ω–∫–æ–¥–µ—Ä–∞ –∫–∞–∫ —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ;\n",
    "-  –≤–µ–∫—Ç–æ—Ä —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–∞ –Ω–∞—á–∞–ª–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è<br>–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ –ø–µ—Ä–≤–æ–π —è—á–µ–π–∫–∏ RNN;\n",
    "- –Ω–∞ –≤—Ö–æ–¥ –≤—Ç–æ—Ä–æ–π —è—á–µ–π–∫–∏ –ø–æ—Å—Ç—É–ø–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä —Å–ª–æ–≤–∞ \"Je\"<br>–∏ –∏–∑–º–µ–Ω–µ–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–∞ –Ω–∞—á–∞–ª–∞ –∫–∞–∫ —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ;\n",
    "- –≤–µ–∫—Ç–æ—Ä —Å–ª–æ–≤–∞ \"Je\" –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ –≤—Ç–æ—Ä–æ–π —è—á–µ–π–∫–∏ RNN;\n",
    "- $\\cdots$\n",
    "- –≤–µ–∫—Ç–æ—Ä —Å–ª–æ–≤–∞ \"√©tudiant\" –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ —á–µ—Ç–≤–µ—Ä—Ç–æ–π —è—á–µ–π–∫–∏ RNN;\n",
    "- –∏–∑–º–µ–Ω–µ–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞<br>–ø–µ—Ä–µ–¥–∞—é—Ç—Å—è –Ω–∞ –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π, –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è softmax;\n",
    "- –Ω–∞ –≤—ã—Ö–æ–¥–µ –ø–æ–ª—É—á–∞–µ–º 4 –≤–µ–∫—Ç–æ—Ä–∞, –¥–ª–∏–Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö —Ä–∞–≤–Ω–∞ –¥–ª–∏–Ω–µ —Å–ª–æ–≤–∞—Ä—è, ‚Äî<br>—ç—Ç–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞<br>–ø—Ä–∏ —É—Å–ª–æ–≤–∏–∏ —Ç–µ–∫—É—â–µ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.\n",
    "\n",
    "\n",
    "</td>\n",
    "     </tr>\n",
    "    </table>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò—Ç–∞–∫,\n",
    "- <font color=\"blue\">–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫</font> –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω–µ–∫—Å—Ç—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è;\n",
    "- <font color=\"red\">–¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫</font> –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ü–µ–ª–µ–≤–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è;\n",
    "- –≤–µ–∫—Ç–æ—Ä –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –Ω–∞ –ø–µ—Ä–≤–æ–º —à–∞–≥–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.\n",
    "\n",
    "**–ü—Ä–æ–±–ª–µ–º–∞ Sequence-to-Sequence**: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–æ–ª—å–∫–æ –∫–æ–Ω–µ—á–Ω–æ–≥–æ —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è —ç–Ω–∫–æ–¥–µ—Ä–∞ –¥–ª—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –≤—Å–µ–π –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –ø–æ—Ç–µ—Ä–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –æ—Å–æ–±–µ–Ω–Ω–æ —Å –Ω–∞—á–∞–ª–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏;\n",
    "- –ø–æ –º–µ—Ä–µ —Ç–æ–≥–æ, –∫–∞–∫ —ç–Ω–∫–æ–¥–µ—Ä –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Ö–æ–¥–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –æ–∂–∏–¥–∞–µ—Ç—Å—è, —á—Ç–æ –≤ –∫–æ–Ω–µ—á–Ω–æ–º —Å–∫—Ä—ã—Ç–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –±—É–¥–µ—Ç —Å–æ–±—Ä–∞–Ω–∞ –≤—Å—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è;\n",
    "- –∫–æ–≥–¥–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –¥–ª–∏–Ω–Ω–µ–µ, —ç—Ç–æ–º—É –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–º—É —Å–æ—Å—Ç–æ—è–Ω–∏—é —Ç—Ä—É–¥–Ω–µ–µ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –≤—Å—é –Ω–µ–æ–±—Ö–æ–¥–∏–º—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —á–∞—Å—Ç–µ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.\n",
    "\n",
    "–ß—Ç–æ–±—ã –º–æ–¥–µ–ª—å –º–æ–≥–ª–∞ —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —á–∞—Å—Ç—è—Ö –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –±–æ–ª—å—à–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–∞ –ø—Ä–æ—Ç—è–∂–µ–Ω–∏–∏ –≤—Å–µ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è, –±—ã–ª –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è (attention mechanism).\n",
    "\n",
    "[[paper] üéì Bahdanau D., Cho K., Bengio Y. (2014). Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/seq2seq_attention.png\" width=\"600\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://blog.paperspace.com/introduction-to-neural-machine-translation-with-bahdanaus-attention/\">Introduction to Neural Machine Translation</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —ç–Ω–∫–æ–¥–µ—Ä–∞ –Ω–µ –º–µ–Ω—è–µ—Ç—Å—è, –≤—Å–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∫–∞—Å–∞—é—Ç—Å—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –¥–µ–∫–æ–¥–µ—Ä–∞.\n",
    "- –Ω–∞ –≤—Ö–æ–¥ –ø–µ—Ä–≤–æ–π —è—á–µ–π–∫–∏ RNN —Ç–∞–∫–∂–µ –ø–æ—Å—Ç—É–ø–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–∞ –Ω–∞—á–∞–ª–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ –≤–µ–∫—Ç–æ—Ä –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏–∑ —ç–Ω–∫–æ–¥–µ—Ä–∞ –∫–∞–∫ —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ;\n",
    "-  –≤–µ–∫—Ç–æ—Ä —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–∞ –Ω–∞—á–∞–ª–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ –ø–µ—Ä–≤–æ–π —è—á–µ–π–∫–∏ RNN;\n",
    "- –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∞ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–∞ –Ω–∞—á–∞–ª–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ –≤–µ–∫—Ç–æ—Ä–æ–≤ –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ –∏—Å—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å—á–∏—Ç–∞–µ—Ç—Å—è –º–µ—Ä–∞ —Å—Ö–æ–¥—Å—Ç–≤–∞ ‚Äî —ç—Ç–æ –∏ –µ—Å—Ç—å –≤–µ—Å–∞ –≤–Ω–∏–º–∞–Ω–∏—è (attention weights);\n",
    "- –µ—Å–ª–∏ –≤–µ–∫—Ç–æ—Ä—ã –Ω–µ–∫–æ—Ç–æ—Ä–æ–≥–æ —Å–ª–æ–≤–∞ —Ü–µ–ª–µ–≤–æ–≥–æ –∏ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å—Ö–æ–∂–∏, —Ç–æ –ø—Ä–∏ –ø–µ—Ä–µ–≤–æ–¥–µ (–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏) –¥–∞–Ω–Ω–æ–≥–æ —Å–ª–æ–≤–∞ –¥–µ–∫–æ–¥–µ—Ä –±–æ–ª—å—à–µ \"–æ–±—Ä–∞—â–∞–µ—Ç –≤–Ω–∏–º–∞–Ω–∏—è\" –Ω–∞ –Ω–µ–≥–æ;\n",
    "- –≤–µ–∫—Ç–æ—Ä –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —É–º–Ω–æ–∂–∞–µ—Ç—Å—è –Ω–∞ —Å–≤–æ–π –≤–µ—Å –≤–Ω–∏–º–∞–Ω–∏—è, –∑–∞—Ç–µ–º –≤—Å–µ –≤–µ–∫—Ç–æ—Ä—ã —Å–∫–ª–∞–¥—ã–≤–∞—é—Ç—Å—è, –ø–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –≤–µ–∫—Ç–æ—Ä (context vector);\n",
    "- –≤–µ–∫—Ç–æ—Ä —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–∞ –Ω–∞—á–∞–ª–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –≤–µ–∫—Ç–æ—Ä –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É—é—Ç—Å—è;\n",
    "- –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è –Ω–∞ –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π, –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è softmax;\n",
    "- –ø–æ–ª—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Ä, –¥–ª–∏–Ω–∞ –∫–æ—Ç–æ—Ä–æ–≥–æ —Ä–∞–≤–Ω–∞ –¥–ª–∏–Ω–µ —Å–ª–æ–≤–∞—Ä—è, ‚Äî —ç—Ç–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ –ø—Ä–∏ —É—Å–ª–æ–≤–∏–∏ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–∞ –Ω–∞—á–∞–ª–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è;\n",
    "- –Ω–∞ –≤—Ö–æ–¥ –≤—Ç–æ—Ä–æ–π —è—á–µ–π–∫–∏ –ø–æ—Å—Ç—É–ø–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä —Å–ª–æ–≤–∞ \"Je\" –∏ –∏–∑–º–µ–Ω–µ–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–∞ –Ω–∞—á–∞–ª–∞ –∫–∞–∫ —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ;\n",
    "- –≤–µ–∫—Ç–æ—Ä —Å–ª–æ–≤–∞ \"Je\" –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ –≤—Ç–æ—Ä–æ–π —è—á–µ–π–∫–∏ RNN;\n",
    "- —Å—á–∏—Ç–∞—é—Ç—Å—è –≤–µ—Å–∞ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è —Å–ª–æ–≤–∞ \"Je\" –∏ –≤—Å–µ—Ö —Å–ª–æ–≤ —Ü–µ–ª–µ–≤–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è;\n",
    "- $\\cdots$\n",
    "- –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ –ø–æ–ª—É—á–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/attention_equation.jpg\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://www.google.com/url?sa=i&url=https%3A%2F%2Fai.plainenglish.io%2Fintroduction-to-attention-mechanism-bahdanau-and-luong-attention-e2efd6ce22da&psig=AOvVaw1NlMR6N0XSxl6zYQwhAlYw&ust=1723824501708000&source=images&cd=vfe&opi=89978449&ved=0CAUQtaYDahcKEwiAhL3usPeHAxUAAAAAHQAAAAAQDw\">Introduction to Attention Mechanism</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –ø–æ–∑–≤–æ–ª–∏–ª –∏—Å–ø—Ä–∞–≤–∏—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã—Ö —Å–µ—Ç–∏, –æ–¥–Ω–∞–∫–æ —Ä—è–¥ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–æ–≤ –æ—Å—Ç–∞–µ—Ç—Å—è:\n",
    "- –Ω–µ —Å–ø–æ—Å–æ–±–Ω—ã –∑–∞–ø–æ–º–∏–Ω–∞—Ç—å –¥–∏—Å—Ç–∞–Ω—Ç–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏\n",
    "- –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –≤–∑—Ä—ã–≤ –∏ –∑–∞—Ç—É—Ö–∞–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞\n",
    "- –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –ø—Ä–æ–≤–æ–¥—è—Ç—Å—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –∏ –∑–∞–Ω–∏–º–∞—é—Ç –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏\n",
    "\n",
    "–ß—Ç–æ –µ—Å–ª–∏ —É–±—Ä–∞—Ç—å —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å –∏ –æ—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –≤ –∫–æ—Ç–æ—Ä–æ–π –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è, –ø–æ–ª—É—á–∏–ª–∞ –Ω–∞–∑–≤–∞–Ω–∏–µ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä.\n",
    "\n",
    "[[paper] üéì Vaswani A. et al. (2017).Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "–ö–∞–∫ –∏ –≤ —Å–ª—É—á–∞–µ —Å —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ–π —Å–µ—Ç—å—é seq2seq, –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –±–ª–æ–∫–∞ —ç–Ω–∫–æ–¥–µ—Ä–∞ –∏ –±–ª–æ–∫–∞ –¥–µ–∫–æ–¥–µ—Ä–∞.\n",
    "- –≠–Ω–∫–æ–¥–µ—Ä –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏—Å—Ö–æ–¥–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –∫–æ–¥–∏—Ä—É–µ—Ç –µ–µ.\n",
    "- –î–µ–∫–æ–¥–µ—Ä –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ü–µ–ª–µ–≤—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å —É—á–µ—Ç–æ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ —ç–Ω–∫–æ–¥–µ—Ä–∞. –í—ã—Ö–æ–¥ –∏–∑ –¥–µ–∫–æ–¥–µ—Ä–∞ —è–≤–ª—è–µ—Ç—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–∏.\n",
    "\n",
    "–ë–ª–æ–∫ —ç–Ω–∫–æ–¥–µ—Ä–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ 6 —ç–Ω–∫–æ–¥–µ—Ä–æ–≤, —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–Ω—ã—Ö –¥—Ä—É–≥ –∑–∞ –¥—Ä—É–≥–æ–º. –ë–ª–æ–∫ –¥–µ–∫–æ–¥–µ—Ä–∞ ‚Äì —ç—Ç–æ —Å—Ç–µ–∫ –¥–µ–∫–æ–¥–µ—Ä–æ–≤, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –≤ —Ç–æ–º –∂–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/transformer.jpg\" width=\"860\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í—Å–µ —ç–Ω–∫–æ–¥–µ—Ä—ã –∏–¥–µ–Ω—Ç–∏—á–Ω—ã –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ, —Ö–æ—Ç—è –∏ –∏–º–µ—é—Ç —Ä–∞–∑–Ω—ã–µ –≤–µ—Å–∞. –ö–∞–∂–¥—ã–π –º–æ–∂–Ω–æ —Ä–∞–∑–¥–µ–ª–∏—Ç—å –Ω–∞ –¥–≤–∞ –ø–æ–¥—Å–ª–æ—è.\n",
    "- –í—Ö–æ–¥–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –ø–æ—Å—Ç—É–ø–∞—é—â–∞—è –≤ —ç–Ω–∫–æ–¥–µ—Ä, —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ—Ö–æ–¥–∏—Ç —á–µ—Ä–µ–∑ —Å–ª–æ–π –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è (self-attention), –ø–æ–º–æ–≥–∞—é—â–∏–π —ç–Ω–∫–æ–¥–µ—Ä—É –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ –¥—Ä—É–≥–∏–µ —Å–ª–æ–≤–∞ –≤–æ –≤—Ö–æ–¥–Ω–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏ –≤–æ –≤—Ä–µ–º—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Å–ª–æ–≤–∞.\n",
    "- –í—ã—Ö–æ–¥ —Å–ª–æ—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è –≤ –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å –ø—Ä—è–º–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è (feed-forward neural network).\n",
    "\n",
    "–î–µ–∫–æ–¥–µ—Ä —Ç–∞–∫–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —ç—Ç–∏ –¥–≤–∞ —Å–ª–æ—è, –Ω–æ –º–µ–∂–¥—É –Ω–∏–º–∏ –µ—Å—Ç—å —Å–ª–æ–π –≤–Ω–∏–º–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–º–æ–≥–∞–µ—Ç –¥–µ–∫–æ–¥–µ—Ä—É —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞—Å—Ç—è—Ö –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ –º–µ—Ö–∞–Ω–∏–∑–º—É –≤–Ω–∏–º–∞–Ω–∏—è –≤ –º–æ–¥–µ–ª—è—Ö seq2seq)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/enc_dec_tr.png\" width=\"650\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ö–∞–∫ –∏ –≤ —Å–ª—É—á–∞–µ –¥—Ä—É–≥–∏—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤, –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ –≤—Ö–æ–¥–Ω–æ–π –∏ —Ü–µ–ª–µ–≤–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç—Å—è –≤ –≤–µ–∫—Ç–æ—Ä.\n",
    "\n",
    "–ü–æ—Å–ª–µ —Ç–æ–≥–æ –∫–∞–∫ —Å–ª–æ–≤–∞ –≤—Ö–æ–¥—è—â–µ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–ª–∏—Å—å –≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏, –∫–∞–∂–¥—ã–π –∏–∑ –Ω–∏—Ö –ø–æ –æ—Ç–¥–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–æ—Ö–æ–¥–∏—Ç —á–µ—Ä–µ–∑ —Å–ª–æ–∏ —ç–Ω–∫–æ–¥–µ—Ä–∞. –û–¥–Ω–∞ –∏–∑ –æ—Å–Ω–æ–≤–Ω—ã—Ö –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ —Å–æ—Å—Ç–æ–∏—Ç –≤ —Ç–æ–º, —á—Ç–æ –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ –∏–¥–µ—Ç –ø–æ —Å–≤–æ–µ–π —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–π —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/encoder.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú—ã –ø–æ–ø—Ä–æ–±—É–µ–º —Å–æ–∑–¥–∞—Ç—å –∏ –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ —Ç—å—é—Ç–æ—Ä–∏–∞–ª–µ:\n",
    "\n",
    "[[doc] üõ†Ô∏è The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ú–µ—Ö–∞–Ω–∏–∑–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—É—Å—Ç—å –º—ã —Ö–æ—Ç–∏–º –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ: *The animal didn't cross the street because it was too tired*. –ú–µ—Å—Ç–æ–∏–º–µ–Ω–∏–µ *it* –º–æ–∂–µ—Ç –æ—Ç–Ω–æ—Å–∏—Ç—å—Å—è –∫ —É–ª–∏—Ü–µ (*street*) –∏–ª–∏ –∫ –∂–∏–≤–æ—Ç–Ω–æ–º—É (*animal*). –ö–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–ª–æ–≤–æ *it*, —Å–ª–æ–π –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –ø–æ–º–æ–≥–∞–µ—Ç –ø–æ–Ω—è—Ç—å, —á—Ç–æ *it* –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ *animal*.\n",
    "\n",
    "–ü–æ –º–µ—Ä–µ —Ç–æ–≥–æ –∫–∞–∫ –º–æ–¥–µ–ª—å –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç \"–≤–∑–≥–ª—è–Ω—É—Ç—å\" –Ω–∞ –¥—Ä—É–≥–∏–µ —Å–ª–æ–≤–∞ –∏ –ª—É—á—à–µ –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω–æ–µ —Å–ª–æ–≤–æ. –ú–µ—Ö–∞–Ω–∏–∑–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è ‚Äì —ç—Ç–æ –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –∏—Å–ø–æ–ª—å–∑—É–µ—Ç, —á—Ç–æ–±—ã —Å–º–æ–¥–µ–ª–∏—Ä–æ–≤–∞—Ç—å \"–ø–æ–Ω–∏–º–∞–Ω–∏–µ\" –¥—Ä—É–≥–∏—Ö —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å–ª–æ–≤ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Å–ª–æ–≤–∞.\n",
    "\n",
    "–í–æ –≤—Ä–µ–º—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è *it* –≤ —ç–Ω–∫–æ–¥–µ—Ä–µ ‚Ññ5 —á–∞—Å—Ç—å –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ *The animal* –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ñ—Ä–∞–≥–º–µ–Ω—Ç –µ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è *it*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/self_attention.png\" width=\"400\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### –ù–∞ –ø—Ä–∏–º–µ—Ä–µ –≤–µ–∫—Ç–æ—Ä–æ–≤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è: –≤–µ–∫—Ç–æ—Ä –∑–∞–ø—Ä–æ—Å–∞ $query$, –≤–µ–∫—Ç–æ—Ä –∫–ª—é—á–∞ $key$ –∏ –≤–µ–∫—Ç–æ—Ä –∑–Ω–∞—á–µ–Ω–∏—è $value$. –û–Ω–∏ —Å–æ–∑–¥–∞—é—Ç—Å—è —Å –ø–æ–º–æ—â—å—é —É–º–Ω–æ–∂–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ —Å–ª–æ–≤–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ —Ç—Ä–∏ –º–∞—Ç—Ä–∏—Ü—ã –≤–µ—Å–æ–≤ (–ª–∏–Ω–µ–π–Ω—ã—Ö —Å–ª–æ—è) $W^Q, W^K, W^V$. –†–∞–∑–º–µ—Ä –Ω–æ–≤—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ ‚Äì 64, —Ä–∞–∑–º–µ—Ä –∏—Å—Ö–æ–¥–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ ‚Äì 512.\n",
    "\n",
    "$q_i=x_iW^Q$\n",
    "\n",
    "$k_i=x_iW^K$\n",
    "\n",
    "$v_i=x_iW^V$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/vector_attention.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–∞–ª–µ–µ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è $score$ –¥–ª—è $i$-–≥–æ —Å–ª–æ–≤–∞ –ø–æ –æ—Ç–Ω–æ—à–µ–Ω–∏—é –∫ –∫–∞–∂–¥–æ–º—É —Å–ª–æ–≤—É –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏. –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ –Ω—É–∂–Ω–æ —Å—Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ –¥—Ä—É–≥–∏—Ö —á–∞—Å—Ç—è—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤–æ –≤—Ä–µ–º—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è —Å–ª–æ–≤–∞ –≤ $i$-–π –ø–æ–∑–∏—Ü–∏–∏. –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –ø–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é —Å–∫–∞–ª—è—Ä–Ω–æ–≥–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–∞ –∑–∞–ø—Ä–æ—Å–∞ $q$ $i$-–≥–æ —Å–ª–æ–≤–∞ –∏ –≤–µ–∫—Ç–æ—Ä–∞ –∫–ª—é—á–∞ $k$ –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞.\n",
    "\n",
    "$score_{ij}=q_i \\cdot k_j$\n",
    "\n",
    "üìå –ö–∞–∫–∏–µ –≤–µ–∫—Ç–æ—Ä—ã –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–º–Ω–æ–∂–∏—Ç—å, —á—Ç–æ–±—ã –ø–æ—Å—á–∏—Ç–∞—Ç—å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤–Ω–∏–º–∞–Ω–∏—è —Å–ª–æ–≤–∞ *Thinking* –ø–æ –æ—Ç–Ω–æ—à–µ–Ω–∏—é –∫ —Å–ª–æ–≤—É *Machines*? –ü–æ –æ—Ç–Ω–æ—à–µ–Ω–∏—é –∫ —Å–∞–º–æ–º—É —Å–µ–±–µ?\n",
    "\n",
    "–ù–∞ —Å–ª–µ–¥—É—é—â–µ–º —à–∞–≥–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –¥–µ–ª—è—Ç—Å—è –Ω–∞ –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã–π –∫–æ—Ä–µ–Ω—å –∏–∑ $d_k$ ‚Äì —Ä–∞–∑–º–µ—Ä–∞ –≤–µ–∫—Ç–æ—Ä–æ–≤ –∫–ª—é—á–∞ $k$. –ö –ø–æ–ª—É—á–∏–≤—à–∏–º—Å—è –∑–Ω–∞—á–µ–Ω–∏—è–º –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —Ñ—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ softmax, —á—Ç–æ–±—ã –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –≤ —Å—É–º–º–µ –¥–∞–≤–∞–ª–∏ 1. –ü–æ–ª—É—á–µ–Ω–Ω—ã–π —Å–æ—Ñ—Ç–º–∞–∫—Å-–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –≤ –∫–∞–∫–æ–π –º–µ—Ä–µ –∫–∞–∂–¥–æ–µ –∏–∑ —Å–ª–æ–≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è \"—Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è\" –Ω–∞ –¥—Ä—É–≥–æ–º —Å–ª–æ–≤–µ.\n",
    "\n",
    "$softmax.score_{ij}=softmax(\\frac{score_i}{\\sqrt d_k})$\n",
    "\n",
    "–ü–æ—Å–ª–µ —ç—Ç–æ–≥–æ –∫–∞–∂–¥—ã–π –≤–µ–∫—Ç–æ—Ä –∑–Ω–∞—á–µ–Ω–∏—è $v$ —É–º–Ω–æ–∂–∞–µ—Ç—Å—è –Ω–∞ —Å–æ—Ñ—Ç–º–∞–∫—Å-–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç, –ø–æ–ª—É—á–∞–µ–º –≤–∑–≤–µ—à–µ–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã. –ò–¥–µ—è –≤ —Ç–æ–º, —á—Ç–æ –Ω—É–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π –∑–Ω–∞—á–µ–Ω–∏—è —Å–ª–æ–≤, –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –º—ã —Ñ–æ–∫—É—Å–∏—Ä—É–µ–º—Å—è, –∏ –æ—Ç–≤–µ—Å—Ç–∏ –Ω–∞ –≤—Ç–æ—Ä–æ–π –ø–ª–∞–Ω –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å–ª–æ–≤–∞ (—É–º–Ω–æ–∂–∏–≤ –∏—Ö –Ω–∞ –Ω–µ–±–æ–ª—å—à–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è, –Ω–∞–ø—Ä–∏–º–µ—Ä, 0.001). –ó–∞—Ç–µ–º  –≤–∑–≤–µ—à–µ–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã —Å–∫–ª–∞–¥—ã–≤–∞—é—Ç—Å—è. –†–µ–∑—É–ª—å—Ç–∞—Ç (–≤–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞) –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –≤—ã—Ö–æ–¥ —Å–ª–æ—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è $i$-–≥–æ —Å–ª–æ–≤–∞.\n",
    "\n",
    "$sum_i=\\sum_{j=1}^nv_j \\cdot softmax.score_{ij}$\n",
    "\n",
    "–ü–æ–ª—É—á–µ–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è –¥–∞–ª—å—à–µ –≤ –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å –ø—Ä—è–º–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/attention_example.png\" width=\"600\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### –ù–∞ –ø—Ä–∏–º–µ—Ä–µ –º–∞—Ç—Ä–∏—Ü"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ, –º–∞—Ç—Ä–∏—Ü—ã –∑–∞–ø—Ä–æ—Å–∞ $Q$, –∫–ª—é—á–∞ $K$ –∏ –∑–Ω–∞—á–µ–Ω–∏—è $V$ –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è —Å –ø–æ–º–æ—â—å—é —É–º–Ω–æ–∂–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –º–∞—Ç—Ä–∏—Ü—ã $X$ –Ω–∞ –º–∞—Ç—Ä–∏—Ü—ã –≤–µ—Å–æ–≤ (–ª–∏–Ω–µ–π–Ω—ã–µ —Å–ª–æ–∏) $W^Q, W^K, W^V$. –ö–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ –≤ –º–∞—Ç—Ä–∏—Ü–µ $X$ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Å–ª–æ–≤—É –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/matrix_attention.png\" width=\"450\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ—Å–ª–µ–¥—É—é—â–∏–µ —ç—Ç–∞–ø—ã –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤—ã—Ö–æ–¥–∞ —Å–ª–æ—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –º–æ–≥—É—Ç –±—ã—Ç—å –æ—Ç—Ä–∞–∂–µ–Ω—ã –≤ –æ–¥–Ω–æ–π —Ñ–æ—Ä–º—É–ª–µ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/attention_score.png\" width=\"600\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–∞–ø–∏—à–µ–º —Ñ—É–Ω–∫—Ü–∏—é `attention` –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è.\n",
    "\n",
    "$Attention(Q, K, V) = softmax\\large(\\frac{QK^T}{\\sqrt{d_k}})V$\n",
    "\n",
    "–î–ª—è —É–º–Ω–æ–∂–µ–Ω–∏—è –º–∞—Ç—Ä–∏—Ü $Q, K, V$ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–µ—Ç–æ–¥ `torch.matmul()`.\n",
    "\n",
    "–¢—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Å—É—â–µ—Å—Ç–≤–ª—è–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–∞ `.transpose`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"\"\"\n",
    "    Compute 'Scaled Dot Product Attention'\n",
    "    \"\"\"\n",
    "    d_k = query.size(-1)\n",
    "    # compute the attention scores by using torch.matmul\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    # compute the result as the values weighted by attention probabilities (again, using torch.matmul)\n",
    "    result = torch.matmul(p_attn, value)\n",
    "    return result, p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, attentions = attention(\n",
    "    torch.tensor([[0, 0], [0, 1], [1, 1]], dtype=torch.float),\n",
    "    torch.tensor([[100, 0], [0, 100], [0, 0]], dtype=torch.float),\n",
    "    torch.tensor([[1, 0], [0, 1], [0, 0]], dtype=torch.float),\n",
    ")\n",
    "print(results)\n",
    "print(attentions)\n",
    "\n",
    "assert np.allclose(results[0].numpy(), [1/3, 1/3])  # the first query attends to all keys equally\n",
    "assert np.allclose(results[1].numpy(), [0, 1])      # the second query attends only to the second key\n",
    "assert np.allclose(results[2].numpy(), [1/2, 1/2])  # the third query attends to the first and second key equally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–µ –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è —Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤—É–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è (multi-headed attention). –≠—Ç–æ –ø–æ–≤—ã—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —Å–ª–æ–≤–∞—Ö. –í —Å–ª—É—á–∞–µ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –º—ã —Ä–∞—Å–ø–æ–ª–∞–≥–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–º–∏ –º–∞—Ç—Ä–∏—Ü–∞–º–∏ –≤–µ—Å–æ–≤ $W^Q, W^K, W^V$  –¥–ª—è –∫–∞–∂–¥–æ–π \"–≥–æ–ª–æ–≤—ã\", —á—Ç–æ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –¥–∞–µ—Ç —Ä–∞–∑–Ω—ã–µ –º–∞—Ç—Ä–∏—Ü—ã $Q, K, V$. –ö–∞–∫ –∏ —Ä–∞–Ω–µ–µ, –º–∞—Ç—Ä–∏—Ü–∞ $X$ —É–º–Ω–æ–∂–∞–µ—Ç—Å—è –Ω–∞ –≤–µ—Å–∞ $W^Q, W^K, W^V$ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –º–∞—Ç—Ä–∏—Ü $Q, K, V$.\n",
    "\n",
    "–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 8 \"–≥–æ–ª–æ–≤\" –≤–Ω–∏–º–∞–Ω–∏—è, —Ç–∞–∫ —á—Ç–æ –≤ –∏—Ç–æ–≥–µ —É –Ω–∞—Å –ø–æ–ª—É—á–∞–µ—Ç—Å—è 8 –Ω–∞–±–æ—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —ç–Ω–∫–æ–¥–µ—Ä–∞ –∏ –¥–µ–∫–æ–¥–µ—Ä–∞. –°–¥–µ–ª–∞–≤ —Ç–µ –∂–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è 8 —Ä–∞–∑ —Å —Ä–∞–∑–Ω—ã–º–∏ –º–∞—Ç—Ä–∏—Ü–∞–º–∏ –≤–µ—Å–æ–≤, –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –ø–æ–ª—É—á–∏–º 8 —Ä–∞–∑–Ω—ã—Ö –º–∞—Ç—Ä–∏—Ü."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/multihead_attention.png\" width=\"600\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–¥–Ω–∞–∫–æ —Å–ª–æ–π —Å–µ—Ç–∏ –ø—Ä—è–º–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –Ω–µ –æ–∂–∏–¥–∞–µ—Ç, —á—Ç–æ –∫ –Ω–µ–º—É –ø–æ—Å—Ç—É–ø–∏—Ç 8 –º–∞—Ç—Ä–∏—Ü ‚Äì –æ–Ω –∂–¥–µ—Ç –≤—Å–µ–≥–æ –æ–¥–Ω—É, –≤ –∫–æ—Ç–æ—Ä—É—é –Ω–∞–º –∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Å–∂–∞—Ç—å –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –º–∞—Ç—Ä–∏—Ü—ã. –ß—Ç–æ–±—ã —ç—Ç–æ —Å–¥–µ–ª–∞—Ç—å, –º–æ–∂–Ω–æ –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä–æ–≤–∞—Ç—å –∏ –∑–∞—Ç–µ–º —É–º–Ω–æ–∂–∏—Ç—å –∏—Ö –Ω–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–µ—Å–∞ –º–∞—Ç—Ä–∏—Ü—ã $W^O$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/concatenate.png\" width=\"800\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú—ã –º–æ–∂–µ–º –æ—Ç–æ–±—Ä–∞–∑–∏—Ç—å –≤—Å–µ \"–≥–æ–ª–æ–≤—ã\" –≤–Ω–∏–º–∞–Ω–∏—è –Ω–∞ –æ–¥–Ω–æ–π –∫–∞—Ä—Ç–∏–Ω–∫–µ.\n",
    "\n",
    "üìå –ö–∞–∫–∏–µ –≥–æ–ª–æ–≤—ã –±–æ–ª—å—à–µ —Ñ–æ–∫—É—Å–∏—Ä—É—é—Ç—Å—è –Ω–∞ –∫–∞–∫–∏—Ö —Å–ª–æ–≤–∞—Ö?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/multihead_viz.png\" width=\"400\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–ª—è —Å–æ–∑–¥–∞–Ω–∏–µ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º –∫–ª–∞—Å—Å `MultiHeadedAttention`.\n",
    "\n",
    "–ö–∞–∂–¥–∞—è –≥–æ–ª–æ–≤–∞ –≤–Ω–∏–º–∞–Ω–∏—è –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –≤—ã—á–∏—Å–ª—è–µ—Ç –≤–Ω–∏–º–∞–Ω–∏–µ (—Å–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ) –¥–ª—è —Å–≤–æ–∏—Ö –º–∞—Ç—Ä–∏—Ü $Q,K,V$. –°–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, –∫–∞–∂–¥–∞—è –≥–æ–ª–æ–≤–∞ –º–æ–∂–µ—Ç \"–æ–±—Ä–∞—â–∞—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ\" –Ω–∞ —Ä–∞–∑–Ω—ã–µ —á–∞—Å—Ç–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.\n",
    "\n",
    "$MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch\n",
    "        x, self.attn = attention(query, key, value, mask=mask,\n",
    "                                 dropout=self.dropout)\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear\n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò–∑-–∑–∞ —Ç–æ–≥–æ, —á—Ç–æ –º—ã –∏–∑–±–∞–≤–∏–ª–∏ –æ—Ç —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏, –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ—Ä—è–¥–∫–µ —Å–ª–æ–≤ –ø–µ—Ä–µ—Å—Ç–∞–ª–∞ —É—á–∏—Ç—ã–≤–∞—Ç—å—Å—è –º–æ–¥–µ–ª—å—é. –í –∫–ª–∞—Å—Å–µ `MultiHeadAttention` –æ–ø–µ—Ä–∞—Ü–∏–∏ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –∫ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ 2 (–ø—Ä–∏–∑–Ω–∞–∫–∏ —Å–ª–æ–≤), –Ω–æ –Ω–µ –∫ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ 1 (—Å–ª–æ–≤–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏).\n",
    "\n",
    "–î–ª—è —Ç–æ–≥–æ —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –ø–æ–Ω–∏–º–∞–ª–∞ –ø–æ—Ä—è–¥–æ–∫ —Å–ª–æ–≤, –≤–≤–æ–¥—è—Ç—Å—è –≤–µ–∫—Ç–æ—Ä—ã –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–∑–∏—Ü–∏–∏ (positional encoding). –û–Ω–∏ —Å—É–º–º–∏—Ä—É—é—Ç—Å—è —Å –≤–µ–∫—Ç–æ—Ä–∞–º–∏ –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/positional_encoding.png\" width=\"850\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ–∑–∏—Ü–∏–∏ –∫–æ–¥–∏—Ä—É–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é —Ç—Ä–∏–≥–æ–Ω–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö —Ñ—É–Ω–∫—Ü–∏–π: —Å–∏–Ω—É—Å–∞ –∏ –∫–æ—Å–∏–Ω—É—Å–∞.\n",
    "\n",
    "–ü—É—Å—Ç—å –µ—Å—Ç—å –≤—Ö–æ–¥–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–ª–∏–Ω—ã $L$, –Ω—É–∂–Ω–æ –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –ø–æ–∑–∏—Ü–∏—é –Ω–µ–∫–æ—Ç–æ—Ä–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ $pos$.\n",
    "\n",
    "$pos$ ‚Äî –∏–Ω–¥–µ–∫—Å —Å–ª–æ–≤–∞ –≤ –∏—Å—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, $0 \\leq pos < L$\n",
    "\n",
    "$d$ ‚Äî —Ä–∞–∑–º–µ—Ä —ç–º–µ–¥–¥–∏–Ω–≥–æ–≤ –≤—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è\n",
    "\n",
    "$i$ ‚Äî –Ω–æ–º–µ—Ä –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –≤—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è, $0 \\leq i < \\large \\frac{d}{2}$\n",
    "\n",
    "$p_0$ ‚Äî –ø–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, $d$ ‚Äî –¥–ª–∏–Ω–∞ –≤–µ–∫—Ç–æ—Ä–∞, $i$ ‚Äî –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –≤–µ–∫—Ç–æ—Ä–∞."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/pe1.png\" width=\"500\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://www.youtube.com/watch?v=dichIcUZfOw\">Visual Guide to Transformer Neural Networks</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–ª–æ–≤–∞ —Å —Ä–∞–∑–Ω—ã–º –Ω–æ–º–µ—Ä–æ–º –ø–æ–∑–∏—Ü–∏–∏ –±—É–¥—É—Ç –∏–º–µ—Ç—å —Ä–∞–∑–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ –æ—Å–∏ $y$. –û–¥–Ω–∞–∫–æ –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –ø–æ–∑–∏—Ü–∏–π ($p_0$ –∏ $p_6$) –∑–Ω–∞—á–µ–Ω–∏—è —Å–æ–≤–ø–∞–¥–∞—é—Ç, –ø–æ—Å–∫–æ–ª—å–∫—É —Å–∏–Ω—É—Å ‚Äî –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è —Ñ—É–Ω–∫—Ü–∏—è."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/pe2.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://www.youtube.com/watch?v=dichIcUZfOw\">Visual Guide to Transformer Neural Networks</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ–ª—É—á–∏—Ç—å —Ä–∞–∑–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –º–æ–∂–Ω–æ –±–ª–∞–≥–æ–¥–∞—Ä—è –ø–∞—Ä–∞–º–µ—Ç—Ä—É $i$. –ü—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ –∑–Ω–∞—á–µ–Ω–∏—è $i$ –º–µ–Ω—è–µ—Ç—Å—è —á–∞—Å—Ç–æ—Ç–∞, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –ø–æ–ª—É—á–µ–Ω–∏—é —Ä–∞–∑–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –¥–ª—è $p_0$ –∏ $p_6$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/pe3.png\" width=\"800\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://www.youtube.com/watch?v=dichIcUZfOw\">Visual Guide to Transformer Neural Networks</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–æ–∑–¥–∞–¥–∏–º –∫–ª–∞—Å—Å `PositionalEncoding` –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–∑–∏—Ü–∏–∏.\n",
    "\n",
    "$PE_{(pos, 2i)}=sin(pos/10000^{2i/d_{model}})$\n",
    "\n",
    "$PE_{(pos, 2i+1)}=cos(pos/10000^{2i/d_{model}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "pe = PositionalEncoding(20, 0)\n",
    "y = pe.forward(Variable(torch.zeros(1, 100, 20)))\n",
    "plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())\n",
    "plt.legend([\"dim %d\"%p for p in [4,5,6,7]])\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –°–ª–æ–∏ Embedding –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ –¥—Ä—É–≥–∏–º –º–æ–¥–µ–ª—è–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π, –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—É—á–µ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –∏ –≤—ã—Ö–æ–¥–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä—ã —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ $d_{\\text{model}}$. –ú—ã —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ª–∏–Ω–µ–π–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏ —Ñ—É–Ω–∫—Ü–∏—é softmax –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–µ–∫–æ–¥–µ—Ä–∞ –≤ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞. –í –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏ –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–¥–Ω—É –∏ —Ç—É –∂–µ –≤–µ—Å–æ–≤—É—é –º–∞—Ç—Ä–∏—Ü—É –º–µ–∂–¥—É —Å–ª–æ—è–º–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ –ª–∏–Ω–µ–π–Ω—ã–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ–º, –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã–º –¥–æ softmax. –í —Å–ª–æ—è—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –º—ã —É–º–Ω–æ–∂–∞–µ–º —ç—Ç–∏ –≤–µ—Å–∞ –Ω–∞ $\\sqrt{d_{\\text{model}}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C–µ—Ç—å –ø—Ä—è–º–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ó–∞ —Å–ª–æ–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –≤ —ç–Ω–∫–æ–¥–µ—Ä–µ –∏ –¥–µ–∫–æ–¥–µ—Ä–µ —Å–ª–µ–¥—É–µ—Ç —Å–µ—Ç—å –ø—Ä—è–º–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è. –û–Ω–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –¥–≤—É—Ö –ª–∏–Ω–µ–π–Ω—ã—Ö —Å–ª–æ–µ–≤ –∏ —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ ReLU –º–µ–∂–¥—É –Ω–∏–º–∏. –†–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω—ã—Ö –∏ –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö ‚Äî 512, –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π —Ä–∞–∑–º–µ—Ä ‚Äî 2048 (–≤ 4 —Ä–∞–∑–∞ –±–æ–ª—å—à–µ)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –≠–Ω–∫–æ–¥–µ—Ä"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú–æ–∂–µ–º –ø–µ—Ä–µ–π—Ç–∏ –∫ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –±–ª–æ–∫–∞ —ç–Ω–∫–æ–¥–µ—Ä–∞. –û–Ω —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —Å—Ç–µ–∫–∞ $N=6$ –∏–¥–µ–Ω—Ç–∏—á–Ω—ã—Ö —Å–ª–æ–µ–≤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/enc_details.png\" width=\"400\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ö–∞–∂–¥—ã–π –ø–æ–¥—Å–ª–æ–π (–≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ –≤–Ω–∏–º–∞–Ω–∏–µ, –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π) –≤–∫–ª—é—á–∞–µ—Ç –æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ (residual connection), –∑–∞ –∫–æ—Ç–æ—Ä—ã–º —Å–ª–µ–¥—É–µ—Ç —ç—Ç–∞–ø –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–ª–æ—è ([[doc] üõ†Ô∏è layer normalization](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.htm)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/connection_normalization.png\" width=\"500\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–≠–º–±–µ–¥–¥–∏–Ω–≥ –∫–∞–∂–¥–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥—É–±–ª–∏—Ä—É–µ—Ç—Å—è: –æ–¥–Ω–∞ –∫–æ–ø–∏—è –ø–æ—Å—Ç—É–ø–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥ —Å–ª–æ—è (–≤–Ω–∏–º–∞–Ω–∏—è –∏–ª–∏ —Å–µ—Ç–∏ –ø—Ä—è–º–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è), –¥—Ä—É–≥–∞—è –∫–æ–ø–∏—è –Ω–µ –º–µ–Ω—è–µ—Ç—Å—è –∏ –ø—Ä–∏–±–∞–≤–ª—è–µ—Ç—Å—è –∫ –≤—ã—Ö–æ–¥—É —Å–ª–æ—è. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –∑–∞—Ç—É—Ö–∞–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞.\n",
    "\n",
    "–ü–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è. –í–µ—Å–∞ –º–æ–≥—É—Ç –ø—Ä–∏–Ω–∏–º–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Ä–∞–∑–Ω–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ, –Ω–æ —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏–µ –æ–¥–∏–Ω–∞–∫–æ–≤–∞—è. –ò–∑-–∑–∞ —ç—Ç–æ–≥–æ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç—å —Å –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º –≤–µ—Å–æ–≤.\n",
    "\n",
    "–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º: –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∫ –ø–µ—Ä–≤–æ–º—É –ø—Ä–∏–∑–Ω–∞–∫—É –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ –≤ –ø–µ—Ä–≤–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏, –∑–∞—Ç–µ–º –∫–æ –≤—Ç–æ—Ä–æ–º—É –ø—Ä–∏–∑–Ω–∞–∫—É –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ –≤ –ø–µ—Ä–≤–æ–º –ø—Ä–µ–¥–æ–∂–µ–Ω–∏–∏ –∏ —Ç.–¥.\n",
    "\n",
    "–°—Ä–µ–¥–Ω–µ–µ: $\\mu_i =\\frac{1}{m}\\sum_{j=1}^mx_{ij}$\n",
    "\n",
    "–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: $\\sigma_i = \\frac{1}{m}\\sum_{j=1}^m(x_{ij}-\\mu_i)$\n",
    "\n",
    "–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: $\\hat x_{ij} = \\frac {x_{ij}-\\mu_i}{\\sigma_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-content/L04/out/layer_normalization.png\" width=\"400\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –≤–æ–∫—Ä—É–≥ –∫–∞–∂–¥–æ–≥–æ –∏–∑ –¥–≤—É—Ö –ø–æ–¥—Å–ª–æ–µ–≤, –¥–∞–ª–µ–µ —Å–ª–µ–¥—É–µ—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–ª–æ—è."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Return a2 * x_normalized + b2,\n",
    "        where x_normalized is calculated by subtracting row-wise means from x and dividing the result by row-wise standard deviation + eps.\n",
    "        standard deviation is calculated with Bessel's correction (the default in Pytorch)\n",
    "        \"\"\"\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln = LayerNorm(2, eps=0)\n",
    "with torch.no_grad():\n",
    "    result = ln(torch.tensor([[0.0, 1], [100, 101], [100, 200]])).numpy()\n",
    "\n",
    "# becasue of Bessel's correction, standard deviation is pulled to 0. Here we un-pull it back.\n",
    "print(result)\n",
    "result_unnormalized = result / np.sqrt(0.5)\n",
    "assert (result_unnormalized[:, 0] == -1).all()\n",
    "assert (result_unnormalized[:, 1] == 1).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–æ –µ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∫–∞–∂–¥–æ–≥–æ –ø–æ–¥—Å–ª–æ—è ‚Äî $\\mathrm{LayerNorm}(x + \\mathrm{Sublayer}(x))$, –≥–¥–µ $\\mathrm{Sublayer}(x)$ ‚Äî —ç—Ç–æ —Ñ—É–Ω–∫—Ü–∏—è, —Ä–µ–∞–ª–∏–∑—É–µ–º–∞—è —Å–∞–º–∏–º –ø–æ–¥—Å–ª–æ–µ–º. –ú—ã –ø—Ä–∏–º–µ–Ω—è–µ–º dropout –∫ –≤—ã—Ö–æ–¥–Ω—ã–º –¥–∞–Ω–Ω—ã–º –∫–∞–∂–¥–æ–≥–æ –ø–æ–¥—Å–ª–æ—è, –ø—Ä–µ–∂–¥–µ —á–µ–º –æ–Ω–∏ —Å–∫–ª–∞–¥—ã–≤–∞—é—Ç—Å—è –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É—é—Ç—Å—è.\n",
    "\n",
    "–ß—Ç–æ–±—ã —É–ø—Ä–æ—Å—Ç–∏—Ç—å —ç—Ç–∏ –æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ, –≤—Å–µ –ø–æ–¥—Å–ª–æ–∏ –≤ –º–æ–¥–µ–ª–∏, –∞ —Ç–∞–∫–∂–µ —Å–ª–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤—ã–¥–∞—é—Ç –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é $d_{\\text{model}}=512$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ö–∞–∂–¥—ã–π —Å–ª–æ–π —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –¥–≤—É—Ö –ø–æ–¥—Å–ª–æ–µ–≤. –ü–µ—Ä–≤—ã–π –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –º–µ—Ö–∞–Ω–∏–∑–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è, –∞ –≤—Ç–æ—Ä–æ–π ‚Äî –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—É—é —Å–µ—Ç—å –ø—Ä—è–º–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –î–µ–∫–æ–¥–µ—Ä"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ë–ª–æ–∫ –¥–µ–∫–æ–¥–µ—Ä–∞ —É—Å—Ç—Ä–æ–µ–Ω –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–µ, –Ω–æ –µ—Å—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ—Ç–ª–∏—á–∏–π.\n",
    "- –î–µ–∫–æ–¥–µ—Ä –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –¥–≤–∞ –∞—Ä–≥—É–º–µ–Ω—Ç–∞: —Ü–µ–ª–µ–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∏ –≤—ã—Ö–æ–¥ —ç–Ω–∫–æ–¥–µ—Ä–∞.\n",
    "- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–≤–∞ —Å–ª–æ—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è\n",
    "  - –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ü–µ–ª–µ–≤–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è\n",
    "  - –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –º–µ–∂–¥—É —ç–Ω–∫–æ–¥–µ—Ä–æ–º –∏ –¥–µ–∫–æ–¥–µ—Ä–æ–º –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ü–µ–ª–µ–≤–æ–≥–æ –∏ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è\n",
    "- –í—Ç–æ—Ä–æ–π —Å–ª–æ–π –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –≤ –∫–∞—á–µ—Å—Ç–≤–µ –º–∞—Ç—Ä–∏—Ü $K$ –∏ $V$ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤—ã—Ö–æ–¥ —ç–Ω–∫–æ–¥–µ—Ä–∞.\n",
    "\n",
    "–î–µ–∫–æ–¥–µ—Ä –≤—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–¥–∞—á—É —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–ª–æ–≤–∞. –°–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ü–µ–ª–µ–≤–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –æ–Ω –Ω–µ –º–æ–∂–µ—Ç \"–∑–∞–≥–ª—è–¥—ã–≤–∞—Ç—å\" –≤–ø–µ—Ä–µ–¥ –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π –µ—â–µ –Ω–µ –±—ã–ª —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω. –ü–æ—ç—Ç–æ–º—É –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –ø–æ–∑–∏—Ü–∏–π –ø–æ—Å–ª–µ —Ç–µ–∫—É—â–µ–π, –∏—Ö –≤–µ–∫—Ç–æ—Ä—ã –∑–∞–ø–æ–ª–Ω—è—é—Ç—Å—è –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ `-inf`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/enc_dec_details.png\" width=\"800\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–µ–∫–æ–¥–µ—Ä —Ç–∞–∫–∂–µ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —Å—Ç–µ–∫–∞ –∏–∑ $N=6$ –∏–¥–µ–Ω—Ç–∏—á–Ω—ã—Ö —Å–ª–æ–µ–≤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –∫ –¥–≤—É–º –ø–æ–¥—Å–ª–æ—è–º –Ω–∞ –∫–∞–∂–¥–æ–º —Å–ª–æ–µ —ç–Ω–∫–æ–¥–µ—Ä–∞, –¥–µ–∫–æ–¥–µ—Ä —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç—Ä–µ—Ç–∏–π –ø–æ–¥—Å–ª–æ–π, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–∏–º–µ–Ω—è–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –∫ –≤—ã—Ö–æ–¥–Ω—ã–º –¥–∞–Ω–Ω—ã–º —ç–Ω–∫–æ–¥–µ—Ä–∞. –ö–∞–∫ –∏ –≤ —Å–ª—É—á–∞–µ —Å —ç–Ω–∫–æ–¥–µ—Ä–æ–º, –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Å—Ç–∞—Ç–æ—á–Ω—ã–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –≤–æ–∫—Ä—É–≥ –∫–∞–∂–¥–æ–≥–æ –∏–∑ –ø–æ–¥—Å–ª–æ–µ–≤ —Å –ø–æ—Å–ª–µ–¥—É—é—â–µ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–ª–æ—è."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"Follow Figure 1 (right) for connections.\"\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú—ã —Ç–∞–∫–∂–µ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –ø–æ–¥—Å–ª–æ–π –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –≤ —Å—Ç–µ–∫–µ –¥–µ–∫–æ–¥–µ—Ä–∞, –¥–æ–±–∞–≤–ª—è—è –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ. –≠—Ç–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ, —á—Ç–æ–±—ã –Ω–µ —É—á–∏—Ç—ã–≤–∞—Ç—å —Ç–æ–∫–µ–Ω—ã –≤ —Å–ª–µ–¥—É—é—â–∏—Ö –ø–æ–∑–∏—Ü–∏—è—Ö –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Ç–µ–∫—É—â–µ–π –∏ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –ø–æ–∑–∏—Ü–∏–π. –ú–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –ø–æ–∑–∏—Ü–∏–∏ $i$ –º–æ–≥—É—Ç –∑–∞–≤–∏—Å–µ—Ç—å —Ç–æ–ª—å–∫–æ –æ—Ç –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –ø–æ–∑–∏—Ü–∏—è—Ö, –º–µ–Ω—å—à–∏—Ö, —á–µ–º $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú–∞—Å–∫–∞ –≤–Ω–∏–º–∞–Ω–∏—è –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø–æ–∑–∏—Ü–∏—é (—Å—Ç—Ä–æ–∫–∞), –Ω–∞ –∫–æ—Ç–æ—Ä—É—é —Ä–∞–∑—Ä–µ—à–µ–Ω–æ —Å–º–æ—Ç—Ä–µ—Ç—å –∫–∞–∂–¥–æ–º—É —Å–ª–æ–≤—É (—Å—Ç–æ–ª–±–µ—Ü). –°–ª–æ–≤–∞ –±–ª–æ–∫–∏—Ä—É—é—Ç—Å—è –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –º–æ–∂–Ω–æ –±—ã–ª–æ –æ–±—Ä–∞—Ç–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –±—É–¥—É—â–∏–µ —Å–ª–æ–≤–∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(subsequent_mask(20)[0])\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–ø—Ä–µ–¥–µ–ª–∏–º —Ñ—É–Ω–∫—Ü–∏—é –æ—Ç –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –ø–æ–ª–Ω–æ–π –º–æ–¥–µ–ª–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/transformer_full.png\" width=\"1000\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html\">Sequence to Sequence and Attention</a></em></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture.\n",
    "    Base for this and many other models.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        return self.decode(self.encode(src, src_mask), src_mask,\n",
    "                            tgt, tgt_mask)\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "\n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    \"Construct a model from hyperparameters\"\n",
    "    c = copy.deepcopy  # use it for attn, ffn, and position in the model layers\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    # insert correct arguments into the EncoderDecoder constructor.\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab)\n",
    "    )\n",
    "\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small example model.\n",
    "tmp_model = make_model(10, 30, 2)\n",
    "assert sum(p.numel() for p in tmp_model.parameters()) == 14750750"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.numel() for p in tmp_model.encoder.parameters()) + sum(p.numel() for p in tmp_model.decoder.parameters()) + sum(p.numel() for p in tmp_model.tgt_embed.parameters()) + sum(p.numel() for p in tmp_model.src_embed.parameters()) + sum(p.numel() for p in tmp_model.generator.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è —Ä–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –î–µ–ª–µ–Ω–∏–µ –Ω–∞ –±–∞—Ç—á–∏ –∏ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í–Ω–∞—á–∞–ª–µ –æ–ø—Ä–µ–¥–µ–ª–∏–º —Ñ–æ—Ä–º–∞—Ç –±–∞—Ç—á–∞, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–¥–µ—Ä–∂–∏—Ç –∏—Å—Ö–æ–¥–Ω–æ–µ –∏ —Ü–µ–ª–µ–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, –∞ —Ç–∞–∫–∂–µ —Å–æ–∑–¥–∞–Ω–∏–µ –º–∞—Å–æ–∫."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"Object for holding a batch of data with mask during training.\"\n",
    "    def __init__(self, src, trg=None, pad=0):\n",
    "        self.src = src\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        if trg is not None:\n",
    "            self.trg = trg[:, :-1]\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            self.trg_mask = \\\n",
    "                self.make_std_mask(self.trg, pad)\n",
    "            self.ntokens = (self.trg_y != pad).data.sum()\n",
    "\n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & Variable(\n",
    "            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "        return tgt_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–æ–∑–¥–∞–¥–∏–º –±–∞—Ç—á–∏ —Å –ø–æ–º–æ—â—å—é —Ç–µ–∫—Å—Ç–æ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ torch, –∫–æ—Ç–æ—Ä–∞—è –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ —Ä–∞–∑–º–µ—Ä –Ω–∞—à–µ–≥–æ –±–∞—Ç—á–∞ –ø–æ—Å–ª–µ –ø–∞–¥–¥–∏–Ω–≥–∞ –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –±–∞—Ç—á–∞ –Ω–µ –ø—Ä–µ–≤—ã—Å–∏—Ç –ø–æ—Ä–æ–≥–æ–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global max_src_in_batch, max_tgt_in_batch\n",
    "def batch_size_fn(new, count, sofar):\n",
    "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
    "    global max_src_in_batch, max_tgt_in_batch\n",
    "    if count == 1:\n",
    "        max_src_in_batch = 0\n",
    "        max_tgt_in_batch = 0\n",
    "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
    "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
    "    src_elements = count * max_src_in_batch\n",
    "    tgt_elements = count * max_tgt_in_batch\n",
    "    return max(src_elements, tgt_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ó–∞—Ç–µ–º —Å–æ–∑–¥–∞–¥–∏–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, –ø–æ–¥—Å—á–µ—Ç–∞ –∑–Ω–∞—á–µ–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def run_epoch(data_iter, model, loss_compute):\n",
    "    \"Standard Training and Logging Function\"\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        out = model.forward(batch.src, batch.trg,\n",
    "                            batch.src_mask, batch.trg_mask)\n",
    "        loss = loss_compute(out, batch.trg_y, batch.ntokens)\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "        if i % 50 == 1:\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
    "                    (i, loss / batch.ntokens, tokens / elapsed))\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "    return total_loss / total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ë—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä `Adam` [üõ†Ô∏è[doc]](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ $\\beta_1=0.9$, $\\beta_2=0.98$ –∏ $\\epsilon=10^{-9}$.  –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –±—É–¥–µ—Ç –≤–∞—Ä—å–∏—Ä–æ–≤–∞—Ç—å—Å—è –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å —Ñ–æ—Ä–º—É–ª–æ–π:\n",
    "$$\n",
    "lrate = d_{\\text{model}}^{-0.5} \\cdot\n",
    "  \\min({step\\_num}^{-0.5},\n",
    "    {step\\_num} \\cdot {warmup\\_steps}^{-1.5})\n",
    "$$\n",
    "\n",
    "–≠—Ç–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ª–∏–Ω–µ–π–Ω–æ–º—É —É–≤–µ–ª–∏—á–µ–Ω–∏—é —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –ø–µ—Ä–≤—ã—Ö —ç—Ç–∞–ø–∞—Ö $warmup\\_steps$ –∏ –ø–æ—Å–ª–µ–¥—É—é—â–µ–º—É –µ–µ —É–º–µ–Ω—å—à–µ–Ω–∏—é –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –æ–±—Ä–∞—Ç–Ω–æ–º—É –∫–≤–∞–¥—Ä–∞—Ç–Ω–æ–º—É –∫–æ—Ä–Ω—é –∏–∑ –Ω–æ–º–µ—Ä–∞ —à–∞–≥–∞. –ë—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ $warmup\\_steps=4000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "\n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "\n",
    "def get_std_opt(model):\n",
    "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
    "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–∏–º–µ—Ä –≥—Ä–∞—Ñ–∏–∫–æ–≤ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –º–æ–¥–µ–ª–∏ –∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Three settings of the lrate hyperparameters.\n",
    "opts = [NoamOpt(512, 1, 4000, None),\n",
    "        NoamOpt(512, 1, 8000, None),\n",
    "        NoamOpt(256, 1, 4000, None)]\n",
    "plt.plot(np.arange(1, 20000), [[opt.rate(i) for opt in opts] for i in range(1, 20000)])\n",
    "plt.legend([\"512:4000\", \"512:8000\", \"256:4000\"])\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫ ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏. –û–Ω –ø–æ–º–æ–≥–∞–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –∑–∞ —Å—á–µ—Ç —É–º–µ–Ω—å—à–µ–Ω–∏—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –æ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –º–µ—Ç–æ–∫ –≤–æ –≤—Ä–µ–º—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è. –≠—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –∑–∞ —Å—á–µ—Ç –ø–æ–≤—ã—à–µ–Ω–∏—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –≤ ¬´–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö¬ª —è—Ä–ª—ã–∫–∞—Ö.\n",
    "\n",
    "–í–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –º–µ—Ç–æ–∫ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º $\\varepsilon_{ls}=0.1$. –í–º–µ—Å—Ç–æ one-hot —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ –æ–¥–Ω–æ–π —Ü–µ–ª–µ–≤–æ–π –≥—Ä—É–ø–ø–µ –º—ã —Å–æ–∑–¥–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ, –≤ –∫–æ—Ç–æ—Ä–æ–º –Ω–∞–∏–±–æ–ª—å—à—É—é –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∏–º–µ–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ, –∞ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è –ø–æ –≤—Å–µ–º—É —Å–ª–æ–≤–∞—Ä—é."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "q_i =\n",
    "\\left\\{\n",
    "    \\begin {aligned}\n",
    "         & 1 - \\varepsilon \\quad & \\text{if } i=y, \\\\\n",
    "         & \\varepsilon/(K-1) \\quad & \\text{otherwise}                  \n",
    "    \\end{aligned}\n",
    "\\right.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L04/label_smoothing.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://arxiv.org/abs/2011.12562\">Delving Deep into Label Smoothing</a></em></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False)\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
