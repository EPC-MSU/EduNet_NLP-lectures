{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Модели машинного обучения</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Необходимость методов классического машинного обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L03/out/ml_dl_learning_plateau.png\" alt=\"alttext\" width=\"600\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует ряд причин, почему важно знать и уметь применять методы классического машинного обучения:\n",
    "\n",
    "* **Недостаточный объем данных для нейронных сетей.** При малом объеме данных методы классического машинного обучения показывают более высокую эффективность.\n",
    "* **Время получения решения задачи.** Скорость экспериментов и получения хорошего бейзлайна выше, чем у решения с помощью нейронных сетей.\n",
    "*  **Интерпретируемость предсказаний.** Из методов классического машинного обучения легче получить важность каждого отдельно взятого признака.\n",
    "* **Комбинирование подходов.** В современных задачах часто используются комбинации нейронных сетей и алгоритмов классического машинного обучения. Например, можно использовать нейронные сети в качестве генераторов признаков для методов классического обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зачастую ваши задачи могут быть полностью и частично сведены к использованию методов машинного обучения. На пришлой лекции мы познакомились с основами, сегодня мы познакомимся с более мощными моделями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее познакомимся с логистической регрессией, но для лучшего понимания  вначале объясним общий подход на примере линейной регрессии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Линейная регрессия\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Регрессия** — это одна из трех базовых задач машинного обучения (классификация, регрессия, кластеризация).\n",
    "\n",
    "В задаче **регрессии** мы используем входные **признаки**, чтобы предсказать **целевые значения**. **Линейная регрессия** сводится к тому, чтобы провести “**линию наилучшего соответствия**” через набор точек данных.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Модель и ее параметры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим, у нас есть набор точек $\\{(x_i, y_i)\\}$.\n",
    "\n",
    "Цель линейной регрессии — **поиск линии, которая наилучшим образом соответствует заданным точкам**. Напомним, что общее уравнение прямой:\n",
    "\n",
    "$$\\large f(x) = w⋅x + b,$$\n",
    "\n",
    "где $w$ — характеризует наклон линии (в будущем мы будем называть значения $w$ весом, weight) а $b$ — её сдвиг по оси $y$ (bias). Таким образом, решение линейной регрессии определяет значения для $w$ и $b$ так, что $f(x)$ приближается как можно ближе к $y(x)$. Здесь $w$ и $b$ — **параметры модели**.\n",
    "\n",
    "Отобразим на графике случайные точки, расположенные в окрестности $y(x) = 6⋅x + 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "x = np.random.rand(100, 1)\n",
    "y = 2 + 6 * x + (np.random.rand(100, 1) - 0.5)\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.scatter(x, y, s=10)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим, что нам неизвестны параметры наклона и сдвига $w$ и $b$. Для их определения мы бы могли рассмотреть все возможные прямые вида $f(x) = w⋅x + b$ и выбрать среди семейства прямых такую, которая лучше всего приближает имеющиеся данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "plt.scatter(x, y, s=10)\n",
    "for w in [6.1, 5.8]:\n",
    "    for b in [1.9, 2.3]:\n",
    "        y_predicted = w * x + b\n",
    "        plt.plot(x, y_predicted, color=\"r\", alpha=0.3)\n",
    "plt.plot(x, 2 + 6 * x, color=\"g\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функция потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как выбрать параметры?\n",
    "\n",
    "**Функция потерь** позволяет вычислить меру количества ошибок. Для задачи **регрессии** такой мерой может быть **расстояние** между предсказанным значением $f(x)$ и его фактическим значением. Распространенной функцией потерь является **средняя квадратичная ошибка** (MSE). Чтобы вычислить MSE, мы просто берем все значения ошибок, считаем квадраты их длин и усредняем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть мы определяем ошибку модели на одном объекте как квадрат расстояния между предсказанием и истинным значением, а общая функция потерь будет задана выражением:\n",
    "\n",
    "$$l_i =|y_i - f(x_i)| $$\n",
    "\n",
    "$$ \\text{Loss} = \\sum l_i^2 = \\frac{1}{N} \\sum (y_i - f(x_i))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для прямой с параметрами $w=4$, $b = 2$ и $w=3$, $b = 2$ (верные значения):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_delta_line(ax, x, y, w, b, color=\"r\"):\n",
    "    y_predicted = w * x + b\n",
    "    # line\n",
    "    ax.plot(x, y_predicted, color=color, alpha=0.5, label=f\"f(x)={w}x+{b}\")\n",
    "    # delta\n",
    "    for x_i, y_i, f_x in zip(x, y, y_predicted):\n",
    "        ax.vlines(x=x_i, ymin=min(f_x, y_i), ymax=max(f_x, y_i), ls=\"--\", alpha=0.3)\n",
    "    # MSE\n",
    "    loss = np.sum((y - (w * x + b)) ** 2) / (len(x))\n",
    "    ax.set_title(f\"MSE = {loss:.3f}\")\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(11, 4))\n",
    "\n",
    "# plot x_i y_i (dots)\n",
    "for ax in axs:\n",
    "    ax.scatter(x, y, s=10)\n",
    "    ax.set_xlim([0, 0.8])\n",
    "    ax.set_ylim([2, 6])\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "\n",
    "plot_delta_line(axs[0], x, y, w=5, b=2, color=\"r\")\n",
    "plot_delta_line(axs[1], x, y, w=6, b=2, color=\"g\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача **поиска оптимальных параметров** модели сводится к задаче **поиска минимума функции потерь**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метод градиентного спуска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача поиска оптимальных параметров модели сводится к задаче **поиска минимума функции потерь**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Точки минимума и максимума функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как найти минимум функции в простом случае?\n",
    "\n",
    "- Найти производную функции.\n",
    "- Найти значения, при которых производная равна нулю.\n",
    "- Определить знаки производной. Когда функция возрастает, то производная положительна. Когда функция убывает, то производная отрицательна.\n",
    "- Определить точки минимума и максимума. Если функция возрастала и в определенной точке начала убывать, то это точка максимума. Если функция убывала и в некоторой точке начала возрастать, то это точка минимума."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L01/function.png\" width=\"700\" ></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Таблица производных:}$$\n",
    "\\begin{array}{|с|c|c|} \\hline\n",
    " f(x) \\text{ (функция)} & f'(x)  \\text{ (производная)}\\\\ \\hline\n",
    "с \\text{ (константа)} & 0 \\\\ \\hline\n",
    "cx & c \\\\ \\hline\n",
    "x^c & cx^{c-1} \\\\ \\hline\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим на примере функции $f(x)=x^2 + x + 3$.\n",
    "\n",
    "Она зависит от одной переменной $x$.\n",
    "\n",
    "Найдем производную $f'(x^2 - x + 3)$.\n",
    "- $f'(x^2 - x + 3) = 2x - 1$\n",
    "- $f'(x^2 - x + 3) = 0$ при $x =0.5$\n",
    "-  $f'(x^2 - x + 3) < 0 $ при $x < 0.5$, $f'(x^2 - x + 3) > 0 $ при $x > 0.5$\n",
    "- $f(x)$ убывает при $x < 0.5$, $f(x)$ возрастает при $x > 0.5$\n",
    "- $x = 0.5$ — точка минимума"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-2, 3, 100)\n",
    "y = x**2 - x + 3\n",
    "fig, ax = plt.subplots()\n",
    "fig.suptitle(\"$x^2 - x + 3$\")\n",
    "ax.plot(x, y)\n",
    "ax.set(xticks = np.arange(-2, 3.5, step=0.5))\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "ax.plot(0.5,2.75, marker='.')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ландшафт функции потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как будет выглядеть ландшафт функции потерь для нашей задачи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.arange(-10, 30, 1)\n",
    "b = np.arange(-10, 10, 1)\n",
    "\n",
    "x = np.random.rand(100, 1)\n",
    "y = 2 + 6 * x + (np.random.rand(100, 1) - 0.5)\n",
    "\n",
    "ww, bb = np.meshgrid(w, b)\n",
    "\n",
    "loss = np.zeros_like(ww)\n",
    "for i in range(ww.shape[0]):\n",
    "    for j in range(ww.shape[1]):\n",
    "        loss[i, j] = np.sum((y - (ww[i, j] * x + bb[i, j])) ** 2) / (len(x))\n",
    "\n",
    "def show_3d(xx, yy, zz, fig, title = \"MSE\", zlim = (0,20), alpha=0.5):\n",
    "    ax = fig.add_subplot(121, projection=\"3d\")\n",
    "    surf = ax.plot_surface(xx, yy, zz, cmap=plt.cm.RdYlGn_r, alpha=alpha)\n",
    "\n",
    "    ax.contourf(xx, yy, zz, zdir=\"zz\", offset=-1, cmap=\"RdYlGn_r\", alpha=alpha)\n",
    "    ax.set_zlim(zlim)\n",
    "\n",
    "    ax.set_xlabel(\"b\")\n",
    "    ax.set_ylabel(\"w\")\n",
    "    ax.set_title(title)\n",
    "    fig.colorbar(surf, location=\"left\")\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "show_3d(ww, bb, loss, fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимым (но недостаточным) условием локального минимума дифференцируемой функции является равенство нулю частных производных:\n",
    "\n",
    "$$\t\\begin{equation*}\n",
    " \\begin{cases}\n",
    "   \\displaystyle\\frac{\\partial \\text{Loss}}{\\partial w}=0,\n",
    "   \\\\\n",
    "   \\displaystyle\\frac{\\partial \\text{Loss}}{\\partial b}=0.\n",
    " \\end{cases}\n",
    "\\end{equation*} $$\n",
    "\n",
    "Т.к. MSE для линейной регрессии — полином второй степени относительно $w$ и $b$, а полином второй степени не может иметь больше одного экстремума, то локальный минимум будет глобальным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Частная производная и градиент функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если функция зависит от нескольких переменных, можно говорить о частной производной, когда все остальные переменные, кроме интересующей нас, становятся константами. Производная функции $f$ от переменной $x_1$: $\\large\\frac{\\partial f}{\\partial x_1}$.\n",
    "\n",
    "**Градиент** в математическом анализе — это вектор, указывающий на направление максимального роста функции в заданной точке. Вектор-градиент состоит из частных производных функции от каждой из ее переменных $(x_{1},...,x_{n})$:\n",
    "\n",
    "$$ \\nabla f(\\vec{x}) = \\begin{bmatrix}\n",
    "\\displaystyle\\frac{\\partial f}{\\partial x_1}\\\\\n",
    "\\displaystyle\\frac{\\partial f}{\\partial x_2}\\\\\n",
    "...\\\\\n",
    "\\displaystyle\\frac{\\partial f}{\\partial x_n}\\\\\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим функцию $\\large f(x, y) = \\sin(x\\cdot y)$ и посчитаем градиент . Для этого воспользуемся [**таблицей производных** 📚[wiki]](https://ru.wikipedia.org/wiki/%D0%A2%D0%B0%D0%B1%D0%BB%D0%B8%D1%86%D0%B0_%D0%BF%D1%80%D0%BE%D0%B8%D0%B7%D0%B2%D0%BE%D0%B4%D0%BD%D1%8B%D1%85) и правилом вычисления [**производной сложной функции** 📚[wiki]](https://ru.wikipedia.org/wiki/%D0%94%D0%B8%D1%84%D1%84%D0%B5%D1%80%D0%B5%D0%BD%D1%86%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5_%D1%81%D0%BB%D0%BE%D0%B6%D0%BD%D0%BE%D0%B9_%D1%84%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D0%B8) (Chain-rule):\n",
    "$$\\frac {\\partial f} {\\partial x} = \\frac {\\partial f} {\\partial t} \\cdot \\frac {\\partial t} {\\partial x}$$\n",
    "\n",
    "Это правило очень нам пригодится в будущем.\n",
    "\n",
    "$$\\nabla f(x, y)=\\begin{bmatrix}\n",
    "\\displaystyle\\frac{\\partial f}{\\partial x}\\\\\n",
    "\\displaystyle\\frac{\\partial f}{\\partial y}\\\\\n",
    "\\end{bmatrix}\n",
    "=\\begin{bmatrix}\n",
    "\\displaystyle\\frac{\\partial\\sin(xy)}{\\partial(xy)}\\cdot\\frac{\\partial(xy)}{\\partial x}\\\\\n",
    "\\displaystyle\\frac{\\partial\\sin(xy)}{\\partial(xy)}\\cdot\\frac{\\partial(xy)}{\\partial y}\\\\\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "\\cos(xy)\\cdot y\\\\\n",
    "\\cos(xy)\\cdot x\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для минимизации ошибки нужен антиградиент, который показывает направление скорейшего убывания функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x, y: np.sin(x * y)\n",
    "\n",
    "x = np.linspace(0, 4, 1000)\n",
    "y = np.linspace(0, 4, 1000)\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "zz = f(xx, yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradf = lambda x, y: (np.cos(x * y) * y, np.cos(x * y) * x)\n",
    "\n",
    "xsmall = np.linspace(0, 4, 15)\n",
    "ysmall = np.linspace(0, 4, 15)\n",
    "xxsmall, yysmall = np.meshgrid(xsmall, ysmall)\n",
    "gradx, grady = gradf(xxsmall, yysmall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как **значение градиента в точке** — это вектор, мы можем говорить о его **величине** и **направлении**. Визуализируем наши расчеты: посмотрим на ландшафт функции $f(x, y)$ и направления градиентов.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 5))\n",
    "show_3d(xx, yy, zz, fig, title = \"sin(xy)\", zlim = (-2,2), alpha = 1)\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax.imshow(\n",
    "    zz,\n",
    "    extent=(np.min(x), np.max(x), np.min(y), np.max(y)),\n",
    "    cmap=\"RdYlGn_r\",\n",
    "    origin=\"lower\",\n",
    ")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "\n",
    "ax.quiver(xxsmall, yysmall, gradx, grady)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На рисунке выше значения градиента в точке обозначены чёрными стрелочками. Можно заметить, что длина стрелок в  районе максимальных и минимальных значений функции **почти нулевая**, стрелки направлены в направлении возрастания значения функции и наиболее длинные стрелки находятся в области наиболее резкого изменения значений функции.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это проявление **свойств градиента**:\n",
    "* Направление $\\frac{\\nabla f}{||\\nabla f||}$ — сообщает нам направление максимального роста функции.\n",
    "\n",
    "*  Величина $||\\nabla f||$ — характеризует мгновенную скорость изменения значений функции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиентный спуск и скорость обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Градиентный спуск** — это алгоритм для итеративного, шаг за шагом, перемещещния в поиске минимума. Мы не напрямую ищем точку, где производная равна нулю, а постепенно передвигаемся вниз по функции. Градиентный спуск выбирает случайную точку, находит направление самого быстрого убывания функции и двигается до ближайшего минимума вдоль этого направления.\n",
    "\n",
    "Важно настроить размер шага или **скорость обучения** $η$ — некоторый коэффициент, как правило, небольшой, который не позволяет нам двигаться слишком быстро. У нас есть точка, в которую мы хотим попасть. Если мы сделаем слишком большой шаг, то мы ее перескочим (график справа), поэтому надо подобрать шаг, который не позволит ее перескочить, но в то же время такой, чтобы тот же процесс не шел слишком медленно (как на графике слева)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet_NLP-content/L01/out/learning_rate_optimal_value.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если на каком-то этапе разность между старой точкой (до шага) и новой снижается ниже предела, считается, что минимум найден, алгоритм завершен."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомним, что функция потерь $\\mathcal{L}(y, \\hat{y})$ зависит от нескольких переменных: весов $\\overline{w} = (w_1, \\cdots, w_m)$ и сдвига $b$.\n",
    "\n",
    "Следовательно, мы будем использовать несколько частных производных:\n",
    "\n",
    "- частные производные функции потерь от весов $w_1,\\cdots,w_m$: $\\large\\frac{\\partial \\mathcal{L}(y, \\hat{y})}{\\partial w_1}, \\cdots, \\frac{\\partial \\mathcal{L}(y, \\hat{y})}{\\partial w_m}$\n",
    "- частная производная функции потерь от сдвига $b$: $\\large\\frac{\\partial \\mathcal{L}(y, \\hat{y})}{\\partial b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После подсчета частных производных нам необходимо обновить значения весов и сдвига.\n",
    "\n",
    "$$w_i = w_i-η\\frac{\\partial \\mathcal{L}(y, \\hat{y})}{\\partial w_i}$$\n",
    "\n",
    "$$b = b-η\\frac{\\partial \\mathcal{L}(y, \\hat{y})}{\\partial b}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем градиентный спуск и решим с его помощью нашу задачу с линейной регрессией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = np.random.rand(250, 1)\n",
    "y = 2 + 6 * x + (np.random.rand(250, 1) - 0.5)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print (x_train.shape, x_test.shape)\n",
    "print (y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Запись смещения**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для компактной записи смещения можно считать его таким же весом, который домножается на 1.\n",
    "\n",
    "Обозначим вектор-столбец из настраиваемых параметров:\n",
    "$$\\vec w = \\begin{bmatrix}\n",
    "b \\\\ w \\\\\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К матрице (в нашем случае был только один признак, поэтому у нас будет вектор-столбец) признаков слева \"дорисуем\" столбец единиц:\n",
    "$$X = \\begin{bmatrix}\n",
    "1 & X \\\\\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "1 & 2.7 \\\\\n",
    "1 & 3.3 \\\\\n",
    "... & ...\\\\\n",
    "1 & 9.2 \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "**Предупреждение:** добавлять столбец единиц нужно, только если вы сами пишете модель. **Если вы пользуетесь готовыми моделями, в этом нет необходимости.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В общем случае:\n",
    "\n",
    "$$\\large \\vec y = b + w_1\\cdot x_1 + w_2\\cdot x_2 + w_3\\cdot x_3 + w_4\\cdot x_4 + w_5\\cdot x_5 + ... + w_n\\cdot x_n = X\\vec w $$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.hstack((np.ones((x_train.shape[0], 1)), x_train))\n",
    "x_test = np.hstack((np.ones((x_test.shape[0], 1)), x_test))\n",
    "\n",
    "print (x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализация градиентного спуска. В будущем это будет спрятано \"внутри\" моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def gradient(x, y, w):\n",
    "    \"\"\"Gradient of mean squared error.\"\"\"\n",
    "    return 2 * (x.T @ (x @ w) - x.T @ y) / len(x)\n",
    "\n",
    "\n",
    "def gradient_descent(x_train, y_train, x_test, y_test, w, alpha, iteration=10):\n",
    "    \"\"\"Gradient descent for optimizing slope in simple linear regression\"\"\"\n",
    "    # history\n",
    "    ws = np.zeros((iteration + 1, 2))\n",
    "    ws[0] = w[:, 0]\n",
    "    mse_train = [mean_squared_error(y_train, x_train @ w)]\n",
    "    dmse_train = []\n",
    "    mse_test = [mean_squared_error(y_test, x_test @ w)]\n",
    "    prediction = {(w[0][0], w[1][0]): x_train @ w}\n",
    "\n",
    "    print(\n",
    "        f\"Iteration 0: b = {w[0][0]:.2f}, w = {w[1][0]:.2f}, \"\n",
    "        f\"Loss_train = {mse_train[0]:.2f}, \"\n",
    "        f\"Loss_test = {mse_test[0]:.2f}.\"\n",
    "    )\n",
    "\n",
    "    for i in range(iteration):\n",
    "        # adjust w based on gradient * learning rate\n",
    "        grad = gradient(x_train, y_train, w)\n",
    "        w -= alpha * grad  # adjust w based on gradient * learning rate\n",
    "        # history\n",
    "        ws[i + 1] = w[:, 0]\n",
    "        mse_train.append(mean_squared_error(y_train, x_train @ w))\n",
    "        dmse_train.append(grad)\n",
    "        mse_test.append(mean_squared_error(y_test, x_test @ w))\n",
    "        prediction[(w[0][0], w[1][0])] = x_train @ w\n",
    "\n",
    "        print(\n",
    "            f\"Iteration {i+1}: b = {w[0][0]:.2f}, w = {w[1][0]:.2f}, \"\n",
    "            f\"Loss_train = {mse_train[i]:.2f}, \"\n",
    "            f\"Loss_test = {mse_test[i]:3.2f}.\"\n",
    "        )\n",
    "    return ws, prediction, mse_train, dmse_train, mse_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим нашу модель, задав начальные значения $b$ и $w$, а также скорость обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([[-5.0], [-3.0]])\n",
    "ws, prediction, mse_train, dmse_train, mse_test = gradient_descent(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    w,\n",
    "    alpha = 0.001,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируем процесс:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mse(mse_train, mse_test):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.title(\"Learning curve\")\n",
    "    plt.plot(mse_train, label=\"train\")\n",
    "    plt.plot(mse_test, label=\"test\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.xlabel(\"iterations\", fontsize=12)\n",
    "    plt.ylabel(\"MSE Loss\", fontsize=12)\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mse(mse_train, mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что ошибка постепенно падает, но до сходимости далеко. Также отмечаем, что **ошибка на тесте меньше ошибки на обучении**. **Что-то явно не так.** Так бывает при утечке данных (об утечке данных вы подробнее узнаете в следующих лекциях), но в данном случае, test выборка просто слишком мала, чтобы отражать генеральную совокупность."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем увеличить скорость обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([[-5.0], [-3.0]])\n",
    "ws, prediction, mse_train, dmse_train, mse_test = gradient_descent(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    w,\n",
    "    alpha = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mse(mse_train, mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Слишком большая скорость, модель не сошлась: ошибка растёт."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([[-5.0], [-3.0]])\n",
    "ws, prediction, mse_train, dmse_train, mse_test = gradient_descent(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    w,\n",
    "    alpha = 0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В дальнейших лекциях вы познакомитесь с различными модификациями метода градиентного спуска и узнаете больше о выборе скорости обучения, а пока ориентируйтесь на кривые обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Необходимость нормализации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выше мы не дошли до оптимальной прямой  $y=6x+2$. При этом график Loss выглядит неплохо:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mse(mse_train, mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такое поведение связано с ландшафтом функции потерь: значение ошибки по оси $b$ изменяется намного медленнее, чем по оси $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачаем **код для интерактивной визуализации**. Он нужен только для объяснения и **не пригодится вам в работе**. Его разбирать мы не будем. Eсли интересно, можно изучить самостоятельно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title *Code for interactive visual\n",
    "# source: https://github.com/TomasBeuzen/deep-learning-with-pytorch\n",
    "\n",
    "!wget -qN https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L02/interactive_visualization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interactive_visualization import plot_grid_search_2d\n",
    "\n",
    "intercepts = np.arange(-7.5, 12.5, 0.1)  # b\n",
    "slopes = np.arange(-5, 5, 0.1)  # w\n",
    "plot_grid_search_2d(x_train[:, 1], y_train, slopes, intercepts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому основное изменение значений происходит вдоль оси $w$, а $b$ меняется слабо (значение $b$ далеко от ожидаемого)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interactive_visualization import plot_gradient_descent_2d\n",
    "\n",
    "plot_gradient_descent_2d(\n",
    "    x_train[:, 1],\n",
    "    y_train[:, 0],\n",
    "    ws,\n",
    "    slopes,\n",
    "    intercepts,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы исправить ситуацию, применим `StandardScaler`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея **`StandardScaler`** заключается в том, что он преобразует данные таким образом, что распределение будет иметь среднее значение $0$ и стандартное отклонение $1$. Большинство значений будет находиться в диапазоне от $-1$ до $1$. Это стандартная трансформация, и она применима во многих ситуациях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large z_i=\\frac{X_i-u}{s},$$\n",
    "\n",
    "где $u$ — среднее значение (или $0$ при `with_mean=False`), $s$ — стандартное отклонение (или $0$ при `with_std=False`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "x_train_scaled = scaler.fit_transform(np.expand_dims(x_train[:, 1], axis=1)).flatten()\n",
    "x_test_scaled = scaler.transform(np.expand_dims(x_test[:, 1], axis=1)).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отметьте ключевой момент. Среднее значение и дисперсия вычисляются **только по тренировочной выборке**.\n",
    "\n",
    "Как правило, решения, которые переводят текст в числовое представление, обеспечивают тот или иной вариант нормировки, но это стоит проверять. Существуют и другие алгоритмы нормировки, об этом подробнее в  [[wiki] 📚 L02 MSU.AI. Линейные модели](https://msu.ai/l02_linear_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercepts = np.arange(-15, 25, 0.2)  # b\n",
    "slopes = np.arange(-20, 20, 0.2)  # w\n",
    "\n",
    "plot_grid_search_2d(x_train_scaled, y_train, slopes, intercepts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь ландшафт функции потерь симметричен, обе переменных становятся во время обучения равнозначны."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_scaled = np.hstack(\n",
    "    (np.ones((len(x_train_scaled), 1)), np.expand_dims(x_train_scaled, axis=1)),\n",
    ")\n",
    "\n",
    "x_test_scaled = np.hstack(\n",
    "    (np.ones((len(x_test_scaled), 1)), np.expand_dims(x_test_scaled, axis=1)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Т.к. диапазоны $x$ изменились, значения $w$ и $b$ тоже изменятся.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([[57.0], [33.0]])\n",
    "ws, prediction, mse_train, dmse_train, mse_test = gradient_descent(\n",
    "    x_train_scaled, y_train, x_test_scaled, y_test, w, 0.35, iteration=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mse(mse_train, mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, что после нормализации мы сходимся к $y = 6x + 2$.  Для этого используем данные о матожидании и дисперсии из `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = ws[-1][0] - ws[-1][1] * scaler.mean_ / (scaler.var_) ** 0.5\n",
    "w = ws[-1][1] / (scaler.var_) ** 0.5\n",
    "\n",
    "print(f\"y = {w[0]:.2f}x + {b[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По визуализации видно, что $w$ и $b$ изменяются во время обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_2d(\n",
    "    x_train_scaled[:, 1],\n",
    "    y_train[:, 0],\n",
    "    ws,\n",
    "    slopes,\n",
    "    intercepts,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cтохастический градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[blog] ✏️ Пост о стохастическом градиентом спуске](https://www.tomasbeuzen.com/deep-learning-with-pytorch/chapters/chapter2_stochastic-gradient-descent.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "До этого мы обучали модель, рассчитывая градиент по **всей train выборке**. Это не всегда возможно:\n",
    "- данных может быть слишком много, чтобы загрузить их в память одновременно и рассчитать градиент,\n",
    "- мы можем хотеть дообучать модель на свежепришедших данных, которых может быть немного.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому появляется идея **стохастического градиентного спуска**: мы можем делать шаг обучения, рассчитывая градиент не по всей выборке (**batch**), а по нескольким случайно выбранным объектам (**mini-batch**) или даже по одному случайно выбранному объекту (**stochastic**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/define_size_of_batch.png\" width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно [показать 📚[book]](https://academy.yandex.ru/handbook/ml/article/shodimost-sgd), что **стохастический** (с размером $\\text{batch}=1$) **градиентный спуск сходится к минимуму (глобальному или локальному) функции потерь** с конечной точностью. Важным условием является **стохастичность** (случайность). Если мы будем использовать одну и ту же последовательность выборок, это приведет к накоплению ошибки и смещению результата. Поэтому батчи на обучении формируются из различных объектов, каждый раз разные."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имеет смысл использовать **максимальный размер mini-batch** для **ускорения расчетов**, .\n",
    "\n",
    "В ходе работы с большими языковыми моделями вы увидите, что практически ничего не помещается в память, ни модель, ни данные, и мы будем знакомиться с различными трюками для обхода этих ограничений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы научились **решать задачу регрессии** множества переменных $x_1,x_2,...,x_n$ (признаков):\n",
    "\n",
    "$$f(x) = w_1x_1 + w_2x_2 + ... + w_nx_n, $$\n",
    "\n",
    "в которой $f$ может принимать значения в любом диапазоне. Чтобы перейти к **задаче бинарной классификации**, т.е. получению вероятность отнесения объекта к классу 0 или 1, нужно привести $f(x)$ в диапазон от 0 до 1.\n",
    "\n",
    "Если классов более двух, задачу можно свести  к бинарной, если для каждого класса формулировать её как \"один против всех\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Переход к вероятности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В диапазон $[0,1]$ можно перевести с  помощью сигмоиды. Если получившееся значение больше 0.5, то объект относится к классу 1 (положительному), иначе — к классу 0 (отрицательному).\n",
    "\n",
    "$$ σ(f(x))=\\frac{1}{1+e^{-f(x)}}$$\n",
    "\n",
    "Запишем формулу сигмоиды, используя методы из библиотеки NumPy:\n",
    "\n",
    "- `np.exp(x)`  для подсчета $e^x$\n",
    "- `np.arange()` для указания диапазона возможных значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "x = np.arange(-10, 10, 0.5)\n",
    "z = 1/(1 + np.exp(-x))\n",
    "\n",
    "plt.plot(x, z)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Sigmoid(X)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для многоклассовой классификации используется функция активации softmax.  Вероятность $i$-го класса при наличии $K$ классов рассчитывается следующим образом:\n",
    "\n",
    "$$\\text{softmax}(f_{i}) = \\frac{e^{f_i}}{\\sum_{j=1}^K e^{f_j}}$$\n",
    "$$i=1,...,K$$\n",
    "\n",
    "Для каждого объекта выбирается класс с наибольшей вероятностью.\n",
    "\n",
    "Запишем формулу для функции софтмакс. Для подсчета суммы используем метод `np.sum()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "x = np.array([2.6, 3.2, 0.5])\n",
    "print('Softmax:', softmax(x))\n",
    "print('Sum:', np.sum(softmax(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы бы хотели такую функцию, которая очень сильно штрафует за большие отступления от истинного ответа и не сильно за небольшие ошибки.\n",
    "\n",
    "<center><img src =\"https://i.ibb.co/FKsSgWL/cross-entropy.png\" width=\"500\" ></center>\n",
    "\n",
    "Этим требованиям отвечает функция кросс-энтропии ($L_{CE}$ — *Cross Entropy Loss*):\n",
    "\n",
    "$$L_{CE}(\\hat y,y) = -\\sum^{K}_{i=1}y_i \\cdot log(\\hat y_i),$$\n",
    "\n",
    "где $i$ — номер класса, $y$ — истинный ответ, $\\hat y$ — предсказанный ответ. Если истинный ответ 0, а предсказывается 1 — loss уходит в бесконечность. Однако, чем ближе мы приближаемся к 0 — тем существенно меньше штраф. В принципе, мы могли бы изобрести и другую функцию.\n",
    "\n",
    "В случае бинарной классификации имеет $K=2$, $y=0$ или $y=1$ ($L_{BCE}$ — *Binary Cross Entropy Loss*):\n",
    "\n",
    "$$L_{BCE}(\\hat y,y) = -(y \\cdot log(\\hat y)+(1-y) \\cdot log(1-\\hat y))$$\n",
    "\n",
    "\n",
    "В случае многоклассовой классификации $K>2$. Истинный ответ $y$ — вектор длины $K$, где элемент вектора $y_c=1$, если $c$ — истинный класс, остальные элементы равны $0$.\n",
    "\n",
    "Например, $K=3$ (классы $0,1,2$). Объект относится к классу $2$. Тогда $y = (0, 0, 1)$, $c=2$, $y_2 = 1$.\n",
    "\n",
    "Модель предсказывает вектор $\\hat y$ длины $K$. Функция кросс-энтропии имеет вид:\n",
    "\n",
    "$$L_{CE}(\\hat y,y) = -\\log \\hat y_c$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Распознавание эмоций"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим задачу многоклассовой классификации на примере распознавания эмоций.\n",
    "\n",
    "Распознавание эмоций ≠ анализ тональности. Анализ тональности — выявление оценки авторов по отношению к объектам, речь о которых идёт в тексте. Она может быть позитивной, негативной или нейтральной. Эмоции могут включать не только радость или грусть, но ужас, гнев, удивление, отвращение и т.п."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чаще всего при обработке текстов стоит задача распознать 6 базовых эмоций: счастье (happiness), печаль (sadness), страх (fear), гнев (anger), отвращение (disgust), удивление (surprise). Данная классификация введена Полом Экманом в книге [[book] 📚 Basic emotions](https://www.paulekman.com/wp-content/uploads/2013/07/Basic-Emotions.pdf). Утверждается, что люди всех культур испытывают их и могут распознавать в других людях. Для каждой базовой эмоции есть соответствующее, безошибочно опознаваемое выражение лица."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L01/basic_emotions.png\" width=\"1100\" ></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка и подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать русскоязычный набор данных, представленный в работе [[paper] 🎓 Emotion classification in Russian: feature engineering and analysis](https://link.springer.com/chapter/10.1007/978-3-030-72610-2_10).\n",
    "\n",
    "Он размечен по **5 классам эмоций**:\n",
    "- радость (joy),\n",
    "- печаль (sadness),\n",
    "- злость (anger),\n",
    "- неуверенность (uncertainty),\n",
    "- нейтральность (neutrality).\n",
    "\n",
    "Из классификации Экмана удалена категория отвращения из-за отсутствия соответствующих данных. Категории страха и удивления  объединены в одну категорию неопределенности из-за сходства способов, которыми они выражаются.\n",
    "\n",
    "Загрузим набор данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/datasets/Emotion_Classification_Russian.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "emo = pd.read_csv('Emotion_Classification_Russian.csv')\n",
    "emo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://i.ibb.co/Jr5SprX/cleandata.png\" width=\"700\" ></center>\n",
    "\n",
    "Вначале всех текстовых задач идёт очистка данных от \"мусора\": номеро встраниц и названий журналов, преобразование или удаление таблиц и картинок и прочее приведение текста в \"чистый\" вид.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо метки класса (label) и текста (text), присутствует столбец с лемматизированными предложениями (lemmatized). Лемматизация — процесс приведения словоформы к лемме, то есть её нормальной (словарной) форме.\n",
    "\n",
    "<center><img src =\"https://i.ibb.co/wB19yJX/lemma.webp\" width=\"500\" ></center>\n",
    "\n",
    "В русском языке это следующие морфологические формы:\n",
    "\n",
    "- для существительных — именительный падеж, единственное число\n",
    "  - кошками → кошка;\n",
    "- для прилагательных — именительный падеж, единственное число, мужской род\n",
    "  - красивых → красивый;\n",
    "- для глаголов, причастий, деепричастий — глагол в инфинитиве (неопределённой форме) несовершенного вида\n",
    "  - бежал → бежать.\n",
    "\n",
    "Лемматизация необходима для языков с богатой морфологией, чтобы не расширять размер словаря за счет слов, у которых одинаковый смысл, но разные морфологические формы.\n",
    "\n",
    "В рассматриваемом наборе данных лемматизация проведена с помощью морфологического анализатора [[git] 🐾 RNNmorph](https://github.com/IlyaGusev/rnnmorph). Он использует реккурентную нейронную сеть для определения части речи и морфологических признаков слова с учетом контекста.\n",
    "\n",
    "Рассмотрим пример его применения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/IlyaGusev/rnnmorph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnnmorph.predictor import RNNMorphPredictor\n",
    "predictor = RNNMorphPredictor(language=\"ru\")\n",
    "\n",
    "forms = predictor.predict([\"мама\", \"поймала\", \"мышь\"])\n",
    "print(f'Часть речи слова \"мама\": {forms[0].pos}')\n",
    "print(f'Лемма слова \"поймала\": {forms[1].normal_form}')\n",
    "print(f'Морфологический анализ слова \"мышь\": {forms[2].tag}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для удобства заменим словесные обозначения классов в датасете на числовые. Метки присваиваются в алфавитном порядке:\n",
    "- **a**nger → 0\n",
    "- **j**oy → 1\n",
    "- **n**eutrality →  2\n",
    "- **s**adness → 3\n",
    "- **u**ncertainty → 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo['num_label'] = emo['label'].astype('category').cat.codes\n",
    "emo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Узнаем количество данных в датасете и их распределение по классам эмоций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.pie(emo['label'].value_counts(), labels=emo['label'].unique(), autopct='%.1f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведем уже знакомые операции для подготовки данных:\n",
    "- запишем в отдельные переменные лемматизированные тексты `X` и метки классов `y`;\n",
    "- разделим данные на обучающую и тестовую выборку;\n",
    "- осуществим векторизацию обучающей и тестовой выборок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X, y = emo['lemmatized'], emo['num_label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify = y)\n",
    "\n",
    "vect = TfidfVectorizer(min_df=0.0002, max_df=0.2)\n",
    "X_train_vect = vect.fit_transform(X_train)\n",
    "X_test_vect = vect.transform(X_test)\n",
    "X_train_vect, X_test_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение и анализ модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель логистической регрессии (при дефолтном параметре `max_iter=100` модель не сходится; чтобы добиться сходимости алгоритма, установим большее количество итераций)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(random_state=42, max_iter=300) # model initialization\n",
    "logreg.fit(X_train_vect, y_train) # trainig\n",
    "y_logreg = logreg.predict(X_test_vect) # predicting labels\n",
    "y_logreg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты классификации можно отразить в виде матрицы ошибок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def show_confusion_matrix(confusion_matrix):\n",
    "  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Greens\")\n",
    "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
    "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
    "  plt.ylabel('True emotion')\n",
    "  plt.xlabel('Predicted emotion')\n",
    "\n",
    "class_names = ['anger', 'joy', 'neutrality', 'sadness', 'uncertainty']\n",
    "cm = confusion_matrix(y_test, y_logreg)\n",
    "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "show_confusion_matrix(df_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метрики для каждого класса, а также усредненные метрики представлены в отчете о классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['anger', 'joy', 'neutrality', 'sadness', 'uncertainty']\n",
    "print(classification_report(y_test, y_logreg, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого класса были посчитаны свои коэффициенты регрессии. Они представляют матрицу $K \\times n$, где $K$ — количество классов, $n$ — количество признаков (слов)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficient_matrix = logreg.coef_\n",
    "#print(coefficient_matrix)\n",
    "print('Размер матрицы: ', coefficient_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Каждый из коэффициентов регрессии описывает размер вклада соответствующего признака.\n",
    " - Положительный коэффициент — признак повышает вероятность принадлежности к классу, отрицательный коэффициент — признак уменьшает вероятность.\n",
    " - Большой коэффициент — признак существенно влияет на вероятность принадлежности к классу, почти нулевой коэффициент — признак имеет небольшое влияние на вероятность результата.\n",
    "\n",
    "Выведем признаки с самыми большими коэффициентами, иными словами, получим наиболее характерные слова для каждой эмоции. Для примера посмотрим на  *Радость* и *Неуверенность*, у которых порядковые номера 1 и 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {1:'Радость', 4:'Неуверенность'}\n",
    "coefficient_matrix = logreg.coef_\n",
    "\n",
    "for i in [1, 4]:\n",
    "\n",
    "    print(f\"\\n{labels[i]}:\")\n",
    "\n",
    "    feature_names = vect.get_feature_names_out() # word features\n",
    "    order = coefficient_matrix[i].argsort() # ascending order for coefficients\n",
    "    class_coefficients = coefficient_matrix[i][order][::-1][:5] # sort and extract top-5 coefficients\n",
    "    feature_names = feature_names[order][::-1][:5] # words with the highest coefficients\n",
    "\n",
    "    for feature, coefficient in zip(feature_names, class_coefficients):\n",
    "      print(feature, coefficient.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Деревья решений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Деревья решений — это одни из первых моделей машинного обучения, которые были известны человеку. Изначально их строили без специальных алгоритмов, а просто вручную.\n",
    "\n",
    "Когда требовалось принять решение по проблеме, для которой построено дерево, человек брал и проходился по этому дереву."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L03/out/manual_construction_of_decision_trees.png\" width=\"900\"></center>\n",
    "\n",
    "<center><em>Предсказание типа линз для человека</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "За принципом работы дерева решений стоит понятная интуиция. В каждом узле есть какой-то вопрос. Например, нормальное ли у человека слезоотделение, есть ли у него астигматизм и так далее. И, отвечая на каждый из этих вопросов, мы перемещаемся по дереву до тех пор, пока не придем к нужному типу линз.\n",
    "\n",
    "Несмотря на возраст подхода, эти модели могут быть неожиданно эффективны и их можно автоматически строить с помощью алгоритмов и делать это достаточно быстро даже на больших объемах данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L03/out/neuro_tree.jpeg\" width=\"600\"></center>\n",
    "\n",
    "<center><em>\"As long as Kaggle has been around, Anthony says, it has <font color=green >almost always</font> been <b>ensembles of decision trees that have won competitions</b>\".</em></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В очень многих задачах, где нет какой-то локальной связанности, которая есть в изображениях, текстах и т.д., деревья решений эффективнее, чем нейронные сети."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Принцип работы дерева решений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L03/out/decision_tree_principle.png\" width=\"900\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Допустим, есть 2 признака, в данном случае вещественных. Для каждой точки мы создаем вопрос: признак $x_1$ больше $0.7$ или меньше? Если больше $0.7$, то это красная точка. Если меньше $0.7$, то идем во второй внутренний узел T2 и спрашиваем: признак $x_2$ меньше $0.5$ или больше? Если меньше $0.5$, то точка будет красная, в другом случае — синяя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L03/out/decision_trees_partitioning_space.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Разбиение пространства</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дерево решений делит пространство признаков с помощью плоскостей на области, и в каждой из этих областей предсказывается константная величина."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Деревья решений, как правило, используются при решении 2-х типов задач — классификации и регрессии.\n",
    "Первая — классификация. Предсказание может быть:\n",
    "\n",
    "\n",
    "*   Жёстким (метка класса)\n",
    "*   Мягким (вероятность класса)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L03/out/probability_estimation_by_decision_tree.png\" width=\"700\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В листе находятся объекты разных классов. Для того, чтобы оценить вероятность принадлежности объекта к какому-то определенному классу, можно просто число представителей данного класса поделить на общее число объектов. Это дает нам одну важную интуицию: желательно, чтобы в листе  было не очень мало объектов. Чем больше объектов, тем меньше ошибка.\n",
    "\n",
    "Фактически, это оценка генеральной совокупности по ограниченной выборке. Можно использовать [распределение Бернулли 📚[wiki]](https://ru.wikipedia.org/wiki/Распределение_Бернулли) для двух классов или [мультиномиальное распределение 📚[wiki]](https://ru.wikipedia.org/wiki/Мультиномиальное_распределение) для большего количества классов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Для бинарных признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L03/out/decision_tree_for_two_features.png\" width=\"900\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Представим, что мы решаем задачу бинарной классификации (есть инфаркт у человека или нет).\n",
    "\n",
    "Возьмем в качестве первого вопроса признак \"боль в груди\". В 1-ом и во 2-ом листах получается разное распределение людей. В левом листе инфаркт более вероятен, в правом — менее вероятен.\n",
    "\n",
    "Другим признаком может быть “как хорошо циркулирует кровь”, в таком случае тоже получится неплохое разделение. Последний признак — “есть ли атеросклероз”.\n",
    "\n",
    "Мы получили три разбиения по разным признакам. Теперь мы бы хотели выбрать лучший признак, для этого нам нужно их сравнить."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логично выбрать такое разбиение, которое дает нам \"хорошие\" узлы — те, в которых преимущественно сосредоточены объекты одного класса\n",
    "\n",
    "Одна из используемых метрик называется [Gini ✏️[blog]](https://www.learndatasci.com/glossary/gini-impurity/) . Она считается по следующей формуле:\n",
    "\n",
    "$$\\large \\text{Gini} = 1 - \\sum_ip_i^2$$\n",
    "\n",
    "Gini — мера [неопределенности ✏️[blog]](https://quantdare.com/decision-trees-gini-vs-entropy/) значения класса внутри узла. Соответственно, чем она ниже, тем лучше получившийся узел."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем Gini для признака \"Боль в груди\":\n",
    "\n",
    "$\\text{Gini}_1 = 1- (\\dfrac{105}{105+33})^2 - (\\dfrac{33}{105+33})^2 = 0.364$\n",
    "\n",
    "$\\text{Gini}_2 = 1- (\\dfrac{34}{34+125})^2 - (\\dfrac{125}{34+125})^2 = 0.336$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L03/out/compute_gini_for_binary_features.png\" width=\"900\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такую метрику можно посчитать для каждого узла. Дальше посчитать метрику для следующего узла. Дальше можем оценить, насколько стал лучше результат в зависимости от используемого признака."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large \\text{Impurity decrease} = \\text{Gini}_0 - (\\frac{n_1}{n_1+n_2}\\text{Gini}_1 + \\frac{n_2}{n_1+n_2}\\text{Gini}_2),$$\n",
    "\n",
    "где $n_1, n_2$ — число объектов в листьях,\n",
    "\n",
    "$ \\quad\\  \\text{Gini}_0$ — чистота исходного узла."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Боль в груди:**\n",
    "\n",
    " $\\text{Impurity decrease} = 0.498 - (\\dfrac{138}{138+159})\\cdot 0.364 - (\\dfrac{159}{138+159})\\cdot 0.336 = 0.149$\n",
    "\n",
    "**Хорошо циркулирует кровь:**\n",
    "\n",
    " $\\text{Impurity decrease} = 0.498 - (\\dfrac{164}{164+133})\\cdot 0.349 - (\\dfrac{133}{164+133})\\cdot 0.373 = 0.138$\n",
    "\n",
    "**Есть атеросклероз:**  \n",
    "\n",
    "$\\text{Impurity decrease} = 0.498 - (\\dfrac{123}{123+174})\\cdot 0.377 - (\\dfrac{174}{123+174})\\cdot 0.383 = 0.117$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L03/out/compute_impurity_decrease.png\" width=\"600\"></center>\n",
    "\n",
    "**Выбираем это разбиение как приводящее к наибольшему уменьшению Impurity**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наибольший $\\text{Impurity decrease}$ в признаке “боль в груди”. Значит, мы возьмем “боль в груди” как признак, на основании которого продолжим строить дерево.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L03/out/compute_gini_for_another_features.png\" width=\"600\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый раз мы будем выбирать новые признаки. В одном листе один признак лучше разделяет его на 2, в другом — другой. Тут мы остановились при достижении глубины дерева 2.\n",
    "\n",
    "Мы могли выбрать другой критерий остановки. Например, когда у нас получатся листья, в которых есть объекты только 1 класса. В этом случае не имеет смысла использовать другие разбиения. Либо может сложиться ситуация, когда разбиение по признаку, которое мы дополнительно взяли, ситуацию никак не улучшает."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Для вещественных признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L03/out/decision_tree_for_real_numbers.png\" width=\"1000\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вещественный признак сортируется, после чего мы выбираем оптимальный порог для разбиения. Для этого считаем `impurity_decrease` для каждого порога и выбираем лучший."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Регрессия\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для решения задач регрессии дерево строится практически так же, но есть несколько нюансов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L03/out/decision_tree_regression.png\" width=\"700\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как лучше всего одним числом охарактеризовать все эти объекты, попавшие в один лист?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L03/out/regression_metrics.png\" width=\"900\"></center>\n",
    "\n",
    "<center><em>Способы формирования предсказания в листе для регрессии</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы помним из статистики, у нас нет однозначного ответа на этот вопрос. Мы можем для распределения всех этих объектов в листе предсказывать наиболее частое значение, медиану, среднее значение.\n",
    "\n",
    "Обычно предсказывают среднее значение, потому что с ним легче всего работать и его чаще всего используют в статистике."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как корректно оценит **среднее** значение объектов **генеральной совокупности** для узла **по объектам тренировочной выборки**, попадающих в данный узел?\n",
    "\n",
    "$$\\large \\overline{y} = \\dfrac {\\sum_i y_i} {n}$$\n",
    "\n",
    "Нужно оценить дисперсию, которая показывает, насколько сильно мы можем ошибаться по сравнению с реальным (несмещенным) средним.\n",
    "\n",
    "$$\\large D(Y) = \\dfrac {\\sum_{i=1}^n(y_i-\\overline{y})} {n-1} $$\n",
    "\n",
    "Из-за неточности в оценке среднего при оценке дисперсии в знаменателе стоит $n-1$. Желательно иметь в каждом листе достаточное число объектов, чтобы компенсировать эту $-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь надо сформулировать критерий качества узла для регрессионного дерева. Мы хотим, чтобы значения в узле отличались как можно меньше, чтобы минимизировать ошибку предсказания (вычисленное нами **среднее является предсказанием** для объекта, попавшего в узел).\n",
    "\n",
    "$\\text{MSE}$ на тренировочной выборке, если мы предсказываем ее среднее, будет таким:\n",
    "\n",
    "$$\\large \\text{MSE} = \\frac 1 N \\sum_{l=1}^L \\sum_{i=1}^{n_l} (y_{li} - \\overline{y_l})^2 = \\frac 1 N \\sum_{l=1}^L \\sum_{i=1}^{n_l}\\dfrac{n_l-1}{n_l}D(Y_l),$$\n",
    "\n",
    "где $N$ — общее количество объектов в выборке, $L$ — общее число листьев, $n_l$ — число объектов в листе $l$.\n",
    "\n",
    "Уменьшение дисперсии листьев $D(Y_l)$ приводит к уменьшению $\\text{MSE}$.\n",
    "\n",
    "Дополнительно будем взвешивать дисперсии на размер узла. Иначе самыми выгодными будут разбиения, отправляющие в один из узлов только один объект."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Свойства деревьев решений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L03/out/why_decision_tree_powerful_method.png\" width=\"900\"></center>\n",
    "\n",
    "$\\displaystyle \\qquad \\qquad \\qquad \\qquad h(x) = \\sum \\sigma(\\dots \\sum(w^Tx)) \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad h(x) = \\sum_dc_dI\\{x \\in R_d\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У деревьев решений есть еще одно хорошее свойство. Позже вы познакомитесь с замечательной [теоремой об универсальном аппроксиматоре 📚[wiki]](https://ru.wikipedia.org/wiki/%D0%A2%D0%B5%D0%BE%D1%80%D0%B5%D0%BC%D0%B0_%D0%A6%D1%8B%D0%B1%D0%B5%D0%BD%D0%BA%D0%BE). Ее суть в том, что нейросеть с одним скрытым слоем сможет аппроксимировать любую заданную гладкую функцию.\n",
    "\n",
    "Для деревьев решений есть аналогичная теорема, говорящая о том, что дерево может аппроксимировать любую заданную кусочно-постоянную функцию.\n",
    "\n",
    "Дерево решений, в отличие от нейронной сети, может адаптироваться к выборке любого размера, любому количеству признаков. Для нейронной сети эта теорема предполагает, что можно бесконечно увеличивать скрытые слои."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Неустойчивость и переобучение деревьев<a class=\"anchor\" style=\"autocontent\" id=\"Неустойчивость-деревьев-решений\"/><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L03/out/instability_of_decision_trees.png\" width=\"900\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Деревья решений не используются в чистом виде, потому что они неустойчивы. Если у нас есть данные, и мы удалим из них несколько объектов, то дерево решений может сильно поменяться."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем учиться отделять Злость от остальных эмоций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = y != 1  # 0 for setosa, 1 - versicolor, 2 - virginica\n",
    "# first set of points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем два разных разбиения на обучение и тест. И посмотрим, будут ли отличаться деревья, построенные для данных разбиений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train1, x_test1, y_train1, y_test1 = train_test_split(\n",
    "    X, target, random_state=0\n",
    ")\n",
    "vect = TfidfVectorizer(min_df=0.0002, max_df=0.2)\n",
    "X_train_vect1 = vect.fit_transform(x_train1)\n",
    "\n",
    "clf1 = DecisionTreeClassifier(max_depth=2)\n",
    "clf1.fit(X_train_vect1.toarray(), y_train1)\n",
    "\n",
    "# second set of points\n",
    "x_train2, x_test2, y_train2, y_test2 = train_test_split(\n",
    "    X, target, random_state=42\n",
    ")\n",
    "vect = TfidfVectorizer(min_df=0.0002, max_df=0.2)\n",
    "X_train_vect2 = vect.fit_transform(x_train2)\n",
    "\n",
    "clf2 = DecisionTreeClassifier(max_depth=2)\n",
    "clf2.fit(X_train_vect2.toarray(), y_train2)\n",
    "\n",
    "cn = ['anger', 'joy', 'neutrality', 'sadness', 'uncertainty']\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5), dpi=100)\n",
    "tree.plot_tree(clf1, class_names=cn, filled=True, ax=axes[0])\n",
    "tree.plot_tree(clf2, class_names=cn, filled=True, ax=axes[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что даже деревья максимальной глубины 2 уже не совпадают между собой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если алгоритм при небольшом изменении признаков сильно меняет свое решение,  то это указывает на возможное переобучение. Алгоритм сильно реагирует на любой шум в данных— доверять его решениям опасно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias, Variance, Irreducible error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно показать, что ошибка любой модели раскладывается в сумму трех компонент:\n",
    "\n",
    "$$ \\large \\text{Model error} = \\text{Bias} + \\text{Variance} + \\text{Irreducible error} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычно высокий bias имеют модели, которые недостаточно сложны по сравнению с реальной закономерностью данных. Например, реальная зависимость, которую мы наблюдаем, нелинейная, а мы пытаемся аппроксимировать ее прямой линией. В этом случае наше **решение** заведомо **смещено** (biased) в сторону линейной модели, и мы будем систематически ошибаться в сравнении с реальной моделью данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L03/out/high_bias.png\" width=\"600\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно получить и обратную ситуацию. Если модель будет слишком сложная (в смысле своей выразительной способности) и \"гибкая\", то она сможет подстроиться под данные и выучить тренировочную выборку полностью. В этом случае модель будет подстраиваться под любой шум в данных и пытаться объяснить его какой-то сложной закономерностью.\n",
    "\n",
    "Малое изменение в данных будет приводить к большим изменениям в прогнозе модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L03/out/high_variance.png\" width=\"600\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias vs variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В практических задачах не получается бесконечно уменьшать и Bias, и Variance — приходится искать **компромисс (bias-variance tradeoff)**. С какого-то момента при уменьшении Bias начнет увеличиваться Variance, и наоборот. Задача исследователя — найти точку оптимума.\n",
    "\n",
    "Можно построить зависимость этих величин от сложности модели (capacity). По мере увеличения сложности Variance имеет тенденцию к возрастанию, а Bias — к убыванию. Более сложные модели подстраиваются под случайные шумы обучающей выборки, а более простые — не могут воспроизвести реальные закономерности.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L03/out/bias_variance_tradeoff.png\" width=\"450\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Управлять эффектом variance и bias можно как с помощью выбора модели, так и с помощью выбора гиперпараметров модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Продемонстрируем источники компонент Bias и Variance на примере решения задачи регрессии зашумленной косинусоиды с помощью решающих деревьев. Создадим функцию для генерации небольшой обучающей выборки и отобразим ее на графике:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "num_points = 300\n",
    "num_grid = 500\n",
    "x_max = 3.14\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "\n",
    "def get_sample(num_points, x_max, std=0.3, x_sample=None):\n",
    "    if x_sample is None:\n",
    "        x_sample = (np.random.rand(num_points) - 0.5) * 2 * x_max\n",
    "    y_sample = np.cos(x_sample.flatten()) + np.random.randn(x_sample.shape[0]) * std\n",
    "    return x_sample.reshape(-1, 1), y_sample\n",
    "\n",
    "\n",
    "x_grid = np.linspace(-x_max, x_max, num_grid).reshape(-1, 1)\n",
    "x_sample, y_sample = get_sample(num_points=num_points, x_max=x_max)\n",
    "_, y_true = get_sample(num_points=num_points, x_max=x_max, std=0, x_sample=x_grid)\n",
    "\n",
    "plt.scatter(x_sample, y_sample, c=\"#5D5DA6\", alpha=0.5, label=\"Noised data\")\n",
    "plt.plot(x_grid, y_true, \"#F9B041\", linewidth=4, label=\"Real function\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим три решающих дерева максимальной глубины (параметр `max_depth=None`) на разных выборках. Сравним предсказания моделей друг с другом и с реальной целевой функцией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "num_points = 30\n",
    "num_models = 3\n",
    "plt.figure(figsize=(24, 6))\n",
    "\n",
    "model = DecisionTreeRegressor(max_depth=None)\n",
    "y_pred = np.zeros((num_models, num_grid))\n",
    "sample_color = [\"#00E134\", \"#3A0DE2\", \"#FF00B3\"]\n",
    "for model_num in range(num_models):\n",
    "    x_sample, y_sample = get_sample(num_points=num_points, x_max=x_max)\n",
    "    model.fit(x_sample, y_sample)\n",
    "    y_pred[model_num] = model.predict(x_grid)\n",
    "    _, y_true = get_sample(num_points=num_points, x_max=x_max, std=0, x_sample=x_grid)\n",
    "\n",
    "    plt.subplot(1, 3, model_num + 1)\n",
    "    plt.scatter(\n",
    "        x_sample, y_sample, c=sample_color[model_num], label=f\"sample {model_num+1}\"\n",
    "    )\n",
    "    plt.plot(\n",
    "        x_grid,\n",
    "        y_pred[model_num],\n",
    "        c=sample_color[model_num],\n",
    "        alpha=0.8,\n",
    "        label=f\"model trained on sample {model_num+1}\",\n",
    "    )\n",
    "    plt.plot(x_grid, y_true, \"#F9B041\", linewidth=4, label=\"real mean\")\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.ylim(-1.5, 1.8)\n",
    "    plt.legend(loc=\"lower center\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предсказания моделей отличаются друг от друга и от истинной кривой средних значений косинуса.\n",
    "\n",
    "Обучим 1000 моделей c разной глубиной ($1, 5$ и максимальной) на разных подвыборках наших данных. Выберем одну тестовую точку $x=0$ и посмотрим, как предсказания моделей в этой точке распределены относительно истинного значения моделируемой функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "num_models = 1000\n",
    "\n",
    "for max_depth in [1, 5, None]:\n",
    "    model = DecisionTreeRegressor(max_depth=max_depth)\n",
    "\n",
    "    y_pred = np.zeros((num_models, num_grid))\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    gs = gridspec.GridSpec(1, 2, width_ratios=[2, 1])\n",
    "    plt.subplot(gs[0])\n",
    "\n",
    "    for model_num in range(num_models):\n",
    "        x_sample, y_sample = get_sample(num_points=num_points, x_max=x_max)\n",
    "        model.fit(x_sample, y_sample)\n",
    "        y_pred[model_num] = model.predict(x_grid)\n",
    "        plt.plot(x_grid, y_pred[model_num], alpha=0.1, c=\"#2DA9E1\", linewidth=1)\n",
    "\n",
    "    _, y_true = get_sample(num_points=num_points, x_max=x_max, std=0, x_sample=x_grid)\n",
    "    plt.plot(x_grid, y_true, c=\"#F9B041\", linewidth=3, label=\"real mean\")\n",
    "    plt.axvline(x=x_grid[num_grid // 2], c=\"#5D5DA6\", linewidth=3, label=\"X text point\")\n",
    "    plt.xlim((-x_max, x_max))\n",
    "    plt.ylim((-1, 2))\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.gca().set_title(f\"{num_models} models: max depth = {max_depth}\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "\n",
    "    plt.subplot(gs[1])\n",
    "    var = y_pred[:, num_grid // 2].var()\n",
    "    bias = np.abs(y_true[num_grid // 2] - y_pred[:, num_grid // 2].mean())\n",
    "    plt.hist(\n",
    "        y_pred[:, num_grid // 2],\n",
    "        bins=15,\n",
    "        color=\"#2DA9E1\",\n",
    "        alpha=0.5,\n",
    "        orientation=\"horizontal\",\n",
    "        label=f\"predictions: \\nvar = {var:.2f}\\nbias = {bias:.2f}\",\n",
    "    )\n",
    "    plt.axhline(y=y_true[num_grid // 2], c=\"#F9B041\", linewidth=3, label=\"real mean\")\n",
    "    plt.ylim((-1, 2))\n",
    "    plt.xlabel(\"hist counts\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.gca().set_title(f\"predictions at test point\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По мере увеличения глубины дерева:\n",
    "- уменьшается абсолютное значение сдвига среднего значения предсказаний моделей относительно истинного значения (Bias) — в среднем предсказания моделей становятся более точными,\n",
    "- увеличивается дисперсия предсказаний моделей (Variance) — предсказания моделей становятся менее устойчивы и сильнее зависят от конкретной обучающей выборки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "*   **Деревья малой глубины имеют малую сложность и высокий Bias.**\n",
    "*   **Деревья большой глубины имеют высокую сложность и высокий Variance.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ансамбли"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Корректирующий код"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть у нас есть сигнал:\n",
    "\n",
    "\n",
    "<font color=2BA8E0 size=30>1110110011</font>\n",
    "\n",
    "Но при передаче на другое устройство в нем могут возникать ошибки:\n",
    "\n",
    "<font color=2BA8E0 size=30>1</font><font color=red size=30>0</font><font color=2BA8E0 size=30>1011</font><font color=red size=30>1</font><font color=2BA8E0 size=30>011</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самое простое решение возникшей проблемы:\n",
    "\n",
    " 1. Шум, который вносит ошибки, скорее всего не зависит от места в сигнале\n",
    " 2. Передадим 3 раза один и тот же сигнал\n",
    "\n",
    "<font color=2BA8E0 size=30>1</font><font color=red size=30>0</font><font color=2BA8E0 size=30>1011</font><font color=red size=30>1</font><font color=2BA8E0 size=30>011</font>\n",
    "\n",
    "<font color=2BA8E0 size=30>1110</font><font color=red size=30>0</font><font color=2BA8E0 size=30>10011</font>\n",
    "\n",
    "<font color=2BA8E0 size=30>11</font><font color=red size=30>0</font><font color=2BA8E0 size=30>0110</font><font color=red size=30>1</font> <font color=2BA8E0 size=30>11</font>\n",
    "\n",
    "\n",
    "\n",
    " 3. Усредним, что получилось (в каждом случае возьмем наиболее часто встречающуюся цифру)\n",
    "\n",
    "<font color=2BA8E0 size=30>1110110011</font>\n",
    "\n",
    "\n",
    " 4. С большой долей вероятности итоговый сигнал восстановится\n",
    " 5. Чем больше копий сигналов передастся, тем выше вероятность, что сигнал восстановится полностью корректно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Усреднение предсказания классификаторов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Постановка задачи:\n",
    "\n",
    "Есть 10 объектов, в реальности все принадлежат классу 1:\n",
    "\n",
    "<font color=2BA8E0 size=30>1111111111</font>\n",
    "\n",
    "Пусть у нас есть три **независимых** классификатора A, B и C. Каждый предсказывает 1 в 70% случаев.\n",
    "\n",
    "Мы хотим получить общий классификатор на основании этих трех.\n",
    "\n",
    "Мы хотим получить предсказание базовых классификаторов и применить к ним какую-то функцию, которая выдаст итоговый ответ. Вид этой функции задается заранее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем просто усреднять предсказание наших классификаторов:\n",
    "\n",
    "$$\\large h(x) = \\dfrac 1 T \\sum_{i=1}^{T}a_i(x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Простое голосование\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L03/out/simple_voting.png\" width=\"450\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем вероятность того, что:\n",
    "\n",
    " 1. Все три классификатора верны: $0.7 * 0.7 * 0.7 = 0.3429$\n",
    " 2. Два классификатора верны: $0.7 * 0.7 * 0.3 + 0.7 * 0.3 * 0.7 + 0.3 * 0.7 * 0.7 = 0.4409$\n",
    "\n",
    "Таким образом, если брать большинство голосов, то мы будем в 78% случаев предсказывать верно. Мы взяли 3 классификатора, которые сами по себе были не очень хорошими, и получили классификатор лучшего качества.\n",
    "Если взять больше классификаторов, то ситуация будет еще лучше.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть теперь у нас три классификатора, выдающие следующие предсказания\n",
    "\n",
    "<font color=2BA8E0 size=30>11111111</font><font color=red size=30>00</font> — $80\\%$ точность\n",
    "\n",
    "<font color=2BA8E0 size=30>11111111</font><font color=red size=30>00</font> — $80\\%$ точность\n",
    "\n",
    "<font color=2BA8E0 size=30>1</font></font><font color=red size=30>0</font><font color=2BA8E0 size=30>111111</font><font color=red size=30>00</font> — $70\\%$ точность"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если объединим предсказания, то получим:\n",
    "\n",
    "<font color=2BA8E0 size=30>11111111</font><font color=red size=30>00</font> — $80\\%$ точность\n",
    "\n",
    "Потому что очень **высокая зависимость предсказаний**. Выше видно, что два классификатора предсказывают абсолютно одинаково. Вероятность, что они делают это случайно, очень мала."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А вот если возьмем такие классификаторы, то все получится:\n",
    "\n",
    "<font color=2BA8E0 size=30>11111111</font><font color=red size=30>00</font> — $80\\%$ точность\n",
    "\n",
    "<font color=red size=30>0</font><font color=2BA8E0 size=30>111</font><font color=red size=30>0</font><font color=2BA8E0 size=30>111</font><font color=red size=30>0</font><font color=2BA8E0 size=30>1</font> — $70\\%$ точность\n",
    "\n",
    "<font color=2BA8E0 size=30>1</font><font color=red size=30>000</font><font color=2BA8E0 size=30>1</font><font color=red size=30>0</font><font color=2BA8E0 size=30>1111</font> — $60\\%$ точность\n",
    "\n",
    "\n",
    "Усреднение:\n",
    "\n",
    "<font color=2BA8E0 size=30>11111111</font><font color=red size=30>0</font><font color=2BA8E0 size=30>1</font> — $90\\%$ точность"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Зависимость качества ансамбля от качества индивидуального предсказателя и от числа предсказателей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(y_real, p, cnt):\n",
    "    size = y_real.shape[0]\n",
    "    guessed = np.random.choice([True, False], (cnt, size), p=[p, 1 - p])\n",
    "    y = np.repeat(y_real.reshape(1, -1), cnt, axis=0)\n",
    "    y[~guessed] = 1 - y[~guessed]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "size = 1000\n",
    "reps = 10\n",
    "\n",
    "cnt_base_predictors = [1] + list(range(5, 105, 5))\n",
    "single_qual = [0.45, 0.5, 0.51, 0.55, 0.6, 0.75, 0.9]\n",
    "\n",
    "dt = {\"cnt\": [], \"single_qual\": [], \"accuracy\": []}\n",
    "\n",
    "for i in range(reps):\n",
    "    y_real = np.random.choice([0, 1], size)\n",
    "    for cnt in cnt_base_predictors:\n",
    "        for p in single_qual:\n",
    "            preds = get_predictions(y_real, p, cnt)\n",
    "            voting = np.round(preds.mean(axis=0))\n",
    "            accuracy = (y_real == voting).mean()\n",
    "            dt[\"cnt\"].append(cnt)\n",
    "            dt[\"single_qual\"].append(f\"{p:.02}\")\n",
    "            dt[\"accuracy\"].append(accuracy)\n",
    "\n",
    "results = pd.DataFrame(dt)\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "sns.lineplot(data=results, x=\"cnt\", y=\"accuracy\", hue=\"single_qual\", lw=3, alpha=0.5)\n",
    "plt.xlabel(\"Number of base classifiers\", size=20)\n",
    "plt.ylabel(\"Accuracy\", size=20)\n",
    "plt.legend(loc=\"best\", fontsize=12, title=\"Single classifier quality\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что:\n",
    "1. Чем лучше базовый классификатор, тем меньше нужно классификаторов при прочих равных условиях для достижения высокого качества.\n",
    "2. Если качество базового классификатора даже чуть больше 0.5, то качество ансамбля растет с увеличением числа моделей в ансамбле.\n",
    "3. Если качество базового классификатора неотличимо от случайного (0.5), то качество ансамбля будет оставаться равным 0.5.\n",
    "4. Если качество базового классификатора ниже случайного (0.5), то качество ансамбля стремится к 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Коррелированность моделей\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как зависит качество предсказания от коррелированности предсказателей. Конкретно — от ожидаемой коррелированности вероятностей ошибиться на данном объекте для любой взятой пары классификаторов из ансамбля."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "\n",
    "def get_correlated_predictions(y_real, p, cnt, r):\n",
    "    size = y_real.shape[0]\n",
    "    x1 = np.random.uniform(0, 1, size)\n",
    "    x2 = np.random.uniform(0, 1, (cnt, size))\n",
    "    q = np.sqrt(r)\n",
    "    y = q * x1 + (1 - q**2) ** 0.5 * x2  # y variables now correlated with correlation=r\n",
    "    y_mod = np.zeros_like(y)\n",
    "    for i in range(y.shape[0]):\n",
    "        y_mod[i] = scipy.stats.rankdata(y[i])\n",
    "\n",
    "    y = y_mod / size  # back to uniform, slightly affects correlations\n",
    "\n",
    "    y_pred = np.repeat(y_real.reshape(1, -1), cnt, axis=0)\n",
    "    y_pred[y < 1 - p] = 1 - y_pred[y < 1 - p]  # to predictions, affects correlations\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "x = np.arange(0, 1, 0.05)\n",
    "accuracy = np.zeros_like(x)\n",
    "p = 0.7\n",
    "cnt = 100\n",
    "for ind, r in enumerate(x):\n",
    "    preds = get_correlated_predictions(y_real, p, cnt, r)\n",
    "    voting = np.round(preds.mean(axis=0))\n",
    "    accuracy[ind] = (y_real == voting).mean()\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.title(f\"Accuracy of {cnt} classifiers ensemble\", size=20)\n",
    "plt.xlabel(\"Correlation among classifiers\", size=20)\n",
    "plt.ylabel(\"Accuracy\", size=20)\n",
    "plt.axhline(y=p, color=\"red\", lw=5, ls=\"--\", label=\"Single classifier\")\n",
    "sns.lineplot(x=x, y=accuracy, lw=5, label=\"Ensemble\")\n",
    "plt.legend(fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что по мере увеличения коррелированности моделей качество все больше и больше приближается к качеству одной базовой модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Случайный лес"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так мы приходим к идее случайного леса. Мы будем брат множество \"плохих\" деревьев, каждое из которых переобучается, каждое из которых не похоже на соседнее и объединять их ответы. Нам достаточно того, чтобы отдельные деревья выдавали ответ лучше случайного.\n",
    "\n",
    "Чтобы добиться ещё лучшего качества леса, будем при построении деревьев в узлах рассматривать случайное подмножество признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L03/out/random_forest.png\" width=\"900\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Случайный лес устойчив к шуму и может обрабатывать данные с пропусками."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы лес работал лучше, следует придерживаться следующее правил:\n",
    "\n",
    "\n",
    "*   Больше деревьев в лесу\n",
    "*   Чем глубже, тем лучше\n",
    "*   Не ограничивать минимальный размер листа\n",
    "\n",
    "Случайный лес переобучить сложно, но можно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "emo = pd.read_csv('Emotion_Classification_Russian.csv')\n",
    "\n",
    "emo['num_label'] = emo['label'].astype('category').cat.codes\n",
    "X, y = emo['lemmatized'], emo['num_label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify = y)\n",
    "\n",
    "vect = TfidfVectorizer(min_df=0.0002, max_df=0.2)\n",
    "X_train_vect = vect.fit_transform(X_train)\n",
    "X_test_vect = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "models_rf = {}\n",
    "\n",
    "# this can be done faster, see warm_start parameter for this\n",
    "# (https://stackoverflow.com/questions/42757892/how-to-use-warm-start)\n",
    "for n_estimators in [3, 5, 10, 50, 100]:\n",
    "    models_rf[f\"RF{n_estimators}\"] = RandomForestClassifier(\n",
    "        n_estimators=n_estimators, random_state=42, n_jobs=-1\n",
    "    )  # run in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим несколько случайных лесов с различным количеством деревьев и простое дерево решений для сравнения с ним (baseline)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим все модели.\n",
    "Для этого напишем вспомогательную функцию, которая будет обучать переданные ей модели и считать для них качество на тесте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_and_test_classifier(models, x_train, y_train, x_test, y_test, verb=True):\n",
    "    scores = {}\n",
    "    for name, model in models.items():\n",
    "        model.fit(x_train.toarray(), y_train)  # train the model\n",
    "        y_pred = model.predict(x_test.toarray())  # get predictions\n",
    "        scores[name] = f1_score(y_test, y_pred, average = 'weighted')\n",
    "        print(f\"Fitted {name} with F1 score {scores[name]:.3f}\")\n",
    "    print (scores)\n",
    "    results = pd.DataFrame(scores, index=[0])\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "results_rf = train_and_test_classifier(models_rf, X_train_vect, y_train, X_test_vect, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "sns.barplot(data=results_rf)\n",
    "plt.ylabel(\"F1\", size=20)\n",
    "plt.xlabel(\"n_estimators\", size=20)\n",
    "plt.title(\"Number of estimators vs F1\", size=20)\n",
    "plt.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['anger', 'joy', 'neutrality', 'sadness', 'uncertainty']\n",
    "rf_pred = models_rf[\"RF100\"].predict(X_test_vect.toarray())\n",
    "print(classification_report(y_test, rf_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бустинг — это еще один способ ансамблирования моделей. В отличие от случайного леса, где каждая модель в ансамбле строится независимо, в бустинге построение очередной модели зависит от уже состоящих в ансамбле моделей. Каждая следующая модель в ансамбле стремится улучшить предсказание всего ансамбля."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бустинг позволяет нам на основе большого числа \"слабых\" моделей получить одну \"сильную\". Под слабыми моделями подразумеваем модели, точность которых может быть лишь немногим выше случайного угадывания.\n",
    "\n",
    "В качестве моделей традиционно используются деревья решений, но не большой глубины, а наоборот — маленькой, чтобы каждое из них не могло выучить выборку хорошо.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L03/out/join_weak_learners.png\" width=\"650\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из всех бустингов больше всего прославился и доказал свою эффективность **градиентный бустинг**. Он позволяет получить решение, которое сложно побить другими видами моделей.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boosting (градиентный бустинг)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнем с интуиции. Предположим, что мы играем в гольф. Наша задача — загнать мячик в лунку, причем, когда мячик далеко от нее, мы можем ударить посильнее, чтобы сократить расстояние, но когда мы уже близко к лунке, то стараемся бить клюшкой аккуратнее, чтобы не промахнуться. После каждого удара мы оцениваем расстояние до лунки и приближаем мячик в ее сторону. Применительно к нашей теме, удар клюшкой по мячику — это каждая модель в градиентном бустинге.\n",
    "\n",
    "[[colab] 🥨 Демонстрация интуиции градиентного бустинга](https://colab.research.google.com/drive/1hILdJzsuAsXabA4aIwtb9RGgUn_bvhCL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L03/out/summarize_predictions_of_ensemble_models.png\" width=\"650\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цель $i$-той модели в ансамбле — скорректировать ошибки предыдущих ${i-1}$ моделей. В результате, когда мы суммируем вклады всех моделей ансамбля, получается хорошее предсказание."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте формализуем, что нам нужно, чтобы обучить алгоритм градиентного бустинга:\n",
    "\n",
    "- набор данных $\\left\\{\\left(x_i, y_i\\right)\\right\\}_{i=1, \\ldots, n}$,\n",
    "- число итераций $M$ (оно же количество моделей),\n",
    "- выбор дифференцируемой функции потерь $L(y, f)$,\n",
    "- выбор семейства функций базовых алгоритмов $h(x, \\theta)$ с процедурой их обучения.\n",
    "\n",
    "Минимизировать ошибку будем с помощью градиентного спуска."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L03/out/gradient_boosting.png\" width=\"600\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разберем пошагово.\n",
    "\n",
    "Начинаем с первого предсказания. Можем выбрать на самом деле любое число, например, среднее значение:\n",
    "\n",
    "$$ \\large \\hat{f}(x)=\\hat{f}_0, \\hat{f}_0=\\gamma, \\gamma \\in \\mathbb{R}\n",
    " \\tag{1}$$\n",
    "\n",
    "Либо подобрать с наименьшей ошибкой:\n",
    "\n",
    "$$\\large \\hat{f}_0=\\underset{\\gamma}{\\arg \\min } \\sum_{i=1}^n L\\left(y_i, \\gamma\\right) $$\n",
    "\n",
    "Посмотрим на графике, как бы могла выглядеть наша зависимость. Посчитаем ошибку и отобразим наше предсказание.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L03/out/gb_explanation_step1.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В нашем примере мы видим, что нужно двигаться вправо, чтобы уменьшить ошибку. В действительности мы этого не знаем, но можем посчитать градиент и узнать направление возрастания функции ошибки. Нас же интересует обратное направление или градиент со знаком минус (антиградиент).\n",
    "\n",
    "$$\\large r_{i t}=-\\frac{\\partial L\\left(y_i, f\\left(x_i\\right)\\right)}{\\partial f\\left(x_i\\right)} \\quad \\tag{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L03/out/gb_explanation_step2.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Строим новый базовый алгоритм $h_t(x)$, который обучается на то, чтобы уменьшить ошибку от текущего состояния ансамбля, и в качестве целевой переменной для него берем антиградиент функции потерь  $\\left\\{\\left(x_i, r_{i t}\\right)\\right\\}_{i=1, \\ldots, n}$\n",
    "\n",
    "$$\\large h_t(x) = \\underset{\\theta}{\\arg \\min } \\sum_{i=1}^n L\\left(h(x_i, \\theta), r_{it}\\right) \\tag{3}$$\n",
    "\n",
    "Подбираем оптимальный параметр $\\large \\rho$. Этот параметр будет разным у каждого дерева в нашем ансамбле, в отличие от learning rate:\n",
    "\n",
    " $$\\large \\rho_t=\\underset{\\rho}{\\arg \\min } \\sum_{i=1}^n L\\left(y_i, \\hat{f}\\left(x_i\\right)+\\rho \\cdot h_t\\left(x_i, \\theta\\right)\\right) \\tag{4}$$\n",
    "\n",
    "Сдвигаем предсказание в сторону уменьшения ошибки, где $\\lambda$ — это learning rate:\n",
    "\n",
    " $$\\large \\hat{f}_t(x)= \\lambda \\cdot \\rho_t \\cdot h_t(x) \\tag{5}$$\n",
    "\n",
    "Обновляем текущее приближение $\\hat{f}(x)$:\n",
    "$$ \\large\n",
    "\\hat{f}(x) \\leftarrow \\hat{f}(x)+\\hat{f}_t(x)=\\sum_{i=0}^t \\hat{f}_i(x) \\tag{6}\n",
    "$$\n",
    "\n",
    "Далее повторяем шаги 2–6, пока не получим требуемое качество, и собираем итоговый ансамбль $\\hat{f}(x)$:\n",
    "\n",
    "$$\n",
    "\\large\n",
    "\\hat{f}(x)=\\sum_{i=0}^M \\hat{f}_i(x)\n",
    "$$\n",
    "\n",
    "[[blog] ✏️ Подробнее о градиентном бустинге](https://habr.com/ru/company/ods/blog/327250/)\n",
    "\n",
    "\n",
    "\n",
    "[[demo] 🎮 Gradient Boosting explained](https://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html)\n",
    "\n",
    "[[demo] 🎮 Gradient Boosting Interactive Playground](https://arogozhnikov.github.io/2016/07/05/gradient_boosting_playground.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда мы говорили про базовые модели, то предполагали, что этими базовыми моделями являются деревья решений. Однако это необязательно, можно использовать и другие алгоритмы в качестве базовых моделей.\n",
    "\n",
    "Деревья решений имеют ряд преимуществ:\n",
    "\n",
    "- Гибкость — деревья решений могут описывать сложные нелинейные взаимосвязи между объектами и целевой переменной. Они могут обрабатывать как числовые, так и категориальные данные и работать с пропущенными значениями.\n",
    "\n",
    "- Интерпретируемость — деревья решений просты в понимании и интерпретации. Они дают четкое представление о том, как модель пришла к определенному прогнозу.\n",
    "\n",
    "- Устойчивость к выбросам — деревья решений менее чувствительны к выбросам, чем другие алгоритмы машинного обучения. Они способны изолировать их и предотвратить их влияние на всю модель.\n",
    "\n",
    "- Эффективность — деревья решений могут быть построены быстро, они способны обрабатывать большие наборы данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "\n",
    "models[\"DT\"] = DecisionTreeClassifier(\n",
    "    max_depth=10,\n",
    "    min_samples_leaf=10,\n",
    ")\n",
    "\n",
    "models[\"RF\"] = RandomForestClassifier(\n",
    "    n_estimators=100,  # for better result set to 1000\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=1,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# models[\"GradientBoosting\"] = GradientBoostingClassifier(\n",
    "#     learning_rate=0.1,  # for better result set to 0.05\n",
    "#     n_estimators=100,  # for better result set to 1000\n",
    "#     random_state=42,\n",
    "# )\n",
    "\n",
    "models[\"GradientBoosting\"] = GradientBoostingClassifier(\n",
    "    learning_rate=0.5,  # for better result set to 0.05\n",
    "    n_estimators=10,  # for better result set to 1000\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "results_gb = train_and_test_classifier(models, X_train_vect, y_train, X_test_vect, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['anger', 'joy', 'neutrality', 'sadness', 'uncertainty']\n",
    "gb_pred = models[\"GradientBoosting\"].predict(X_test_vect.toarray())\n",
    "print(classification_report(y_test, gb_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "ax = sns.barplot(data=results_gb)\n",
    "plt.xlabel(\"\", size=20)\n",
    "plt.ylabel(\"F1\", size=20)\n",
    "plt.title(\"DT vs RF vs GB\", size=25)\n",
    "plt.xticks(size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Переобучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В то же время **Gradient boosting**, в отличие от случайного леса, может сильно переобучиться. Это важно понимать. Для небольших датасетов часто может оказаться, что случайный лес дает более надежные результаты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L03/out/gradient_boosting_overfitting.png\" width=\"600\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В отличии от случайного леса, для градиентного бустинга п**одходят слабые модели** (деревья с малой глубиной)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shrinkage (learning rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как у градиентного спуска есть learning rate, который определяет силу каждого нашего следующего шага,  так и у градиентного бустинга есть параметр, который называется shrinkage, на который мы домножаем вес, с которым добавляются новые модели в ансамбль.\n",
    "\n",
    "Если не домножать вес каждой модели дополнительно на этот параметр, то мы можем попасть в ситуацию, когда будем пролетать мимо минимума функции ошибки (та же опасность, что и в обычном градиентном спуске)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L03/out/shrinkage_learning_rate_for_gradient_boosting.png\" width=\"450\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как и в случае с градиентным спуском, **learning rate** влияет не только на то, как быстро мы станем переобучаться, но и на глубину минимума, который мы найдем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модификации градиентного бустинга"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть много модификаций градиентного бустинга. В отличие от реализации в `sklearn`, большая часть из них умеет параллелиться на CPU или даже на GPU.\n",
    "\n",
    "Поэтому при работе с реальными данными использовать градиентный бустинг из `sklearn` не стоит. XGBoost и/или LigthGBM дадут результат как правило лучше и быстрее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[doc] 🛠️ Официальная документация](https://catboost.ai/en/docs/)\n",
    "\n",
    "Разработан Яндексом.\n",
    "\n",
    "1. Хорошо умеет работать с категориальными признаками. Если у вас много категориальных признаков, он может дать существенный выигрыш.\n",
    "2. Умеет работать с текстом без предварительной обработки. Достаточно указать, что колонка содержит текстовый признак.\n",
    "3. По умолчанию использует в качестве модели модификацию обычного дерева решения — Symmetric Tree, которое менее склонно к переобучению."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/datasets/Emotion_Classification_Russian.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "emo = pd.read_csv('Emotion_Classification_Russian.csv')\n",
    "\n",
    "emo['num_label'] = emo['label'].astype('category').cat.codes\n",
    "X, y = emo['lemmatized'], emo['num_label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify = y)\n",
    "\n",
    "vect = TfidfVectorizer(min_df=0.0002, max_df=0.2)\n",
    "X_train_vect = vect.fit_transform(X_train)\n",
    "X_test_vect = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "models_cb = {}\n",
    "models_cb[\"CatBoost\"] = CatBoostClassifier(\n",
    "    iterations=200, #2000\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    verbose=0,\n",
    "    task_type=\"GPU\",\n",
    ")\n",
    "# task_type=\"GPU\") # can use gpu, but no parallel-cpu option\n",
    "\n",
    "results_cb = train_and_test_classifier(models_cb, X_train_vect, y_train, X_test_vect, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['anger', 'joy', 'neutrality', 'sadness', 'uncertainty']\n",
    "cb_pred = models_cb[\"CatBoost\"].predict(X_test_vect.toarray())\n",
    "print(classification_report(y_test, cb_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'num_label'\n",
    "text_cols = ['lemmatized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo = pd.read_csv('Emotion_Classification_Russian.csv')\n",
    "\n",
    "emo['num_label'] = emo['label'].astype('category').cat.codes\n",
    "X, y = emo['lemmatized'], emo['num_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "msk = np.random.rand(len(emo)) < 0.8\n",
    "\n",
    "train = emo[msk]\n",
    "test = emo[~msk]\n",
    "\n",
    "train = train.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,\n",
    "X_test,\n",
    "y_train,\n",
    "y_test = train.drop (columns = ['num_label', 'label', 'text']), test.drop(columns = ['num_label', 'label', 'text']), train['num_label'], test['num_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "y_preds = []\n",
    "models = []\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "catboost_params = {\n",
    "    'iterations': 100,#1000\n",
    "    'learning_rate': 0.1,\n",
    "    'objective': 'MultiClass',\n",
    "    #'eval_metric': 'F1', #'Logloss'\n",
    "    'custom_metric': 'F1',\n",
    "    'task_type': 'GPU',\n",
    "    'early_stopping_rounds': 10,\n",
    "    'use_best_model': True,\n",
    "    'verbose': 100\n",
    "}\n",
    "\n",
    "for fold_id, (train_index, valid_index) in enumerate(cv.split(X_train, y_train)):\n",
    "    X_tr = X_train.loc[train_index]\n",
    "    X_val = X_train.loc[valid_index]\n",
    "    y_tr = y_train[train_index]\n",
    "    y_val = y_train[valid_index]\n",
    "\n",
    "    train_pool = Pool(\n",
    "        X_tr,\n",
    "        y_tr,\n",
    "        #cat_features=categorical_cols,\n",
    "        text_features=text_cols,\n",
    "        feature_names=list(X_tr)\n",
    "    )\n",
    "    valid_pool = Pool(\n",
    "        X_val,\n",
    "        y_val,\n",
    "        #cat_features=categorical_cols,\n",
    "        text_features=text_cols,\n",
    "        feature_names=list(X_tr)\n",
    "    )\n",
    "\n",
    "    model = CatBoostClassifier(**catboost_params)\n",
    "    model.fit(train_pool, eval_set=valid_pool)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_preds.append(y_pred)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(y_preds).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cb_pred = models_cb[\"CatBoost\"].predict(X_test_vect.toarray())\n",
    "print(classification_report(y_test, y_preds[-1], target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для оптимизации гиперпараметров существуют готовые решения, которые используют различные методы black-box оптимизации. Разберем одну из наиболее популярных библиотек — [**Optuna** 🛠️[doc]](https://optuna.org/).\n",
    "\n",
    "Есть интеграции с scikit-learn, XGBoost и LightGBM, но мы рассмотрим общий случай, который подойдет для любой модели, даже нейронной сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "# Define function which will optimized\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # boundaries for the optimizer's\n",
    "    depth = trial.suggest_int(\"depth\", 5, 8, step=1)\n",
    "    min_data_in_leaf = trial.suggest_int(\"min_data_in_leaf\", 6, 10, step=2)\n",
    "    l2_leaf_reg = trial.suggest_categorical(\"l2_leaf_reg\", [2, 5, 7, 10])\n",
    "    random_strength = trial.suggest_float(\"random_strength\", 1, 2)\n",
    "\n",
    "    # create new model(and all parameters) every iteration\n",
    "    model = CatBoostClassifier(\n",
    "        iterations=200,#\n",
    "        learning_rate=0.1,\n",
    "        depth=depth,\n",
    "        min_data_in_leaf=min_data_in_leaf,\n",
    "        l2_leaf_reg=l2_leaf_reg,\n",
    "        random_strength=random_strength,\n",
    "        random_state=42,\n",
    "        verbose=0,\n",
    "    )\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    f1 = cross_val_score(\n",
    "        model, X_train_vect.toarray(), y_train, cv=kf, scoring=\"f1\"\n",
    "    ).mean()\n",
    "    error = f1\n",
    "\n",
    "    return error\n",
    "\n",
    "\n",
    "# Create \"exploration\"\n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\", study_name=\"Optimizer\", sampler=TPESampler(42)\n",
    ")\n",
    "\n",
    "study.optimize(\n",
    "    objective, n_trials=7\n",
    ")  # The more iterations, the higher the chances of catching the most optimal hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим на историю оптимизации нашей метрики:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можем посмотреть, с какими параметрами какие результаты получились, и сделать выводы, в каких диапазонах лучше подбирать параметры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [\"depth\", \"min_data_in_leaf\", \"l2_leaf_reg\", \"random_strength\"]\n",
    "optuna.visualization.plot_slice(study, params=params, target_name=\"MSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем лучшие параметры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show best params\n",
    "study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optuna оценивает важность параметров для лучшего результата:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим CatBoost на значениях, подобранных Optuna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_add5 = {}\n",
    "models_add5[\"CatBoost tuned\"] = catboost.CatBoostRegressor(\n",
    "    iterations=2000,\n",
    "    learning_rate=0.1,\n",
    "    depth=study.best_params[\"depth\"],\n",
    "    min_data_in_leaf=study.best_params[\"min_data_in_leaf\"],\n",
    "    l2_leaf_reg=study.best_params[\"l2_leaf_reg\"],\n",
    "    random_strength=study.best_params[\"random_strength\"],\n",
    "    random_state=42,\n",
    "    verbose=0,\n",
    ")\n",
    "# task_type=\"GPU\") # can use gpu, but no parallel-cpu option\n",
    "\n",
    "cat_tuned_add = train_and_test_classifier(models_add5, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "ax = sns.boxplot(data=pd.concat([xgb_add2, lgb_add, cat_add, cat_tuned_add]))\n",
    "plt.xlabel(\"\", size=20)\n",
    "plt.ylabel(\"MSE\", size=20)\n",
    "plt.title(\"Catboost tuned and others\", size=20)\n",
    "plt.xticks(size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество незначительно улучшилось."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Проблемы при работе с реальными данными"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В реальных задачах вы столкнетесь с широким спектром трудностей, и одним из первых этапов будет подготовка данных. Уже на этом этапе есть ряд возможных проблем, например:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Нехватка данных** — у вас может быть мало данных. Более того, понять, какой объем данных необходим — тоже отдельная задача. Другая вариация похожей проблемы — это большой объем данных, но без разметки.\n",
    "\n",
    "- **Некачественная разметка** —  даже в широко известных MNIST, CIFAR-10 и ImageNet есть [ошибки в разметке 🎮[demo]](https://labelerrors.com/). В вашем датасете они тоже будут. Важно понимать, что разметка может не являться бесспорным эталоном.\n",
    "\n",
    "- **Шум в данных** — полезно подумать о потенциальной зашумленности данных. Например, когда люди заполняют таблицы, они могут допускать ошибки. Хорошо понимать, какого рода ошибки могут содержаться в ваших данных.\n",
    "\n",
    "- **Несбалансированность датасета** — какие-то классы могут быть плохо представлены (**минорные классы**). Например, если в вашем датасете будет всего 10 фотографий одного класса и 10 000 другого, то модели будет очень заманчиво вообще не пытаться определять минорный класс (всего 0.1% ошибок).\n",
    "\n",
    "- **Ковариантный сдвиг** — явление, когда признаки тренировочной и тестовой выборок **распределены по-разному**. Из-за этого мы можем получать плохие предсказания на тестовой выборке, когда модель просто не знает, что признаки могут находиться в другом распределении."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L04/out/covariate_shift.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Практический совет: для быстрого обнаружения ковариантного сдвига можно **обучить модель**, которая будет предсказывать, относится ли объект к **train** или **test** выборке. Если модель легко делит данные по такому принципу, то имеет смысл визуализировать значения признаков, по которым она это делает.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Малое число источников данных** — проблема, родственная предыдущей. В вашем датасете могут быть данные только от одного прибора или одной модели прибора. Могут быть данные, снятые только одним специалистом, или в одной больнице, или только у взрослых. Это также может влиять на способность вашего алгоритма обобщать полученное решение и требует пристального внимания.\n",
    "\n",
    "- **Грязные данные** — в данных могут быть полные/неполные дубликаты, пропуски, ошибки форматов, перемешанные столбцы и многое другое. Такие проблемы могут быть как естественной характеристикой (нет данных о каких-то объектах, отсюда пропуски), так и ошибкой при агрегации данных из разных источников. Важно подумать о проверках и тестах, чтобы быть уверенным в качестве.\n",
    "\n",
    "Все это приводит к целому спектру сложностей, из которых самой типичной будет переобучение модели — какую бы простую модель вы не взяли, она все равно будет выучивать искажения вашего датасета."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дисбаланс классов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прежде всего надо убедиться, что датасет сбалансирован:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "\n",
    "def show_class_balance(y, classes):\n",
    "    _, counts = torch.unique(torch.tensor(y), return_counts=True)\n",
    "    plt.bar(classes, counts)\n",
    "    plt.ylabel(\"n_samples\")\n",
    "    plt.ylim([0, 75])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "wine = load_wine()\n",
    "classes = wine.target_names\n",
    "\n",
    "show_class_balance(wine.target, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разница в 10–20% будет незначительна, поэтому для наглядности мы искусственно разбалансируем наш датасет при помощи метода `make_imbalance` [🛠️[doc]](https://imbalanced-learn.org/stable/references/generated/imblearn.datasets.make_imbalance.html) из библиотеки [imbalanced-learn 🛠️[doc]](https://imbalanced-learn.org/stable/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.datasets import make_imbalance\n",
    "\n",
    "x, y = make_imbalance(\n",
    "    wine.data, wine.target, sampling_strategy={0: 10, 1: 70, 2: 40}, random_state=42\n",
    ")\n",
    "show_class_balance(y, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Изменение баланса класса сэмплированием"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если в данных нехватка именно конкретного класса, то можно бороться с этим при помощи разных способов сэмплирования.\n",
    "\n",
    "Важно понимать, что в большинстве случаев данные, полученные таким способом, должны использоваться в качестве **обучающего набора**, но ни в коем случае не в качестве **валидации** или **теста**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Дублирование примеров меньшего класса (oversampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем увеличить число объектов меньшего класса за счет дублирования."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L04/out/oversampling_scheme.png\" width=\"600\"></center>\n",
    "\n",
    "<center><em>Дублирование примеров меньшего класса</em></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такой oversampling может быть выполнен с помощью класса `RandomOverSampler` [🛠️[doc]](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.RandomOverSampler.html) из imbalanced-learn, как показано ниже:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "x_ros, y_ros = ros.fit_resample(x, y)\n",
    "\n",
    "show_class_balance(y_ros, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Уменьшение числа примеров большего класса (undersampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично, можно взять для обучения не всех представителей большего класса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L04/out/undersampling_scheme.png\" width=\"600\"></center>\n",
    "\n",
    "<center><em>Удаление примеров преобладающего класса</em></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "x_res, y_res = rus.fit_resample(x, y)\n",
    "\n",
    "show_class_balance(y_res, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Минус подхода: мы можем выбросить важных представителей большего класса, ответственных за существенное улучшение генерализации, и из-за этого качество модели существенно ухудшится.\n",
    "\n",
    "Выбрасывать объекты большего класса  можно разными способами. Например, кластеризовать объекты большего класса и брать по заданному количеству объектов из каждого кластера."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Борьба с переобучением"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Сложность модели__ (*model complexity*) — важный гиперпараметр. В частности, для линейных моделей сложность может быть представлена **количеством параметров**, например, для полиномиальных моделей — степенью полинома."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сложность модели связана с **ошибкой обобщения** (_generalization error_):\n",
    "- **Cлишком простой модели** не будет хватать **количества параметров для обобщения сложной закономерности в данных**, что приведёт к большой ошибке обобщения.\n",
    "- **Избыточная сложность модели** также приводит к большой ошибке обобщения за счет того, что в силу своей сложности модель начинает **пытаться искать закономерности в шуме**, добиваясь большей точности на тренировочных данных, теряя при этом часть обобщающей способности.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/model_complexity.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметры модели задают некоторую **аппроксимацию целевой функции**. Аппроксимировать целевую функцию можно несколькими способами, например:\n",
    "1. Использовать все имеющиеся данные и провести ее строго **через все точки**, которые нам известны ($f1$ на картинке);\n",
    "2. Использовать более простую функцию (в данном случае, линейную), которая не попадет точно во все данные, но зато будет соответствовать некоторым **общим закономерностям**, которые у них есть ($f2$ на картинке).\n",
    "\n",
    "Характерной чертой переобучения является первый сценарий, и сопровождается он, как правило, **большими весами**. Поэтому регуляризацию используют для борьбы с переобучением."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/l2_regularization.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проиллюстрируем описанное явление на примере полиномиальной модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 2 * np.pi, 10)\n",
    "y = np.sin(x) + np.random.normal(scale=0.25, size=len(x))\n",
    "x_true = np.linspace(0, 2 * np.pi, 200)\n",
    "y_true = np.sin(x_true)\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.scatter(x, y, s=50, facecolors=\"none\", edgecolors=\"b\", label=\"noisy data\")\n",
    "plt.plot(x_true, y_true, c=\"lime\", label=\"ground truth\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем аппроксимировать имеющуюся зависимость с помощью полиномиальной модели, используя шумные данные в качестве тренировочных данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "x_train = x.reshape(-1, 1)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "for i, degree in enumerate([0, 1, 3, 9]):\n",
    "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "\n",
    "    model.fit(x_train, y)\n",
    "    y_plot = model.predict(x_true.reshape(-1, 1))\n",
    "\n",
    "    fig.add_subplot(2, 2, i + 1)\n",
    "    plt.plot(x_true, y_plot, c=\"red\", label=f\"M={degree}\")\n",
    "    plt.scatter(x, y, s=50, facecolors=\"none\", edgecolors=\"b\")\n",
    "    plt.plot(x_true, y_true, c=\"lime\")\n",
    "    plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель **переобучается, подстраиваясь под тренировочную выборку**. В полиноме степень/количество весов — это гиперпараметр, который можно подбирать на кросс-валидации, но **ограничивая количество параметров**, мы накладываем **ограничение на обобщающую способность модели** в целом. Вместо этого можно оставить модель сложной, но использовать **регуляризатор**, который будет заставлять модель отдавать предпочтение более простому обобщению."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Регуляризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем ограничить значения весов, введя в функционал ошибок\n",
    "специальную добавку, называемую регуляризацией. L2-регуляризация имеет формулу:\n",
    "\n",
    "\n",
    "$$\\large L_2 = \\alpha \\sum_i w_i^2,$$\n",
    "где $\\alpha$ — это коэффициент регуляризации.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Введение **L2-регуляризации** приводит к тому, что **большие веса больше штрафуются** и предпочтение отдается решениям, использующим **малые значения весов**. При этом модель будет **сохранять скоррелированные и неважные признаки с маленькими весами**.\n",
    "\n",
    "Это связано с градиентом $L_2$:\n",
    "$$\\large L'_{2w_i} = 2\\alpha w_i$$\n",
    "Он будет “тянуть” модель в сторону большого количества маленьких весов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/l1_and_l2_regularization.gif\" alt=\"alttext\" width=\"550\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для отбора признаков можно использовать L1-регуляризацию, она одинаково \"штрафует\" модель за любые ненулевые веса.\n",
    "\n",
    "$$\\large L_1 = \\alpha \\sum_i |w_i|$$\n",
    "$$\\large L_{1w_i}' = \\alpha, \\text{  где } w_i\\neq 0$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для получения интуиции, что L1-регуляризация позволяет отбирать признаки, обычно используют картинку ниже. На ней член функции потерь, отвечающий за регуляризацию, жестко ограничен: $|L_\\text{reg}|≤1$. Вращающиеся овалы показывают, как Loss решаемой задачи изменяется в результате изменения входных данных и таргета.\n",
    "\n",
    "Голубая область — ограничение на значения весов, которое дает регуляризация:\n",
    "- Для **L2** это **окружность**.  Оранжевая точка — это минимальное значение для функции Loss с регуляризацией. Для **L2** она будет кататься **по касательной к окружности**.\n",
    "- Для **L1** ограничения на значения весов будут иметь **форму ромба**. При этом минимальное значение для функции Loss с регуляризацией будет зависать в **уголу ромба**, что соответствует **обнулению веса** одного из признаков.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L02/loss_landscape_with_regularization.gif\" width=\"800\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://people.eecs.berkeley.edu/~jrs/189/\">Introduction to Machine Learning\n",
    "</a></em></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "\n",
    "model = make_pipeline(PolynomialFeatures(9), LinearRegression())\n",
    "model_ridge = make_pipeline(PolynomialFeatures(9), Ridge(alpha=0.1))\n",
    "\n",
    "model.fit(x_train, y)\n",
    "y_plot = model.predict(x_true.reshape(-1, 1))\n",
    "\n",
    "model_ridge.fit(x_train, y)\n",
    "y_plot_ridge = model_ridge.predict(x_true.reshape(-1, 1))\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(x_true, y_plot, c=\"red\", label=f\"M={degree}\")\n",
    "plt.plot(x_true, y_plot_ridge, c=\"black\", label=f\"M={degree}, alpha=0.1\")\n",
    "plt.scatter(x, y, s=50, facecolors=\"none\", edgecolors=\"b\")\n",
    "plt.plot(x_true, y_true, c=\"lime\", label=\"ground truth\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "poly_coef = model[1].coef_\n",
    "\n",
    "eq = f\"y = {round(poly_coef[0], 2)}+{round(poly_coef[1], 2)}*x\"\n",
    "for i in range(2, 10):\n",
    "    eq += f\"+{round(poly_coef[i], 2)}*x^{i}\"\n",
    "\n",
    "print(\"Without regularization: \", eq)\n",
    "\n",
    "poly_coef = model_ridge[1].coef_\n",
    "\n",
    "eq = f\"y = {round(poly_coef[0], 2)}+{round(poly_coef[1], 2)}*x\"\n",
    "for i in range(2, 10):\n",
    "    eq += f\"+{round(poly_coef[i], 2)}*x^{i}\"\n",
    "\n",
    "print(\"With regularization: \", eq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что одним из \"симптомов\" переобучения являются аномально большие веса. Модель Ridge Regression, показанная в примере выше, использует L2-регуляризацию для борьбы с этим явлением.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Литература</font>\n",
    "\n",
    "<font size=\"5\">Инструменты:</font>\n",
    "\n",
    "- [[doc] 🛠️ Scikit-learn](https://scikit-learn.org/stable/) — ML алгоритмы\n",
    "- [[doc] 🛠️ NumPy](https://numpy.org/) — массивы и математические функции\n",
    "- [[doc] 🛠️ Pandas](https://pandas.pydata.org/) — табличные данные\n",
    "- [[doc] 🛠️ Matplotlib](https://matplotlib.org/) — визуализация\n",
    "- [[doc] 🛠️ NLTK](https://www.nltk.org/) — токенизация, словари стоп-слов\n",
    "- [[git] 🐾 RNNmorph](https://github.com/IlyaGusev/rnnmorph) — морфологический парсер для русского и английского языков\n",
    "- [[doc] 🛠️ UDPipe](https://ufal.mff.cuni.cz/udpipe) — морфологический и синтаксический парсер для различных языков\n",
    "\n",
    "<font size=\"5\">Методы и алгоритмы:</font>\n",
    "\n",
    "- [[wiki] 📚 Мешок слов](https://ru.wikipedia.org/wiki/Мешок_слов)\n",
    "- [[wiki] 📚 TF-IDF](https://ru.wikipedia.org/wiki/TF-IDF)\n",
    "- [[wiki] 📚 Наивный байесовский классификатор](https://ru.wikipedia.org/wiki/Наивный_байесовский_классификатор)\n",
    "- [[wiki] 📚 Логистическая регрессия](https://ru.wikipedia.org/wiki/Логистическая_регрессия)\n",
    "\n",
    "<font size=\"5\">Подробные материалы:</font>\n",
    "- [[wiki] 📚 MSU.AI Линейные модели](https://msu.ai/l02_linear_models)\n",
    "- [[wiki] 📚 MSU.AI Деревья и леса](https://msu.ai/l03_classic_ml)\n",
    "\n",
    "* [[book] 📚 Про разделение на классическое и глубокое машинное обучение ](https://www.deeplearningbook.org/contents/intro.html)\n",
    "* [[book] 📚 Один из лучших учебников по классическому машинному обучению](https://web.stanford.edu/~hastie/ElemStatLearn/)\n",
    "* [[book] 📚 Хорошая книга по машинному обучению](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646)\n",
    "\n",
    "<font size=\"5\">Про деревья решений:</font>\n",
    "\n",
    "* [[blog] ✏️ Деревья решений](https://www.kaggle.com/kashnitsky/topic-3-decision-trees-and-knn)\n",
    "\n",
    "<font size=\"5\">Про ансамбли:</font>\n",
    "\n",
    "* [[blog] ✏️ Kaggle Ensembling guide](https://www.kaggle.com/code/amrmahmoud123/1-guide-to-ensembling-methods/notebook)\n",
    "* [[blog] ✏️ Comprehensive guide for ensemble models](https://www.projectpro.io/article/a-comprehensive-guide-to-ensemble-learning-methods/432)\n",
    "* [[blog] ✏️ Про стэкинг и блендинг](https://dyakonov.org/2017/03/10/c%D1%82%D0%B5%D0%BA%D0%B8%D0%BD%D0%B3-stacking-%D0%B8-%D0%B1%D0%BB%D0%B5%D0%BD%D0%B4%D0%B8%D0%BD%D0%B3-blending/)\n",
    "\n",
    "<font size=\"5\">XGBoost:</font>\n",
    "\n",
    "* [[doc] 🛠️ Официальная документация](https://xgboost.readthedocs.io/en/stable/)\n",
    "* [[blog] ✏️ Отличие XGBoost от обычного градиентного бустинга](https://stats.stackexchange.com/questions/202858/xgboost-loss-function-approximation-with-taylor-expansion)\n",
    "* [[arxiv] 🎓 Оригинальная статья](https://arxiv.org/pdf/1603.02754.pdf)\n",
    "* [[blog] ✏️ Как подбирать параметры XGBoost](https://usermanual.wiki/Document/Complete20Guide20to20Parameter20Tuning20in20XGBoost20with20codes20in20Python.1988513968)\n",
    "\n",
    "<font size=\"5\">CatBoost:</font>\n",
    "\n",
    "* [[video] 📺 Решение задач классификации при помощи CatBoost](https://www.youtube.com/watch?v=xl1fwCza9C8)\n",
    "* [[doc] 🛠️ Официальная документация](https://catboost.ai/en/docs/)\n",
    "\n",
    "<font size=\"5\">LightGBM:</font>\n",
    "\n",
    "* [[doc] 🛠️ Очень подробная и удобная документация](https://lightgbm.readthedocs.io/en/latest/ )\n",
    "* [[blog] ✏️ Описание параметров](https://neptune.ai/blog/lightgbm-parameters-guide?utm_source=datacamp&utm_medium=post&utm_campaign=blog-lightgbm-parameters-guide&utm_campaign=News&utm_medium=Community&utm_source=DataCamp.com)\n",
    "* [[arxiv] 🎓 Новый бустинг с деревьями, содержащими в листах линейные регрессии](https://arxiv.org/pdf/1802.05640.pdf)\n",
    "\n",
    "<font size=\"5\">Дисбаланс классов:</font>\n",
    "\n",
    "* [[blog] ✏️ Обучение в случае дисбаланса классов](http://www.svds.com/learning-imbalanced-classes/)\n",
    "* [[blog] ✏️ Bagging и случайные леса для обучения с дисбалансом классов](https://machinelearningmastery.com/bagging-and-random-forest-for-imbalanced-classification/)\n",
    "* [[article] 🎓 Коэффициент корреляции Мэтьюса](https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-019-6413-7)\n",
    "\n",
    "<font size=\"5\">Нейронные сети и бустинг:</font>\n",
    "\n",
    "* [[arxiv] 🎓 TabNet](https://arxiv.org/pdf/1908.07442.pdf)\n",
    "* [[git] 🐾 TabNet, реализация на PyTorch](https://github.com/dreamquark-ai/tabnet)\n",
    "* [[arxiv] 🎓 NODE](https://arxiv.org/pdf/1909.06312.pdf)\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
