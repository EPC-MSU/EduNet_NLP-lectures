{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Машинное обучение</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Два пути"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Допустим, у нас есть задача автоматически опеределить тональность (эмоциональную окраску) отзыва на некоторый товар или услугу: является он позитивным, негативным или нейтральным. К её решению можно подойти двумя способами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-content/L01/out/sentiment_task.png\" width=\"650\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вариант №1: подход на основе правил (rule-based)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать **вручную** заданные **правила** классификации и эмоционально размеченные **словари**. Эти правила рассчитывают класс текста на основе эмоциональных ключевых слов и их совместного использования с другими ключевыми словами.\n",
    "\n",
    "Несмотря на высокую эффективность в текстах какой-то определенной тематики, методы на основе правил плохо способны обобщать. Чем больше примеров мы будем анализировать, тем больше исключений будет появляться.\n",
    "\n",
    "Следовательно, потребуется добавлять новые правила и увеличивать размер словаря. Алгоритмическая сложность программы будет расти, ее будет сложнее поддерживать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-content/L01/out/sentiment_rules.png\" width=\"750\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вариант №2: машинное обучение (machine learning)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С появлением **машинного обучения** мы можем применить принципиально другой подход. Он заключается в том, чтобы обучить модель, которая сама будет извлекать шаблоны из данных.\n",
    "\n",
    "Зачастую пользователь не только пишет текст отзыва, но и ставит оценку от 1 до 10. Мы можем использовать ее в качестве разметки для данных: отзывы с оценкой от 1 до 3 будем считать негативными, от 4 до 7 — нейтральными, от 8 до 10 — позитивными.\n",
    "\n",
    "Соберем некоторое количество отзывов и оценок за определенное время. Загрузим данные в модель, и она обучится на этих данных. При достаточном количестве данных и адекватно подобранной модели мы сможем научить ее решать конкретные задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-content/L01/out/sentiment_model.png\" width=\"550\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По сути, модели всё равно, что сделать: проанализировать тональность отзыва, отфильтровать спам-письма, распределить новости по тематикам, определить язык текста и т.д. Нет необходимости писать под каждый пример отдельную программу: достаточно собрать данные, и мы сможем решить множество абсолютно разных задач.\n",
    "\n",
    "Важно лишь понимать, какую модель предпочтительнее выбрать. С этим мы будем разбираться в ходе курса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача курса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  AI, ML, ANN, DL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Место глубокого обучения и нейронных сетей в ИИ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-content/L01/out/ai_ml_ann_dl.png\" width=\"550\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Искусственный интеллект (ИИ)/ Artificial Intelligence (AI)**  — область компьютерных наук, связанная с моделированием интеллектуальных или творческих видов человеческой деятельности.\n",
    "\n",
    "**Машинное обучение/ Machine learning (ML)** — подраздел ИИ, связанный с разработкой алгоритмов и статистических моделей, которые компьютерные системы используют для выполнения задач без явных инструкций.\n",
    "\n",
    "**Искусственная нейронная сеть (ИНС)/  Artificial neural network (ANN)** — разновидность алгоритмов машинного обучения, математическая модель, построенная по принципу организации и функционирования биологических нейронных сетей. ИНС состоит из слоев «нейронов», которые связаны между собой. В простом случае это входной слой и выходной слой.\n",
    "\n",
    "**Глубокое обучение/ Deep Learning (DL)** — обучение «глубоких» ИНС. Помимо входного и выходного слоя, они состоят из сотен дополнительных «скрытых» слоев между видимыми слоями для ввода и вывода."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует множество определений сильного и слабого ИИ, рассуждений о появлении искусственного сознания и восстании машин.\n",
    "\n",
    "Всё намного **приземлённее**. Есть набор **объектов $X$** (для чего надо сделать предсказание) и набор **ответов $Y$** (что надо предсказать). Пары \"объект-ответ\" составляют **обучающую выборку**.\n",
    "\n",
    "Мы будем заниматься **восстановлением решающей функции $F$**, которая переводит признаки $X$, описывающие объекты, в ответы $Y$.\n",
    "\n",
    "$$ F: X \\xrightarrow\\ Y $$\n",
    "\n",
    "По ходу курса мы будем уточнять постановку задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Области применения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В последнее время именно такого рода модели показывают высокую эффективность в тех областях, с которыми ранее могли справиться только люди.\n",
    "Алгоритмы машинного обучения могут обрабатывать данные различного типа:\n",
    "- Компьютерное зрение / Computer vision (CV) → изображения и видео\n",
    "- Обработка естественного языка / Natural language processing (NLP) → тексты\n",
    "- Распознавание и синтез речи / Automatic Speech Recognition (ASR) & Text to speech (TTS) → аудио"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обзор курса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<big>Лекция 1 Машинное обучение</big>\n",
    "\n",
    "**Зачем:**\n",
    "\n",
    "* Разобраться, как в целом подходить к задачам машинного обучения.\n",
    "\n",
    "**Что будет:**\n",
    "\n",
    "* Основные понятия, обучение с учителем и без учителя;\n",
    "* Инструменты (numpy, pandas);\n",
    "* Разведывательный анализ, работа с данными;\n",
    "* Базовые метрики;\n",
    "* Методы векторизации: мешок слов, TF-IDF;\n",
    "* Методы машинного обучения: наивный байесовский классификатор, логистическая регрессия;\n",
    "* Построение классификатора и оценка качества.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L01/l01_meme.png\" width=\"400\" ></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<big>Лекция 2 Нейронные сети</big>\n",
    "\n",
    "**Зачем:**\n",
    "\n",
    "* Познакомиться с нейронными сетями — классом моделей машинного обучения, которые позволяют решать разнообразные задачи, начать их создавать и обучать.\n",
    "\n",
    "**Что будет:**\n",
    "\n",
    "* Интуиция, почему нейросети — очень выразительный класс моделей машинного обучения;\n",
    "* Основные «строительные блоки» нейросетей;\n",
    "* Основной метод обучения нейросетей — метод обратного распространения ошибки;\n",
    "* Знакомство с **PyTorch** — основной программной библиотекой глубокого обучения, которой будем пользоваться на курсе;\n",
    "* Построение векторных представлений слов на основе нейросетей;\n",
    "* Процесс создания и обучения нейронной сети для задачи классификации по тональности отзывов на фильмы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-content/L01/out/nn_fully_connected.png\" width=\"450\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Для остальных лекций пока нет расширенных таблиц с планом, и содержание может меняться, поэтому не стала вписывать их.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задачи машинного обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-content/L01/out/machine_learning.png\" width=\"700\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение с учителем (supervised learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый элемент выборки представляет собой пару «объект, ответ». Требуется найти функциональную зависимость ответов от описаний объектов и построить алгоритм, принимающий на входе описание объекта и выдающий на выходе ответ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Классификация (classification)** — отнесение образца к одному из нескольких попарно не пересекающихся множеств. Множество допустимых ответов конечно. Их называют метками классов (class label)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-content/L01/out/classification.png\" width=\"650\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Примеры задач классификации:* определение наличия заболевания у пациента, оценивание кредитоспособности заемщика, предсказание оттока клиентов.\n",
    "\n",
    "*Примеры задач классификации из NLP:* фильтрация спама, анализ тональности, классификация по тематике, определение языка."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Регрессия (regression)** — соотнесение объекта с некоторым числом или числовым вектором. Отсутствуют жесткие ограничения на пространство ответов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-content/L01/out/regression.png\" width=\"650\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Примеры задач регрессии:* оценка времени выздоровления, определение кредитного лимита, ожидаемый доход магазина на следующий месяц.\n",
    "\n",
    "*Примеры задач регрессии из NLP:* предсказание стоимости товара по текстовому описанию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение без учителя (unsupervised learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом случае ответы не задаются, и требуется искать зависимости между объектами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Кластеризация (clustering)** — разбиение множества входных данных на группы с учетом попарного сходства объектов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-content/L01/out/clustering.png\" width=\"650\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Примеры задач кластеризации:* выделение типичных групп покупателей,  разделение клиентов по уровню платёжеспособности, отнесение космических объектов к категории (планета, звёзда, чёрная дыра и т. п.).\n",
    "\n",
    "*Пример задачи кластеризации из NLP:* распределение новостей по тематическим кластерам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы подробно рассмотрим алгоритмы **обучения с учителем** для задачи **классификации**. В этом случае есть множество объектов $X$, множество ответов $y$ и функция, которая каждому объекту сопоставляет ответ. Она называется **алгоритмом** или **моделью**.\n",
    "\n",
    "Имеющиеся данные разделяются на **обучающую** и **тестовую** выборку.\n",
    "- Обучающая выборка — это примеры, на основе которых алгоритм ищет зависимость ответов от описаний объектов и строит общую закономерность.\n",
    "- Тестовая выборка используется для оценки качества алгоритма на новых данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-content/L01/out/train_test.png\" width=\"700\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка качества"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как оценивать качество алгоритма? Допустим, мы хотим внести изменения в алгоритм. Как узнать, сделают ли эти изменения алгоритм лучше?\n",
    "\n",
    "В задачах машинного обучения для оценки качества моделей и сравнения  алгоритмов используются различные метрики. Мы рассмотрим некоторые из них вначале на примере бинарной классификации, а затем узнаем способы их усреднения для задач с несколькими классами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Матрица ошибок"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для подсчета метрик качества классификации используется матрица ошибок.\n",
    "\n",
    "Есть алгоритм, предсказывающий принадлежность каждого объекта одному из классов.\n",
    "- $\\hat y$ — предсказанный алгоритмом класс объекта\n",
    "- $y$ — истинный класс объекта\n",
    "\n",
    "Два класса делятся на положительный (1) и отрицательный (0 или –1).\n",
    "- Объекты, которые алгоритм относит к положительному классу, – положительные (Positive).\n",
    "- Те, которые на самом деле принадлежат к этому классу, – истинно положительные (True Positive).\n",
    "- Остальные – ложно положительные (False Positive).\n",
    "\n",
    "Аналогичная терминология для отрицательного (Negative) класса.\n",
    "\n",
    "Таким образом, ошибки классификации бывают двух видов: False Negative (FN) и False Positive (FP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\\begin{array}{|c|c|} \\hline\n",
    "& y=1 & y=0 \\\\ \\hline\n",
    "\\hat{y}=1 & \\text{True Positive (TP)} & \\text{False Positive (FP)}  \\\\ \\hline\n",
    "\\hat{y}=0 & \\text{False Negative (FN)} & \\text{True Negative (TN)} \\\\ \\hline\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример задачи: определение беременности у пациентки.\n",
    "- Пациентка беремена → положительный класс (positive).\n",
    "- Пациентка не беремена → отрицательный класс (negative).\n",
    "\n",
    "Модель может «предсказать» наличие беременности (true) или нет (false).\n",
    "\n",
    "Пусть какой-то набор медицинских данных характерен для наличия беременности.\n",
    "- Модель верно определила и поставила положительный класс → истинно положительный исход (true positive).\n",
    "- Модель ставит отрицательную метку класса → ложно отрицательный исход (false negative).\n",
    "\n",
    "В случае отсутствия беременности для рассматриваемого набора данных исходы модели остаются аналогичными.\n",
    "- Модель относит запись к положительному классу → ложно положительный исход (false positive): модель «сказала», что пациентка беремена, но на самом деле нет.\n",
    "- Модель определят запись как отрицательный класс → истинно отрицательный исход (true negative)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-content/L01/out/1_2_errors.png\" width=\"400\" ></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy — доля правильных ответов алгоритма среди всех ответов (непоказательна в задачах с неравными классами):\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Допустим, мы хотим оценить работу спам-фильтра почты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*КАРТИНКА с подсчетом accuracy для двух моделей, одна из которых определяет все письма как не-спам.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При этом модель 2 совершенно не обладает никакой предсказательной силой, так как изначально мы хотели определять письма со спамом. Преодолеть это поможет переход с общей для всех классов метрики к отдельным показателям качества классов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае дисбаланса классов есть специальный аналог – метрика balanced accuracy.\n",
    "\n",
    "$$\\text{Balanced Accuracy} = \\dfrac{1}{2} (\\dfrac{TP}{TP + FN} + \\dfrac{TN}{TN + FP})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*КАРТИНКА с подсчетом balanced accuracy для обоих моделей*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Точность, полнота, F-мера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Точность — доля объектов, названных классификатором положительными и при этом действительно являющимися положительными.\n",
    "\n",
    "$$\\text{Precision} = \\frac{TP}{TP + FP}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важность метрики определяется тем, насколько высока «цена» ложно положительного результата. Например, если стоимость дальнейшей проверки наличия беременности у пациентки высока и мы не можем проверить все ложно положительные результаты, стоит максимизировать данную метрику."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полнота — доля объектов положительного класса, которые нашел алгоритм, из всех объектов положительного класса.\n",
    "\n",
    "$$\\text{Recall} = \\frac{TP}{TP + FN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо уделить особое внимание этой оценке, когда в поставленной задаче ошибка нераспознания положительного класса высока. Например, если у рассматриваемой группы пациенток беременность может протекать особенно тяжело."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-content/L01/out/precision-recall.png\" width=\"650\" ></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В реальной жизни максимальная точность и полнота не достижимы одновременно и приходится искать некий баланс. Хотелось бы иметь некую метрику которая объединяла бы в себе информацию о точности и полноте нашего алгоритма.\n",
    "\n",
    "F-мера — среднее гармоническое точности и полноты:\n",
    "\n",
    "$$\\text{F-score}=2\\frac{\\text{Precision}×\\text{Recall}}{\\text{Precision}+\\text{Recall}}$$\n",
    "​\n",
    "Данная формула придает одинаковый вес точности и полноте, поэтому F-мера будет падать одинаково при уменьшении и точности и полноты. Можно рассчитать F-меру придав различный вес точности и полноте, если отдать приоритет одной из этих метрик при разработке алгоритма.\n",
    "\n",
    "$$\\text{F-score}=(β^2+1)\\frac{\\text{Precision}×\\text{Recall}}{β^2\\text{Precision}+\\text{Recall}}$$\n",
    "​\n",
    "$β$ принимает значения в диапазоне, $0<β<1$ если нужно отдать приоритет точности, а при $β>1$ приоритет отдается полноте. При $β=1$ формула сводится к предыдущей, что дает сбалансированную F-меру (также ее называют F1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Методы усреднения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В многоклассовых задачах подсчет качества сводится к вычислению одной из двухклассовых метрик.\n",
    "\n",
    "Пусть выборка состоит из  $K$  классов. Задача классификации ставится как $K$  задач об отделении класса  $i$  от остальных $(i=1,...,K)$. Для каждой из них можно посчитать свою матрицу ошибок.\n",
    "\n",
    "Выделяют три подхода:\n",
    "\n",
    "1. Микроусреднение\n",
    "\n",
    "Сначала элементы матрицы ошибок усредняются по всем классам. Например $TP = \\frac{1}{K}\\sum_{i=1}^KTP_i$. Затем по одной усреднённой матрице ошибок считаем точность, полноту, F-меру.\n",
    "\n",
    " $$\\text{Micro-precision} = \\frac{\\sum_{i=1}^KTP_i}{\\sum_{i=1}^KTP_i+\\sum_{i=1}^KFP_i}$$\n",
    "\n",
    " $$\\text{Micro-recall} = \\frac{\\sum_{i=1}^KTP_i}{\\sum_{i=1}^KTP_i+\\sum_{i=1}^KFN_i}$$\n",
    "\n",
    "2. Макрусреднение\n",
    "\n",
    "Сначала вычисляется итоговая метрика для каждого класса, а затем результаты усредняются по всем классам.\n",
    "\n",
    "$$\\text{Macro-precision} = \\frac{\\sum_{i=1}^K\\text{Precision}_i}{K}$$\n",
    "\n",
    "$$\\text{Macro-recall} = \\frac{\\sum_{i=1}^K\\text{Recall}_i}{K}$$\n",
    "\n",
    "3. Взвешивание\n",
    "\n",
    "Метрики для каждого класса умножаются на его вес, а затем складываются.\n",
    "\n",
    "$$\\text{Weighted-precision} = \\sum_{i=1}^K{w_i*\\text{Precision}_i}$$\n",
    "\n",
    "$$\\text{Weighted-recall} = \\sum_{i=1}^K{w_i*\\text{Recall}_i}$$\n",
    "\n",
    "$$w_i = \\frac{количество \\; объектов\\; класса\\; i}{общее \\; количество \\; объектов}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Библиотека scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для классификации будем использовать готовые методы из библиотеки [[doc] 🛠️ scikit-learn](https://scikit-learn.org/stable/) для машинного обучения.\n",
    "\n",
    " У них стандартные функции:\n",
    " - `fit` обучает модель на обучающей выборке\n",
    " - `predict` предсказывает классы на тестовой выборке\n",
    "\n",
    "Составление матрицы ошибок и подсчет всех метрик также может осуществляться инструментами sklearn.\n",
    "- матрица ошибок [🛠️[doc]](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)\n",
    "- точность [🛠️[doc]](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html)\n",
    "- полнота [🛠️[doc]](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)\n",
    "- F-мера [🛠️[doc]](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)\n",
    "\n",
    "Чтобы не считать все метрики по отдельности, можно сразу получить отчет о классификации [🛠️[doc]](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html):\n",
    "- метрики для каждого класса\n",
    "  - точность (precision)\n",
    "  - полнота (recall)\n",
    "  - F-мера (f1-score)\n",
    "  - количество объектов каждого класса (support)\n",
    "- усредненные метрики\n",
    "  - микроусредненные (micro avg)\n",
    "  - макроусредненные (macro avg)\n",
    "  - взвешенные (weighted avg)\n",
    "  \n",
    "Если микроусредненные точность, полнота и F-мера равны, выводится одно значение, равное также accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Методы векторизации текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый объект имеет некоторую числовую характеристику — признак. Совокупность всех признаков объекта называется его **признаковым описанием** и представляется в виде вектора.\n",
    "\n",
    "Если объектом является текст, в качестве признаков выступают слова, которые он содержит. Процесс преобразования текста в числа называется **векторизацией**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим два способа векторизации предложений из библиотеки scikit-learn. Для наглядности будем использовать небольшой корпус из трех предложений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "corpus = pd.Series(['She loves pizza, pizza is delicious.',\n",
    "                     'She is good person.',\n",
    "                     'Good people are the best.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Мешок слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мешок слов (bag of words) — представление текста, которое описывает вхождение слова в документ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{array}{c} \\hline\n",
    " & \\text{she} & \\text{loves} & \\text{pizza} & \\text{is} & \\text{delicious} & \\text{a} & \\text{good} & \\text{person} & \\text{people} & \\text{are} & \\text{the} & \\text{best} \\\\ \\hline\n",
    "\\text{She loves pizza, pizza is delicious.} & 1 & 1 & 2 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "\\text{She is a good person.} & 1 & 0 & 0 & 1 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 0 \\\\\n",
    "\\text{Good people are the best.} & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 1 & 1 & 1 & 1 \\\\ \\hline\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем векторизацию мешком слов с помощью класса `CountVectorizer` [🛠️[doc]](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). Метод `fit` собирает словарь, метод `transform` преобразует тексты в векторы на основе собранного словаря. Метод `fit_transform` выполняет все это сразу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bow = CountVectorizer()\n",
    "#bow.fit(corpus)\n",
    "#corpus_bow = bow.transform(corpus)\n",
    "corpus_bow = bow.fit_transform(corpus)\n",
    "corpus_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате мы получаем разрежённую (sparse) матрицу — это матрица с преимущественно нулевыми элементами. Если бо́льшая часть элементов матрицы ненулевая, она считается плотной (dense). Особенностью разреженных матриц является их компактность.\n",
    "\n",
    "[[blog] ✏️ Введение в разреженные матрицы](https://python-school.ru/blog/python/sparse-matrix/)\n",
    "\n",
    "Выведем результат для всех предложений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bow_df = pd.DataFrame(corpus_bow.toarray(),\n",
    "                      columns = bow.get_feature_names_out(),\n",
    "                      index=corpus)\n",
    "bow_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По умолчанию в качестве признаков используются слова (униграммы). С помощью параметра `ngram_range` можно считать частоту встречаемости для *n*-грамм. Необходимо задать значения `min_n` и `max_n` (`default=(1, 1)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow1 = CountVectorizer(ngram_range=(2,3))\n",
    "corpus_bow1 = bow1.fit_transform(corpus)\n",
    "corpus_bow1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow1_df = pd.DataFrame(corpus_bow1.toarray(),\n",
    "                      columns = bow1.get_feature_names_out(),\n",
    "                      index=corpus)\n",
    "bow1_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметр `analyzer` определяет, какая единица предложения является признаком — целое слово или подслово. По умолчанию он принимает значение `‘word’`. Для использования символьных *n*-грамм нужно установить значение `‘char’` (границы слов включаются в *n*-граммы) или `‘char_wb’` (создает n-граммы символов только из текста внутри границ слов)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow2 = CountVectorizer(ngram_range=(4,6), analyzer='char_wb')\n",
    "corpus_bow2 = bow2.fit_transform(corpus)\n",
    "corpus_bow2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow2_df = pd.DataFrame(corpus_bow2.toarray(),\n",
    "                      columns = bow2.get_feature_names_out(),\n",
    "                      index=corpus)\n",
    "bow2_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$TF{\\text -}IDF$ ($TF$ — term frequency, $IDF$ — inverse document frequency)  — это способ векторизации текста, отражающий важность слова в документе, а не только частоту его появления.\n",
    "\n",
    "Частота слов ($TF$) — это мера частоты употребления слова $w$ в документе $d$. $TF$ определяется как отношение появления слова в документе к общему количеству слов в документе.\n",
    "\n",
    "$$TF(w,d) = \\frac{количество\\:вхождений\\:слова\\:w\\:в\\:документе\\:d}{общее\\:количество\\:слов\\:n\\:в\\:документе\\:d}$$\n",
    "\n",
    "Обратная частота документов ($IDF$) —  это мера важности слова. Некоторые слова могут присутствовать наиболее часто, но не имеют большого значения. $IDF$ присваивает вес каждому слову в зависимости от его частоты в корпусе $D$.\n",
    "\n",
    "$$IDF(w,D) = ln(\\frac{общее\\:количество\\:документов\\:N\\:в\\:корпусе\\:D}{количество\\:документов,\\:содержащих\\:слово\\:w})$$\n",
    "\n",
    "$TF{\\text -}IDF$ является произведением $TF$ и $IDF$.\n",
    "$$TF{\\text -}IDF(w,d,D)=TF(w,d)*IDF(w,D)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для векторизации воспользуемся классом `TfidfVectorizer` [🛠️[doc]](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). Применим метод `fit_transform` и посмотрим на результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "corpus_tfidf = tfidf.fit_transform(corpus)\n",
    "corpus_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame(corpus_tfidf.toarray(),\n",
    "                      columns = tfidf.get_feature_names_out(),\n",
    "                      index=corpus)\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно ограничить размер словаря и включать только слова, которые встречаются не реже N раз(`min_df`). Также можно убрать слова, которые встречаются слишком часто и являются стоп-словами в пределах данного корпуса (`max_df`). Оба параметра могут быть выражены целым числом либо числом с плавающей точкой в диапазоне [0.0, 1.0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf1 = TfidfVectorizer(min_df=2)\n",
    "corpus_tfidf1 = tfidf1.fit_transform(corpus)\n",
    "corpus_tfidf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf1_df = pd.DataFrame(corpus_tfidf1.toarray(),\n",
    "                      columns = tfidf1.get_feature_names_out(),\n",
    "                      index=corpus)\n",
    "tfidf1_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Библиотеки для анализа данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[doc] 🛠️ NumPy](https://numpy.org/) — это библиотека для поддержки больших многомерных массивов и быстрых математических функций для операций с этими массивами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Базовые функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основным объектом NumPy является массив чисел одного типа (`np.ndarray`).\n",
    "\n",
    "Один из способов создать массив — из обычных списков Python, используя функцию `numpy.array()`.\n",
    "\n",
    "Количество измерений массива можно узнать с помощью свойства `.ndim`, размер массива — с помощью свойства `.shape`.\n",
    "\n",
    "Кроме размерностей, можно также узнать тип элементов (свойство `.dtype`) и их количество (свойство `.size`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = np.array([1, 2, 3])\n",
    "print(f'Array a1: {a1}')\n",
    "print(f'Data type: {a1.dtype}')\n",
    "print(f'Number of array dimensions: {a1.ndim}')\n",
    "print(f'Shape of the array: {a1.shape}')\n",
    "print(f'Number of elements in the array: {a1.size}')\n",
    "\n",
    "a2 = np.array([[1.0, 2.0, 3.0]])\n",
    "print(f'\\nArray a2: {a2}')\n",
    "print(f'Data type: {a2.dtype}')\n",
    "print(f'Number of array dimensions: {a2.ndim}')\n",
    "print(f'Shape of the array: {a2.shape}')\n",
    "print(f'Number of elements in the array: {a2.size}')\n",
    "\n",
    "a3 = np.array([[1],\n",
    "               [2],\n",
    "               [3]])\n",
    "print(f'\\nArray a3:\\n{a3}')\n",
    "print(f'Data type: {a3.dtype}')\n",
    "print(f'Number of array dimensions: {a3.ndim}')\n",
    "print(f'Shape of the array: {a3.shape}')\n",
    "print(f'Number of elements in the array: {a3.size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Доступ к элементам массива"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно обращаться к отдельным элементам массива с помощью специального оператора `[]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array([[1, 2, 3, 4, 5, 6, 7],\n",
    "              [8, 9, 10, 11, 12, 13, 14]])\n",
    "b[0, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме отдельных элементов, можно обратиться к целой строке или столбцу с помощью оператора среза `:`. Он позволяет выбрать все элементы указанной строки или столбца."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Array b:\\n{b}')\n",
    "print(f'Elements in first row:\\n{b[0, :]}')\n",
    "print(f'Elements in first column:\\n{b[:, 0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оператор `:` на самом деле представляет собой сокращённую форму конструкции начальный_индекс: конечный_индекс: шаг. Можно обращаться к любо выбранной последовательности элементов массива.\n",
    "\n",
    "Мы указали, что хотим выбрать первую строку, а затем уточнили, какие именно столбцы нам нужны: `1:4:2`.\n",
    "- Первое число означает, что мы начинаем брать элементы с первого индекса — второго столбца.\n",
    "- Второе число — что мы заканчиваем итерацию на четвёртом индексе, то есть проходим всю строку.\n",
    "- Третье число указывает, с каким шагом мы идём по строке. В нашем примере — с шагом в два элемента."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = np.array([[1, 2, 3, 4],\n",
    "              [5, 6, 7, 8]])\n",
    "e[0, 1:4:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание специальных массивов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имеется несколько функций для того, чтобы создавать массивы с каким-то исходным содержимым (по умолчанию тип создаваемого массива — float64).\n",
    "\n",
    "Функция `zeros()` создает массив из нулей, а функция `ones()` — массив из единиц. Обе функции принимают кортеж с размерами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Array filled with zeros:\\n{np.zeros((3, 5))}')\n",
    "print(f'\\nArray filled with ones:\\n{np.ones((2, 2))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для создания последовательностей чисел в NumPy имеется функция `arange()`. Она возвращает одномерный массив с равномерно разнесенными значениями внутри заданного интервала."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Array with values from 0 to 10:\\n{np.arange(11)}')\n",
    "print(f'Array with values from 3 to 10:\\n{np.arange(3, 11)}')\n",
    "print(f'Array with values from 0 to 10 with spacing given by step 3:\\n{np.arange(0, 11, 3)}')\n",
    "print(f'Array with values from 1 to 10 with spacing given by step 3:\\n{np.arange(1, 11, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Математические операции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Массивы можно складывать, умножать на число и на другой массив."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "print(f'Number n={n}')\n",
    "v1 = np.array([7, 8, 9])\n",
    "print(f'Vector v1: {v1}')\n",
    "v2 = np.array([3, 4, 5])\n",
    "print(f'Vector v2: {v2}')\n",
    "m1 = np.array([[10, 11, 12], [13, 14, 15]])\n",
    "print(f'Matrix m1:\\n{m1}')\n",
    "m2 = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(f'Matrix m2:\\n{m2}')\n",
    "\n",
    "print(f'\\nSum of vectors v1 and v2: {v1 + v2}')\n",
    "print(f'Product of vector v1 and number n: {v1 * n}')\n",
    "print(f'Product of vectors v1 and v2: {v1 * v2}')\n",
    "\n",
    "print(f'\\nSum of matrices m1 and m2:\\n{m1 + m2}')\n",
    "print(f'Product of matrix m1 and number n:\\n{m1 * n}')\n",
    "print(f'Product of matrix m1 and vector v1:\\n{m1 * v1}')\n",
    "print(f'Product of matrix m1 and matrix m2:\\n{m1 * m2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напоминание про матричное умножение:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ AB= \\begin {pmatrix} 2 & 3\\\\ 4 & -1 \\end{pmatrix} \\times \\begin{pmatrix} 1 & 0\\\\ 5 & -2 \\end{pmatrix} =\\begin {pmatrix} 2+15 & 0-6 \\\\ 4-5 & 0+2 \\end{pmatrix} = \\begin{pmatrix} 17 & -6\\\\ -1 & 2 \\end{pmatrix}$\n",
    "\n",
    "$ BA= \\begin{pmatrix} 1 & 0\\\\ 5 & -2 \\end{pmatrix} \\times \\begin {pmatrix} 2 & 3\\\\ 4 & -1 \\end{pmatrix} =\\begin {pmatrix} 2+0 & 3+0 \\\\ 10-8 & 15+2 \\end{pmatrix} = \\begin{pmatrix} 2 & 3\\\\ 2 & 17 \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скалярное произведение векторов:\n",
    "$$\\vec{a}\\cdot\\vec{b}=|\\vec{a}|\\cdot|\\vec{b}|\\cdot\\cos(\\alpha)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(v1, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[doc] 🛠️ Pandas](https://pandas.pydata.org/) — библиотека для обработки и анализа табличных данных. В этой библиотеке используется NumPy для удобного хранения данных и вычислений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В библиотеке Pandas определены два класса объектов для работы с данными:\n",
    "\n",
    "> `Series` – одномерный массив, который может хранить значения любого типа данных.\n",
    "\n",
    "> `DataFrame` – двумерный массив (таблица), в котором столбцами являются объекты класса Series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создать объект класса `Series` можно следующим образом:\n",
    "\n",
    "`s = pd.Series(data, index=index)`\n",
    "\n",
    "В качестве `data` могут выступать: массив `numpy`, словарь, число. В аргумент `index` передаётся список меток строк. Метка может быть числом или строкой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.Series(np.arange(10,35,5), index=[\"a\", \"b\", \"c\", \"d\", \"e\"])\n",
    "print(f'Series s1 from numpy array with letter indexes:\\n{s1}')\n",
    "s2 = pd.Series(np.arange(10,35,5))\n",
    "print(f'\\nSeries s2 from numpy array with number indexes:\\n{s2}')\n",
    "s3 = pd.Series({\"a\": 10, \"b\": 20, \"c\": 30, \"d\": 40})\n",
    "print(f'\\nSeries s3 from dictionary with number indexes:\\n{s3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для `Series` доступно взятие элемента по индексу, срезы, поэлементные математические операции аналогично массивам `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(np.arange(5), index=[\"a\", \"b\", \"c\", \"d\", \"e\"])\n",
    "print(f'Series s:\\n{s}')\n",
    "print(f'\\nChoice of element with index \"a\": {s[\"a\"]}')\n",
    "print(f'\\nChoice of elements with indexes \"a\" and \"d\":\\n{s[[\"a\", \"d\"]]}')\n",
    "print(f'\\nChoice of elements from 1:\\n{s[1:]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для `Series` можно применять фильтрацию данных по условию, записанному в качестве индекса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(np.arange(5), index=[\"a\", \"b\", \"c\", \"d\", \"e\"])\n",
    "s[s > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объект класса `DataFrame` работает с двумерными табличными данными. Создать `DataFrame` проще всего из словаря Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students_marks_dict = {\"student\": [\"Студент_1\", \"Студент_2\", \"Студент_3\"],\n",
    "                       \"math\": [5, 3, 4],\n",
    "                       \"physics\": [4, 5, 5]}\n",
    "students = pd.DataFrame(students_marks_dict)\n",
    "students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " У объекта класса `DataFrame` есть индексы по строкам (`index`) и столбцам(`columns`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Row indexes: {students.index}')\n",
    "print(f'Column indexes: {students.columns}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для индекса по строке по умолчанию задаётся числовое значение. Значения индекса можно заменить путем записи списка в атрибут `index`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students.index = [\"A\", \"B\", \"C\"]\n",
    "students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для доступа к записям таблицы по строковой метке используется атрибут `loc`. При использовании строковой метки доступна операция среза."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students.loc[\"B\":]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Импорт данных из файла"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычно табличные данные хранятся в файлах. Такие наборы данных принято называть датасетами. Файлы с датасетом могут иметь различный формат. Pandas поддерживает операции чтения и записи для CSV, Excel 2007+, SQL, HTML, JSON и др.\n",
    "\n",
    "Несколько примеров, как получить датасет из файлов разных форматов:\n",
    "\n",
    "> CSV. Используется функция `read_csv()`. Аргумент `file` является строкой, в которой записан путь до файла с датасетом. Для записи данных из DataFrame в CSV-файл используется метод `to_csv(file)`.\n",
    "\n",
    "> Excel. Используется функция `read_excel()`. Для записи данных из `DataFrame` в Excel-файл используется метод `to_excel()`.\n",
    "\n",
    "> JSON. Используется функция `read_json()`. Для записи данных из `DataFrame` в JSON используется метод `to_json()`.\n",
    "\n",
    "Для работы с другими форматами файлов в Pandas есть функции, работающие аналогично рассмотренным.\n",
    "\n",
    "Для дальнейшей работы загрузим файл с датасетом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/datasets/students_performance.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим датасет из CSV-файла с данными о студентах. Он относится к классу `DataFrame`. Для получения первых n строк датасета используется метод `head(n)`. По умолчанию возвращается 5 первых строк."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students = pd.read_csv(\"students_performance.csv\")\n",
    "students.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для получения последних n строк используется метод `tail(n)`. По умолчанию возвращается 5 последних строк."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно вывести конкретные строки датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students[10:13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве индекса можно использовать условия для фильтрации данных. Выберем 5 первых результатов теста по математике для студентов, прошедших подготовительный курс."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students[students[\"test preparation course\"] == \"completed\"][\"math score\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно вывести 10 последних результатов теста по чтению для студентов, у которых родители имеют степень бакалавра."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students[students[\"parental level of education\"] == \"bachelor's degree\"][\"reading score\"].tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Разведочный анализ данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Разведочный анализ данных/ Exploratory data analysis (EDA)** — анализ основных свойств данных, нахождение в них общих закономерностей, зачастую с использованием инструментов визуализации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачаем набор данных [[doc] 🛠️ SMS Spam Collection Dataset](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset). Он содержит смс-сообщения на английском языке, размеченные как «спам» (spam) и «не спам» (ham). О том, почему классы называются именно так, можно почитать в статье [[wiki] 📚 Spam (food)](https://simple.wikipedia.org/wiki/Spam_(food)#:~:text=The%20Hormel%20Foods%20Corporation%20once,%E2%80%9CSizzle%20Pork%20And%20Mmmm%E2%80%9D.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/datasets/SMS_Spam_Collection.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sms = pd.read_csv('SMS_Spam_Collection.csv', sep='\\t',\n",
    "                  # texts and labels are separated by tabs\n",
    "                  header=None, names=['label', 'text'])\n",
    "                  # give names to the columns\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим данные на наличие дублирующихся строк."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms.duplicated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалим повторяющиеся данные, сохранив первое вхождение для каждого дубля."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms.drop_duplicates(keep='first',inplace=True)\n",
    "sms.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, какое количество сообщений каждого класса представлено в датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Распределение классов можно визуализировать. Воспользуемся библиотекой [[doc] 🛠️ Matplotlib](https://matplotlib.org/) для рисования круговой диаграммы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.pie(sms['label'].value_counts(), labels=sms['label'].unique(), autopct='%.1f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим, насколько длинные сообщения встречаются в датасете. Разделим тексты по пробелам и посчитаем количество слов. Пословное деление предложений называется **токенизацией**. Токенизация может осуществляться не только на слова, но и на части слов (подслова). Об этом мы подробно поговорим в следующих лекциях.\n",
    "\n",
    "Посчитаем количество слов для всех сообщений и запишем результат в датафрейм в качестве нового столбца."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms['word_count'] = sms['text'].map(lambda x: len(x.split()))\n",
    "sms.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируем результат с помощью гистограммы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sms['word_count'])\n",
    "plt.title('Word count in messages')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно отдельно вывести минимальную, максимальную и среднюю длину сообщения, которые встретились в датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Minimum word count: {sms['word_count'].min()}\")\n",
    "print(f\"Maximum word count: {sms['word_count'].max()}\")\n",
    "print(f\"Mean word count: {sms['word_count'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем количество вхождений для каждого слова. Для этого создадим объект класса Counter(), он позволяет быстро посчитать количество появлений элементов в последовательности. Определим количество уникальных слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "word_frequency = Counter(\" \".join(sms['text']).split())\n",
    "print(word_frequency)\n",
    "print(len(word_frequency))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем в отдельную переменную топ-10 самых частотных слов и выведем результат в виде столбчатой диаграммы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10 = word_frequency.most_common(10)\n",
    "plt.barh(y=[x[0] for x in top_10], width=[x[1] for x in top_10])\n",
    "plt.title('Top 10 most frequently occuring words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно заметить, что для слов разного регистра частотность считается отдельно (i vs. I). Также в топ-10 попало много служебных слов, которые вряд ли помогут понять, является ли сообщение спамом. Следовательно, прежде чем переходить к построении модели машинного обучения, необходимо осуществить подготовку и чистку данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо удалить стоп-слова — это часто используемые слова, которые не вносят никакой дополнительной информации в текст. В билиотеке [[doc] 🛠️ NLTK](https://www.nltk.org/) есть встроенный список стоп-слов, которым мы воспользуемся."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, входят ли наши самые частотные слова в этот список."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in top_10:\n",
    "    print(f\"'{item[0]}' in stopwords:\\\n",
    "    {item[0] in stopwords.words('english')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Слово I не вошло из-за того, что написано в верхнем регистре. Еще один этап предобработки текста — приведение слов к нижнему регистру. Это можно сделать с помощью метода `lower()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in top_10:\n",
    "    print(f\"'{item[0].lower()}' in stopwords: {item[0].lower() in stopwords.words('english')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Слово u отсутствует в исходном списке, но мы можем добавить его сами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = stopwords.words('english') + ['u']\n",
    "for item in top_10:\n",
    "    print(f\"'{item[0].lower()}' in stopwords: {item[0].lower() in STOPWORDS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь все частотные слова входят в список стоп-слов.\n",
    "\n",
    "Наконец, из текста нужно удалить знаки препинания. Для этого воспользуемся `string.punctuation` — это предварительно инициализированная строка,содержащая знаки препинания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для примера к каждому токену первого текста применим метод `strip()` и удалим знаки препинания, а затем через пробел соединим токены обратно к строку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sms['text'][0])\n",
    "print(' '.join([token.strip(punctuation).lower()\n",
    "for token in sms['text'][0].split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объединим все этапы предобработки в функции `text_preprocessing`. Она принимает на вход сырой текст и список стоп-слов. Функция токенизирует текст по словам, каждый токен приводит к нижнему регистру, удаляет знаки препинания и стоп-слова. На выходе мы должны получить строку из токенов после предобработки, разделенных пробелами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text, stopwords):\n",
    "    tokens = [token.strip(punctuation).lower() for token in text.split()\n",
    "    if token.lower() not in STOPWORDS]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "print(sms['text'][0])\n",
    "print(text_preprocessing(sms['text'][0], STOPWORDS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осуществим предобработку всех текстов в датсете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms['preprocessed'] = sms['text'].apply(lambda x:\n",
    "                                        text_preprocessing(x, STOPWORDS))\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В следующем разделе обучим на наших данных модель для автоматической классификации спама."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Наивный байесовский классификатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наивный байесовский классификатор — это алгоритм классификации, основанный на теореме Байеса с допущением о независимости признаков. Другими словами, НБА предполагает, что наличие какого-либо признака в классе не связано с наличием какого-либо другого признака."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теорема Байеса позволяет рассчитать вероятность события $A$, основываясь на прошлом (априорном) знании условий $B$:\n",
    "\n",
    "$$P (A | B) = \\frac{P (B | A) × P(A)}{P(B)}$$\n",
    "\n",
    "$P(A$) – априорная (безусловная) вероятность события $A$;\n",
    "  - связана с некоторым случайным событием $A$;\n",
    "  - представляет степень уверенности в том, что данное событие произошло, в отсутствие любой другой информации, связанной с этим событием.\n",
    "\n",
    "Например, мы можем выдвинуть гипотезу H, что некоторый объект или наблюдение принадлежит классу C, безотносительно свойств этого объекта. Тогда P(H) будет априорной вероятностью данного события.\n",
    "\n",
    "$P(A | B)$ – апостериорная вероятность $A$ (вероятность события $A$\n",
    "при наступлении события $B$)\n",
    "- вероятность значения, принимаемого случайной переменной,\n",
    "-назначается после принятия во внимание некоторой новой, связанной с ней информации.\n",
    "  \n",
    "Иными словами, это вероятность события $A$ при условии, что произошло событие\n",
    "$B$.\n",
    "\n",
    "Например, при условии, что плод красный и круглый, мы с большой долей уверенности можем предположить, что это яблоко, чем в случае, если эта информация отсутствует, т.е. апостериорная вероятность данного события будет\n",
    "$P (\\text{яблоко|красный, круглый})$.\n",
    "\n",
    "$P(B)$ – априорная вероятность события $B$;\n",
    "\n",
    "$P(B | A)$ – апостериорная вероятность $B$ (вероятность наступления события $B$ при истинности события $A$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификация спама"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем использовать наивный байесовский классификатор для фильтрации спама.  В рамках данной задачи имеются:\n",
    "- Датасет из текстов сообщений с некоторым фиксированным словарём возможных слов.\n",
    "- Два класса сообщений: спам и нормальное.\n",
    "- Признаковое описание для каждого сообщения, характеризующее количество вхождений каждого из слов словаря в текст сообщения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого класса $c$ требуется найти $P(c|d)$ — вероятность класса $c$ для документа $d$. Она рассчитывается по формуле Байеса:\n",
    "\n",
    "$$P(c|d) = \\frac{P(d|c) P(c)} {P(d)}$$\n",
    "\n",
    "$P(d)$: вероятность документа $d$ одинакова для всех классов, поэтому её можно опустить.\n",
    "\n",
    "Получим:\n",
    "\n",
    "$$P(c|d) = P(d|c) P(c)$$\n",
    "\n",
    "$P(c)$: вероятность класса $c$ — это доля документов класса $c$ среди всех документов.\n",
    "\n",
    "$P(d|c)$: вероятность документа $d$ для класса $c$ зависит от слов $x_1, x_2, ..., x_n$, входящих в документ:\n",
    "\n",
    "$$P(d|c) = P(x_1,x_2,...,x_n|c) = P(x_1|c)P(x_2|c)...P(x_n|c) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть у нас есть датасет, где все письма состоят из слов $x_1, x_2, x_3, x_4$: **‘добрый’, ‘день’, ‘гости’, ‘деньги’**. Мы уже посчитали, сколько раз каждое слово встречается в каждом классе.\n",
    "\n",
    "Мы можем посчитать $P(x_1|c)$ — вероятность встретить слово **‘добрый’** в нормальном письме: берем количество нормальных писем со словом **‘добрый’** и делим на количество всех нормальных писем. Аналогично для других слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*КАРТИНКА из блокнота [Naive_bayes_NLP](https://edunet.kea.su/repo/EduNet-additions/L02/naive_bayes_1.png) с исправлениями.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Делаем то же самое для слов из спама."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*КАРТИНКА из блокнота [Naive_bayes_NLP](https://edunet.kea.su/repo/EduNet-additions/L02/naive_bayes_2.png) с исправлениями.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем $P(c)$ — вероятность того, что письмо не является спамом. Для этого количество нормальных писем делим на общее количество писем. Аналогично для спама."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*КАРТИНКА из блокнота [Naive_bayes_NLP](https://edunet.kea.su/repo/EduNet-additions/L02/naive_bayes_3.png).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем вычислить $P(d|c)$ для письма **‘добрый день’**. Для этого перемножим $P(x_1|d)$ — вероятность нормального письма со словом **‘добрый’** и $P(x_2|d)$ — вероятность нормального письма со словом **‘день’**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*КАРТИНКА с формулами:*\n",
    "\n",
    "*p(добрый день|нормальное) = p(добрый|нормальное) × p(день|нормальное)*\n",
    "\n",
    "*p(добрый день|СПАМ) = p(добрый|СПАМ) × p(день|СПАМ)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось получить $P(c|d)$ — вероятность нормального письма с фразой **‘добрый день’** в «наивном» предположении. Нужно умножить $P(d|c)$ — вероятность нормального письма **‘добрый день’** на $P(c)$  — вероятность того, что письмо не является спамом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*КАРТИНКА из блокнота [Naive_bayes_NLP](https://edunet.kea.su/repo/EduNet-additions/L02/naive_bayes_3_5.png) с исправлениями.*\n",
    "\n",
    "*Итоговый вариант*:\n",
    "\n",
    "*p(нормальное|добрый день) = p(добрый день|нормальное) × p(нормальное)*\n",
    "\n",
    "*p(СПАМ|добрый день) = p(добрый день|СПАМ) × p(СПАМ)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение и тестирование модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем работать с уже известным нам набором данных [[doc] 🛠️ SMS Spam Collection Dataset](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset). Для удобства заменим словесные обозначения классов на числовые."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms['num_label'] = sms['label'].astype('category').cat.codes\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем в отдельные переменные предобработанные тексты `X` и метки классов `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = sms['preprocessed'], sms['num_label']\n",
    "X[:5], y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим данные на обучающую и тестовую выборку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы подавать тексты в модель машинного обучения, необходимо представить каждое предложение в виде набора признаков — вектора. В качестве признаков будем подавать модели слова, встретившиеся в обучающей выборке. Те слова, которые встретятся в тестовой выборке, но отсутствуют в обучающей, не могут быть проинтерпретироованы моделью.\n",
    "\n",
    "Будем использовать модель векторизации «мешок слов». Словарь необходимо собирать на основе обучающей выборки (метод `fit`). При этом преобразование текстов в векторы на основе собранного словаря нужно осуществить для всего датасета (метод `transform`).\n",
    "\n",
    "Осуществим векторизацию обучающей и тестовой выборок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer()\n",
    "#vect.fit(X_train)\n",
    "#X_train_vect = vect.transform(X_train)\n",
    "X_train_vect = vect.fit_transform(X_train)\n",
    "X_test_vect = vect.transform(X_test)\n",
    "X_train_vect, X_test_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим модель для классификации — наивный байесовский классификатор. Осуществим обучение модели на обучающей выборке из предложений (`X_train_bow`) и меток (`y_train`) с помощью метода `fit`. Затем используем обученную модель для предсказания меток на основе предложений тестовой выборки (`X_test_bow`) с помощью метода `predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_vect, y_train)\n",
    "y_mnb = mnb.predict(X_test_vect)\n",
    "y_mnb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим качество классификации  с помощью метрики accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_mnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку классы не сбалансированы, посчитаем также точность (precision), полноту (recall) и F-меру (f1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "precision_score(y_test, y_mnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "recall_score(y_test, y_mnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test, y_mnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы получили довольно высокое качество. Но можно ли еще улучшить его? Например, за счет гиперпараметров векторизации?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подбор гиперпараметров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Для автоматического подбора параметров используется модуль `GridSearchCV`[🛠️[doc]](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). Он создает модель для каждой возможной комбинации параметров.\n",
    "\n",
    "Все этапы обработки — векторизацию и классификацию — объединим в `Pipeline`[🛠️[doc]](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html). Пайплайн в машинном обучении — это процесс работы с данными от начала до конца, включая ввод данных, их обработку, применение модели с установленными параметрами и вывод результата.\n",
    "\n",
    "Создаем словарь `parameters`, содержащий диапазон значений каждого параметра. Далее инициализируем объект `grid_search`, передавая ему пайплайн (векторизатор и модель). По умолчанию количество итераций равно 10 (`n_iter`), то есть будут сравниваться 10 разных моделей. Установим количество кросс-валидаций (`cv=5`).\n",
    "\n",
    "Кросс-валидация — перекрестная проверка.\n",
    "\n",
    "1. Фиксируется целое число $k$, меньшее числа примеров в датасете.\n",
    "2. Датасет разбивается на $k$ одинаковых частей.\n",
    "3. Происходит $k$ итераций, в каждой из которых одна часть выступает в роли тестового множества, а объединение остальных — в роли тренировочного.\n",
    "4. Финальный результат модели получается либо усреднением получившихся тестовых результатов, либо измеряется на отложенном тестовом множестве, не участвовавшем в кросс-валидации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-content/L01/out/cross_validation_on_train_data.png\" width=\"600\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим следующие параметры:\n",
    "- `ngram_range`:\n",
    "  - (1, 1) — только униграммы;\n",
    "  - (1, 2) — униграммы и биграммы.\n",
    "- `min_df`\n",
    "  -  0.0001 — исключаем токены, которые встретились в меньше чем 0,01% документов;\n",
    "  -  0.001 — исключаем токены, которые встретились в меньше чем 0,1% документов.\n",
    "- `max_df`:\n",
    "  - 0.7 — исключаем токены, которые встретились в больше чем 70% документов;\n",
    "  - 1.0 — исключаем токены, которые встретились в больше чем 100% документов.\n",
    "\n",
    "В качестве метрики для сравнения (`scoring`) будем использовать F1-меру. Выведем лучшие значения параметров (`best_params_`) и лучшее значение метрики (`best_score_`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipeline = Pipeline([\n",
    "           ('vect', CountVectorizer()),\n",
    "           ('clf', MultinomialNB()),\n",
    "])\n",
    "parameters = {\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "    'vect__min_df': (0.0001, 0.001),\n",
    "    'vect__max_df': (0.7, 1.0)\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, cv=5, scoring='f1')\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_, grid_search.best_score_.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_['vect__ngram_range']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "           ('vect',\n",
    "            CountVectorizer(max_df=grid_search.best_params_['vect__max_df'],\n",
    "                            min_df=grid_search.best_params_['vect__min_df'],\n",
    "                            ngram_range=grid_search.best_params_['vect__ngram_range'])),\n",
    "           ('clf', MultinomialNB()),\n",
    "])\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "f1_score(y_test, y_pred).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "За счет автоматического подбора гиперпараметров качество слегка возросло."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Общее назначение регрессии состоит в анализе связи между несколькими независимыми переменными и зависимой переменной. Зависимая переменная $z$ является взвешенной суммой независимых переменных $x_1,x_2,...,x_n$ (признаков). Веса $w_1, w_2, ..., w_n$ подбираются на обучающих данных — в этом и состоит обучение модели.\n",
    "\n",
    "$$z(x) = w_1x_1 + w_2x_2 + ... + w_nx_n$$\n",
    "\n",
    "Переменная $z$ может принимать значения в любом диапазоне. Чтобы получить вероятность отнесения объекта к классу, нужно привести его в диапазон от 0 до 1 с помощью функции активации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция активации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае бинарной классификации используется сигмоида. Если получившееся значение больше 0.5, то объект относится к положительному классу, иначе — к отрицательному.\n",
    "\n",
    "$$ σ(z(x))=\\frac{1}{1+e^{-z(x)}}$$\n",
    "\n",
    "Запишем формулу сигмоиды, используя методы из библиотеки NumPy:\n",
    "\n",
    "- `np.exp(x)`  для подсчета $e^x$\n",
    "- `np.arange()` для указания диапазона возможных значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "x = np.arange(-10, 10, 0.5)\n",
    "z = 1/(1 + np.exp(-x))\n",
    "\n",
    "plt.plot(x, z)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Sigmoid(X)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для многоклассовой классификации используется функция активации softmax.  Вероятность $i$-го класса при наличии $K$ классов рассчитывается следующим образом:\n",
    "\n",
    "$$\\text{softmax}(z_{i}) = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}$$\n",
    "$$i=1,...,K$$\n",
    "\n",
    "Для каждого объекта выбирается класс с наибольшей вероятностью.\n",
    "\n",
    "Запишем формулу для функции софтмакс. Для подсчета суммы используем метод `np.sum()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "x = np.array([2.6, 3.2, 0.5])\n",
    "print(softmax(x))\n",
    "print(np.sum(softmax(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо определить оптимальные параметры $w_1,...,w_n$, при которых различие между предсказанными и истинными значениями будет минимально. Рассчитать величину ошибки позволяет **функция потерь**, которую необходимо минимизировать.\n",
    "\n",
    "Используется функция кросс-энтропии ($L_{CE}$ — *Cross Entropy Loss*):\n",
    "\n",
    "$$L_{CE}(\\hat y,y) = -\\sum^{K}_{i=1}y_i \\cdot log(\\hat y_i),$$\n",
    "\n",
    "где $i$ — номер класса, $y$ — истинный ответ, $\\hat y$ — предсказанный ответ.\n",
    "\n",
    "В случае бинарной классификации имеет $K=2$, $y=0$ или $y=1$ ($L_{BCE}$ — *Binary Cross Entropy Loss*):\n",
    "\n",
    "$$L_{BCE}(\\hat y,y) = -(y \\cdot log(\\hat y)+(1-y) \\cdot log(1-\\hat y))$$\n",
    "\n",
    "В случае многоклассовой классификации $K>2$. Истинный ответ $y$ — вектор длины $K$, где элемент вектора $y_c=1$, если $c$ — истинный класс, остальные элементы равны $0$.\n",
    "\n",
    "Например, $K=3$ (классы $0,1,2$). Объект относится к классу $2$. Тогда $y = (0, 0, 1)$, $c=2$, $y_2 = 1$.\n",
    "\n",
    "Модель предсказывает вектор $\\hat y$ длины $K$. Функция кросс-энтропии имеет вид:\n",
    "\n",
    "$$L_{CE}(\\hat y,y) = -\\log \\hat y_c$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод градиентного спуска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача поиска оптимальных параметров модели сводится к задаче **поиска минимума функции потерь**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Точки минимума и максимума функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как найти минимум функции в простом случае?\n",
    "\n",
    "- Найти производную функции.\n",
    "- Найти значения, при которых производная равна нулю.\n",
    "- Определить знаки производной. Когда функция возрастает, то производная положительна. Когда функция убывает, то производная отрицательна.\n",
    "- Определить точки минимума и максимума. Если функция возрастала и в определенной точке начала убывать, то это точка максимума. Если функция убывала и в некоторой точке начала возрастать, то это точка минимума."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L01/function.png\" width=\"700\" ></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Таблица производных:}$$\n",
    "\\begin{array}{|с|c|c|} \\hline\n",
    " f(x) \\text{ (функция)} & f'(x)  \\text{ (производная)}\\\\ \\hline\n",
    "с \\text{ (константа)} & 0 \\\\ \\hline\n",
    "cx & c \\\\ \\hline\n",
    "x^c & cx^{c-1} \\\\ \\hline\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим на примере функции $f(x)=x^2 + x + 3$.\n",
    "\n",
    "Она зависит от одной переменной $x$.\n",
    "\n",
    "Найдем производную $f'(x^2 - x + 3)$.\n",
    "- $f'(x^2 - x + 3) = 2x - 1$\n",
    "- $f'(x^2 - x + 3) = 0$ при $x =0.5$\n",
    "-  $f'(x^2 - x + 3) < 0 $ при $x < 0.5$, $f'(x^2 - x + 3) > 0 $ при $x > 0.5$\n",
    "- $f(x)$ убывает при $x < 0.5$, $f(x)$ возрастает при $x > 0.5$\n",
    "- $x = 0.5$ — точка минимума"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.linspace(-2, 3, 100)\n",
    "y = x**2 - x + 3\n",
    "plt.figure()\n",
    "plt.title(\"$x^2 - x + 3$\")\n",
    "plt.plot(x, y)\n",
    "plt.xticks(np.arange(-2, 3.5, step=0.5))\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.plot(0.5,2.75, marker='.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Частная производная и градиент функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":Если функция зависит от нескольких переменных, можно говорить о частной производной, когда все остальные переменные, кроме интересующей нас, становятся константами. Производная функции $f$ от переменной $x_1$: $\\large\\frac{\\partial f}{\\partial x_1}$.\n",
    "\n",
    "**Градиент** в математическом анализе — это вектор, указывающий на направление максимального роста функции в заданной точке. Вектор-градиент состоит из частных производных функции от каждой из ее переменных $(x_{1},...,x_{n})$:\n",
    "\n",
    "$$ \\nabla f(\\vec{x}) = \\begin{bmatrix}\n",
    "\\displaystyle\\frac{\\partial f}{\\partial x_1}\\\\\n",
    "\\displaystyle\\frac{\\partial f}{\\partial x_2}\\\\\n",
    "...\\\\\n",
    "\\displaystyle\\frac{\\partial f}{\\partial x_n}\\\\\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим функцию $f(x,y,z)=2xy^3+3z^2$. Найдем градиент функции для переменных $x,y,z$.\n",
    "\n",
    "$$\n",
    "\\nabla f(x, y, z)=\\begin{bmatrix}\n",
    "\\displaystyle\\frac{\\partial f}{\\partial x}\\\\\n",
    "\\displaystyle\\frac{\\partial f}{\\partial y}\\\\\n",
    "\\displaystyle\\frac{\\partial f}{\\partial z}\\\\\n",
    "\\end{bmatrix}\n",
    "=\\begin{bmatrix}\n",
    "2y^3\\\\\n",
    "\\\\\n",
    "6xy^2\\\\\n",
    "\\\\\n",
    "6z\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для минимизации ошибки нужен антиградиент, который показывает направление скорейшего убывания функции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Градиентный спуск и скорость обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Градиентный спуск** — это некоторый алгоритм, который помогает нам итеративно шагами перемещаться по функции и в конце концов оказаться в ее минимуме. Мы не напрямую ищем точку, где производная равна нулю, а некоторыми манипуляциями передвигаемся вниз по функции. Градиентный спуск выбирает случайную точку, находит направление самого быстрого убывания функции и двигается до ближайшего минимума вдоль этого направления.\n",
    "\n",
    "Важно настроить размер шага или **скорость обучения** $η$ — некоторый коэффициент, как правило, небольшой, который не позволяет нам двигаться слишком быстро. У нас есть точка, в которую мы хотим попасть. Если мы сделаем слишком большой шаг, то мы ее перескочим (график справа), поэтому надо подобрать шаг, который не позволит ее перескочить, но в то же время такой, чтобы тот же процесс не шел слишком медленно (как на графике слева)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet_NLP-content/L01/out/learning_rate_optimal_value.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если на каком-то этапе разность между старой точкой (до шага) и новой снижается ниже предела, считается, что минимум найден, алгоритм завершен."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомним, что функция потерь $\\mathcal{L}(y, \\hat{y})$ зависит от нескольких переменных: весов $\\overline{w} = (w_1, \\cdots, w_m)$ и сдвига $b$.\n",
    "\n",
    "Следовательно, мы будем использовать несколько частных производных:\n",
    "\n",
    "- частные производные функции потерь от весов $w_1,\\cdots,w_m$: $\\large\\frac{\\partial \\mathcal{L}(y, \\hat{y})}{\\partial w_1}, \\cdots, \\frac{\\partial \\mathcal{L}(y, \\hat{y})}{\\partial w_m}$\n",
    "- частная производная функции потерь от сдвига $b$: $\\large\\frac{\\partial \\mathcal{L}(y, \\hat{y})}{\\partial b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После подсчета частных производных нам необходимо обновить значения весов и сдвига.\n",
    "\n",
    "$$w_i = w_i-η\\frac{\\partial \\mathcal{L}(y, \\hat{y})}{\\partial w_i}$$\n",
    "\n",
    "$$b = b-η\\frac{\\partial \\mathcal{L}(y, \\hat{y})}{\\partial b}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Распознавание эмоций"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим задачу многоклассовой классификации на примере распознавания эмоций.\n",
    "\n",
    "Распознавание эмоций ≠ анализ тональности. Анализ тональности — выявление оценки авторов по отношению к объектам, речь о которых идёт в тексте. Она может быть позитивной, негативной или нейтральной. Эмоции могут включать не только радость или грусть, но ужас, гнев, удивление, отвращение и т.п."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L01/sentiment_emotion.png\" width=\"700\" ></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чаще всего при обработке текстов стоит задача распознать 6 базовых эмоций: счастье (happiness), печаль (sadness), страх (fear), гнев (anger), отвращение (disgust), удивление (surprise). Данная классификация введена Полом Экманом в книге [[book] 📚 Basic emotions](https://www.paulekman.com/wp-content/uploads/2013/07/Basic-Emotions.pdf). Утверждается, что люди всех культур испытывают их и могут распознавать в других людях. Для каждой базовой эмоции есть соответствующее, безошибочно опознаваемое выражение лица."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L01/basic_emotions.png\" width=\"1100\" ></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка и подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать русскоязычный набор данных, представленный в работе [[paper] 🎓 Emotion classification in Russian: feature engineering and analysis](https://link.springer.com/chapter/10.1007/978-3-030-72610-2_10).\n",
    "\n",
    "Он размечен по **5 классам эмоций**:\n",
    "- радость (joy),\n",
    "- печаль (sadness),\n",
    "- злость (anger),\n",
    "- неуверенность (uncertainty),\n",
    "- нейтральность (neutrality).\n",
    "\n",
    "Из классификации Экмана удалена категория отвращения из-за отсутствия соответствующих данных. Категории страха и удивления  объединены в одну категорию неопределенности из-за сходства способов, которыми они выражаются.\n",
    "\n",
    "Загрузим набор данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/datasets/Emotion_Classification_Russian.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "emo = pd.read_csv('Emotion_Classification_Russian.csv')\n",
    "emo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо метки класса (label) и текста (text), присутствует столбец с лемматизированными предложениями (lemmatized). Лемматизация — процесс приведения словоформы к лемме, то есть её нормальной (словарной) форме.\n",
    "\n",
    "В русском языке это следующие морфологические формы:\n",
    "\n",
    "- для существительных — именительный падеж, единственное число\n",
    "  - кошками → кошка;\n",
    "- для прилагательных — именительный падеж, единственное число, мужской род\n",
    "  - красивых → красивый;\n",
    "- для глаголов, причастий, деепричастий — глагол в инфинитиве (неопределённой форме) несовершенного вида\n",
    "  - бежал → бежать.\n",
    "\n",
    "Лемматизация необходима для языков с богатой морфологией, чтобы не расширять размер словаря за счет слов, у которых одинаковый смысл, но разные морфологические формы.\n",
    "\n",
    "В рассматриваемом наборе данных лемматизация проведена с помощью морфологического анализатора [[git] 🐾 RNNmorph](https://github.com/IlyaGusev/rnnmorph). Он использует реккурентную нейронную сеть для определения части речи и морфологических признаков слова с учетом контекста.\n",
    "\n",
    "Рассмотрим пример его применения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/IlyaGusev/rnnmorph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnnmorph.predictor import RNNMorphPredictor\n",
    "predictor = RNNMorphPredictor(language=\"ru\")\n",
    "\n",
    "forms = predictor.predict([\"мама\", \"поймала\", \"мышь\"])\n",
    "print(f'Часть речи слова \"мама\": {forms[0].pos}')\n",
    "print(f'Лемма слова \"поймала\": {forms[1].normal_form}')\n",
    "print(f'Морфологический анализ слова \"мышь\": {forms[2].tag}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для удобства заменим словесные обозначения классов в датасете на числовые. Метки присваиваются в алфавитном порядке:\n",
    "- **a**nger → 0\n",
    "- **j**oy → 1\n",
    "- **n**eutrality →  2\n",
    "- **s**adness → 3\n",
    "- **u**ncertainty → 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo['num_label'] = emo['label'].astype('category').cat.codes\n",
    "emo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Узнаем количество данных в датасете и их распределение по классам эмоций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.pie(emo['label'].value_counts(), labels=emo['label'].unique(), autopct='%.1f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведем уже знакомые операции для подготовки данных:\n",
    "- запишем в отдельные переменные лемматизированные тексты `X` и метки классов `y`;\n",
    "- разделим данные на обучающую и тестовую выборку;\n",
    "- осуществим векторизацию обучающей и тестовой выборок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X, y = emo['lemmatized'], emo['num_label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "vect = TfidfVectorizer(min_df=0.0002, max_df=0.2)\n",
    "X_train_vect = vect.fit_transform(X_train)\n",
    "X_test_vect = vect.transform(X_test)\n",
    "X_train_vect, X_test_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение и анализ модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель логистической регрессии (при дефолтном параметре `max_iter=100` модель не сходится; чтобы добиться сходимости алгоритма, установим большее количество итераций)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(random_state=42, max_iter=300) # model initialization\n",
    "logreg.fit(X_train_vect, y_train) # trainig\n",
    "y_logreg = logreg.predict(X_test_vect) # predicting labels\n",
    "y_logreg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты классификации можно отразить в виде матрицы ошибок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def show_confusion_matrix(confusion_matrix):\n",
    "  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Greens\")\n",
    "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
    "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
    "  plt.ylabel('True emotion')\n",
    "  plt.xlabel('Predicted emotion')\n",
    "\n",
    "class_names = ['anger', 'joy', 'neutrality', 'sadness', 'uncertainty']\n",
    "cm = confusion_matrix(y_test, y_logreg)\n",
    "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "show_confusion_matrix(df_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метрики для каждого класса, а также усредненные метрики представлены в отчете о классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['anger', 'joy', 'neutrality', 'sadness', 'uncertainty']\n",
    "print(classification_report(y_test, y_logreg, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого класса были посчитаны свои коэффициенты регрессии. Они представляют матрицу $K \\times n$, где $K$ — количество классов, $n$ — количество признаков (слов)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficient_matrix = logreg.coef_\n",
    "print(coefficient_matrix)\n",
    "print(coefficient_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Каждый из коэффициентов регрессии описывает размер вклада соответствующего признака.\n",
    " - Положительный коэффициент — признак повышает вероятность принадлежности к классу, отрицательный коэффициент — признак уменьшает вероятность.\n",
    " - Большой коэффициент — признак существенно влияет на вероятность принадлежности к классу, почти нулевой коэффициент — признак имеет небольшое влияние на вероятность результата.\n",
    "\n",
    "Выведем признаки с самыми большими коэффициентами для каждого класса и получим наиболее характерные слова для каждой эмоции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Злость', 'Радость', 'Нейтральность', 'Печаль', 'Неуверенность']\n",
    "coefficient_matrix = logreg.coef_\n",
    "\n",
    "for i in range(coefficient_matrix.shape[0]):\n",
    "\n",
    "    print(f\"\\n{labels[i]}:\")\n",
    "\n",
    "    feature_names = vect.get_feature_names_out() # word features\n",
    "    order = coefficient_matrix[i].argsort() # ascending order for coefficients\n",
    "    class_coefficients = coefficient_matrix[i][order][::-1][:5] # sort and extract top-5 coefficients\n",
    "    feature_names = feature_names[order][::-1][:5] # words with the highest coefficients\n",
    "\n",
    "    for feature, coefficient in zip(feature_names, class_coefficients):\n",
    "      print(feature, coefficient.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Литература</font>\n",
    "\n",
    "<font size=\"5\">Инструменты:</font>\n",
    "\n",
    "- [[doc] 🛠️ Scikit-learn](https://scikit-learn.org/stable/) — ML алгоритмы\n",
    "- [[doc] 🛠️ NumPy](https://numpy.org/) — массивы и математические функции\n",
    "- [[doc] 🛠️ Pandas](https://pandas.pydata.org/) — табличные данные\n",
    "- [[doc] 🛠️ Matplotlib](https://matplotlib.org/) — визуализация\n",
    "- [[doc] 🛠️ NLTK](https://www.nltk.org/) — токенизация, словари стоп-слов\n",
    "- [[git] 🐾 RNNmorph](https://github.com/IlyaGusev/rnnmorph) — морфологический парсер для русского и английского языков\n",
    "- [[doc] 🛠️ UDPipe](https://ufal.mff.cuni.cz/udpipe) — морфологический и синтаксический парсер для различных языков\n",
    "\n",
    "<font size=\"5\">Методы и алгоритмы:</font>\n",
    "\n",
    "- [[wiki] 📚 Мешок слов](https://ru.wikipedia.org/wiki/Мешок_слов)\n",
    "- [[wiki] 📚 TF-IDF](https://ru.wikipedia.org/wiki/TF-IDF)\n",
    "- [[wiki] 📚 Наивный байесовский классификатор](https://ru.wikipedia.org/wiki/Наивный_байесовский_классификатор)\n",
    "- [[wiki] 📚 Логистическая регрессия](https://ru.wikipedia.org/wiki/Логистическая_регрессия)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
