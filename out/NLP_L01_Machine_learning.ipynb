{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Машинное обучение</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Два пути"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Допустим, у нас есть задача автоматически опеределить тональность (эмоциональную окраску) отзыва на некоторый товар или услугу: является он позитивным, негативным или нейтральным. К её решению можно подойти двумя способами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*КАРТИНКА про анализ тональности в целом. Примеры ниже. Можно перерисовать и поменять текст на русский.*\n",
    "\n",
    "[Пример 1](https://h2oai.github.io/tutorials/natural-language-processing-sentiment-analysis/img/ecc3ddedae3c6e65.jpg), [пример 2](https://media.licdn.com/dms/image/D4D12AQFkW4jtocfa-w/article-cover_image-shrink_600_2000/0/1692806351486?e=2147483647&v=beta&t=mkbv1V041iw0_0FyLEq6iG0TQToYc5CC94_4RID4JBk), [пример 3](https://monkeylearn.com/static/348bb1d70089176ca2f61ea402094382/50bf7/main.png), [пример 4](https://media.licdn.com/dms/image/D4D12AQHcGWp7wMTjHg/article-cover_image-shrink_600_2000/0/1684090082406?e=2147483647&v=beta&t=fNqNTDZjltS9T3RE960WrSew0Tarf8cObBxC8acqVRA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вариант №1: подход на основе правил (rule-based)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать **вручную** заданные **правила** классификации и эмоционально размеченные **словари**. Эти правила рассчитывают класс текста на основе эмоциональных ключевых слов и их совместного использования с другими ключевыми словами.\n",
    "\n",
    "Несмотря на высокую эффективность в текстах какой-то определенной тематики, методы на основе правил плохо способны обобщать. Чем больше примеров мы будем анализировать, тем больше исключений будет появляться.\n",
    "\n",
    "Следовательно, потребуется добавлять новые правила и увеличивать размер словаря. Алгоритмическая сложность программы будет расти, ее будет сложнее поддерживать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*КАРТИНКА: слева тексты с выделенными цветом словами (как на картинке выше), справа список слов с разбивкой по классам (aka словарь) и правила в виде дерева решений (референс — [картинка в L01](https://camo.githubusercontent.com/39892f58707bbec36f2c70c0fea7dc6b34314b13caa7ab4168b102015a6210de/68747470733a2f2f6564756e65742e6b65612e73752f7265706f2f4564754e65742d636f6e74656e742f6465762d322e312f4c30312f6f75742f616363656c65726f6d6574725f736f6c7574696f6e5f7374616e646172742e706e67)).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вариант №2: машинное обучение (machine learning)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С появлением **машинного обучения** мы можем применить принципиально другой подход. Он заключается в том, чтобы обучить модель, которая сама будет извлекать шаблоны из данных.\n",
    "\n",
    "Зачастую пользователь не только пишет текст отзыва, но и ставит оценку от 1 до 10. Мы можем использовать ее в качестве разметки для данных: отзывы с оценкой от 1 до 3 будем считать негативными, от 4 до 6 — нейтральными, от 7 до 10 — поозитивными.\n",
    "\n",
    "Соберем некоторое количество отзывов и оценок за определенное время. Загрузим данные в модель, и она обучится на этих данных. При достаточном количестве данных и адекватно подобранной модели мы сможем научить ее решать конкретные задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*КАРТИНКА: слева тексты с выделенными цветом словами как в картинке выше, далее стрелочка, модель, снова стрелочка и справа смайлики в качестве итоговых классов (референс — [картинка в L01](https://camo.githubusercontent.com/0b9b0d7279bcb10dea0d004c7c51b6a7d2b720218f8e06453eabae754e4397ed/68747470733a2f2f6564756e65742e6b65612e73752f7265706f2f4564754e65742d636f6e74656e742f6465762d322e312f4c30312f6f75742f616363656c65726f6d657465725f736f6c7574696f6e5f6e6e2e706e67)).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По сути, модели всё равно, что сделать: проанализировать тональность отзыва, отфильтровать спам-письма, распределить новости по тематикам, определить язык текста и т.д. Нет необходимости писать под каждый пример отдельную программу: достаточно собрать данные, и мы сможем решить множество абсолютно разных задач.\n",
    "\n",
    "Важно лишь понимать, какую модель предпочтительнее выбрать. С этим мы будем разбираться в ходе курса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача курса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  AI, ML, ANN, DL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Место глубокого обучения и нейронных сетей в ИИ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*КАРТИНКА: самое большое множество — AI, внутри него подмножество ML, далее ANN, внутри него DL. Референс — [картинка из L01](https://camo.githubusercontent.com/607742ab4da78e76c867de94ff578b84a5fe87a17e6b1953feee4b82438ce056/68747470733a2f2f6564756e65742e6b65612e73752f7265706f2f4564754e65742d636f6e74656e742f6465762d322e312f4c30312f6f75742f61695f6d6c5f646c2e706e67), но с добавлением ANN. Примеры ниже.*\n",
    "\n",
    "[Пример 1](https://res.cloudinary.com/talend/image/upload/w_1600/q_auto/qlik/glossary/augmented-analytics/seo-hero-machine-learning-vs-ai_kls4c0.png), [пример 2](https://www.bbntimes.com/images/jch-optimize/ng/images_AI__Complete__Graph.webp), [пример 3](https://droider.ru/wp-content/uploads/2022/02/image12-1.png), [пример 4](https://droider.ru/wp-content/uploads/2022/02/image7-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Искусственный интеллект (ИИ)/ Artificial Intelligence (AI)**  — область компьютерных наук, связанная с моделированием интеллектуальных или творческих видов человеческой деятельности.\n",
    "\n",
    "**Машинное обучение/ Machine learning (ML)** — подраздел ИИ, связанный с разработкой алгоритмов и статистических моделей, которые компьютерные системы используют для выполнения задач без явных инструкций.\n",
    "\n",
    "**Искусственная нейронная сеть (ИНС)/  Artificial neural network (ANN)** — разновидность алгоритмов машинного обучения, математическая модель, построенная по принципу организации и функционирования биологических нейронных сетей. ИНС состоит из слоев «нейронов», которые связаны между собой. В простом случае это входной слой и выходной слой.\n",
    "\n",
    "**Глубокое обучение/ Deep Learning (DL)** — обучение «глубоких» ИНС. Помимо входного и выходного слоя, они состоят из сотен дополнительных «скрытых» слоев между видимыми слоями для ввода и вывода."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует множество определений сильного и слабого ИИ, рассуждений о появлении искусственного сознания и восстании машин.\n",
    "\n",
    "Всё намного **приземлённее**. Есть набор **объектов $X$** (для чего надо сделать предсказание) и набор **ответов $Y$** (что надо предсказать). Пары \"объект-ответ\" составляют **обучающую выборку**.\n",
    "\n",
    "Мы будем заниматься **восстановлением решающей функции $F$**, которая переводит признаки $X$, описывающие объекты, в ответы $Y$.\n",
    "\n",
    "$$ F: X \\xrightarrow\\ Y $$\n",
    "\n",
    "По ходу курса мы будем уточнять постановку задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Области применения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В последнее время именно такого рода модели показывают высокую эффективность в тех областях, с которыми ранее могли справиться только люди.\n",
    "Алгоритмы машинного обучения могут обрабатывать данные различного типа:\n",
    "- Компьютерное зрение / Computer vision (CV) → изображения и видео\n",
    "- Обработка естественного языка / Natural language processing (NLP) → тексты\n",
    "- Распознавание и синтез речи / Automatic Speech Recognition (ASR) & Text to speech (TTS) → аудио"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*КАРТИНКА: множества CV, NLP ASR & TTS пересекают множества ML, ANN и DL. Референс — [картинка из L01](https://camo.githubusercontent.com/8ad0efe7f7656c4f1f330e24f0cc83e89481c18b3b062eaf926a5f83e172e79a/68747470733a2f2f6564756e65742e6b65612e73752f7265706f2f4564754e65742d636f6e74656e742f6465762d322e312f4c30312f6f75742f61695f6d6c5f646c5f63765f6e6c705f73722e706e67), но с добавлением ANN и абрревиатурой для обработки речи.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обзор курса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<big>Лекция 1 Машинное обучение</big>\n",
    "\n",
    "**Зачем:**\n",
    "\n",
    "* Разобраться, как в целом подходить к задачам машинного обучения.\n",
    "\n",
    "**Что будет:**\n",
    "\n",
    "* Основные понятия, обучение с учителем и без учителя;\n",
    "* Инструменты (numpy, pandas, seaborn);\n",
    "* Разведывательный анализ, работа с данными;\n",
    "* Базовые метрики;\n",
    "* Методы векторизации: мешок слов, TF-IDF;\n",
    "* Методы машинного обучения: наивный байесовский классификатор, логистическая регрессия, деревья решений, случайный лес;\n",
    "* Построение классификатора и оценка качества.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L01/l01_meme.png\" width=\"400\" ></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<big>Лекция 2 Нейронные сети</big>\n",
    "\n",
    "**Зачем:**\n",
    "\n",
    "* Познакомиться с нейронными сетями — классом моделей машинного обучения, которые позволяют решать разнообразные задачи, начать их создавать и обучать.\n",
    "\n",
    "**Что будет:**\n",
    "\n",
    "* Интуиция, почему нейросети — очень выразительный класс моделей машинного обучения;\n",
    "* Основные «строительные блоки» нейросетей;\n",
    "* Основной метод обучения нейросетей — метод обратного распространения ошибки;\n",
    "* Знакомство с **PyTorch** — основной программной библиотекой глубокого обучения, которой будем пользоваться на курсе;\n",
    "* Построение векторных представлений слов на основе нейросетей;\n",
    "* Процесс создания и обучения нейронной сети для задачи классификации по тональности отзывов на фильмы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*КАРТИНКА [из L05](https://edunet.kea.su/repo/EduNet-content/dev-2.1/L01/out/nn_fully_connected.png)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Для остальных лекций пока нет расширенных таблиц с планом, и содержание может меняться, поэтому не стала вписывать их.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задачи машинного обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*КАРТИНКА: типы задач машинного обучения. С учителем (классификация, регрессия) и без учителя (кластеризация).*\n",
    "[Пример](https://exponenta.ru/storage/app/media/uploaded-files/mach-learn.png)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение с учителем (supervised learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый элемент выборки представляет собой пару «объект, ответ». Требуется найти функциональную зависимость ответов от описаний объектов и построить алгоритм, принимающий на входе описание объекта и выдающий на выходе ответ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Классификация (classification)** — отнесение образца к одному из нескольких попарно не пересекающихся множеств. Множество допустимых ответов конечно. Их называют метками классов (class label)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*КАРТИНКА [из L01](https://edunet.kea.su/repo/EduNet-content/dev-2.1/L01/out/classification.png).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Примеры задач классификации:* определение наличия заболевания у пациента, оценивание кредитоспособности заемщика, предсказание оттока клиентов.\n",
    "\n",
    "*Примеры задач классификации из NLP:* фильтрация спама, анализ тональности, классификация по тематике, определение языка."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Регрессия (regression)** — соотнесение объекта с некоторым числом или числовым вектором. Отсутствуют жесткие ограничения на пространство ответов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*КАРТИНКА [из L01](https://edunet.kea.su/repo/EduNet-content/dev-2.1/L01/out/regression.png).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Примеры задач регрессии:* оценка времени выздоровления, определение кредитного лимита, ожидаемый доход магазина на следующий месяц.\n",
    "\n",
    "*Примеры задач регрессии из NLP:* предсказание стоимости товара по текстовому описанию"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение без учителя (unsupervised learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом случае ответы не задаются, и требуется искать зависимости между объектами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Кластеризация (clustering)** — разбиение множества входных данных на группы с учетом попарного сходства объектов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "КАРТИНКА [из L01](https://edunet.kea.su/repo/EduNet-content/dev-2.1/L01/out/clustering.png)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Примеры задач кластеризации:* выделение типичных групп покупателей,  разделение клиентов по уровню платёжеспособности, отнесение космических объектов к категории (планета, звёзда, чёрная дыра и т. п.).\n",
    "\n",
    "*Пример задачи кластеризации из NLP:* распределение новостей по тематическим кластерам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы подробно рассмотрим алгоритмы **обучения с учителем** для задачи **классификации**. Как мы помним, в этом случае у нас есть множество объектов $X$ и множество ответов $y$. Также существует функция, которая каждому объекту сопоставляет ответ. Она называется **алгоритмом** или **моделью**.\n",
    "\n",
    "Имеющиеся данные разделяются на **обучающую** и **тестовую** выборку.\n",
    "- Обучающая выборка — это примеры, на основе которых алгоритм ищет зависимость ответов от описаний объектов и строит общую закономерность.\n",
    "- Тестовая выборка используется для оценки качества алгоритма на новых данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*КАРТИНКА: пайплайн классификации, включая пары из объектов и ответов, разделение данных на обучающую и тестовую выборки. [Пример](https://res.cloudinary.com/practicaldev/image/fetch/s--tBiGjeL2--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/cww5b22ktelnzr75cflg.jpg).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка качества"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как оценивать качество алгоритма? Допустим, вы хотите внести изменения в алгоритм. Откуда вы знаете что эти изменения сделают алгоритм лучше?\n",
    "\n",
    "В задачах машинного обучения для оценки качества моделей и сравнения  алгоритмов используются различные метрики. Мы рассмотрим некоторые из них вначале на примере бинарной классификации, а затем узнаем способы их усреднения для задач с несколькими классами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Матрица ошибок"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для подсчета метрик качества классификации используется матрица ошибок.\n",
    "\n",
    "Есть алгоритм, предсказывающий принадлежность каждого объекта одному из классов.\n",
    "- $\\hat y$ — предсказанный алгоритмом класс объекта\n",
    "- $y$ — истинный класс объекта\n",
    "\n",
    "Два класса делятся на положительный (1) и отрицательный (0 или –1).\n",
    "- Объекты, которые алгоритм относит к положительному классу, – положительные (Positive).\n",
    "- Те, которые на самом деле принадлежат к этому классу, – истинно положительные (True Positive).\n",
    "- Остальные – ложно положительные (False Positive).\n",
    "\n",
    "Аналогичная терминология для отрицательного (Negative) класса.\n",
    "\n",
    "Таким образом, ошибки классификации бывают двух видов: False Negative (FN) и False Positive (FP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\\begin{array}{|c|c|} \\hline\n",
    "& y=1 & y=0 \\\\ \\hline\n",
    "\\hat{y}=1 & \\text{True Positive (TP)} & \\text{False Positive (FP)}  \\\\ \\hline\n",
    "\\hat{y}=0 & \\text{False Negative (FN)} & \\text{True Negative (TN)} \\\\ \\hline\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример задачи: определение беременности.\n",
    "- Пациент беремен → положительный класс (positive).\n",
    "- Пациент не беремен → отрицательный класс (negative).\n",
    "\n",
    "Модель может «предсказать» наличие беременности (true) или нет (false).\n",
    "\n",
    "Пусть какой-то набор медицинских данных характерен для наличия беременности.\n",
    "- Модель верно определила и поставила положительный класс → истинно положительный исход (true positive).\n",
    "- Модель ставит отрицательную метку класса → ложно отрицательный исход (false negative).\n",
    "\n",
    "В случае отсутствия беременности для рассматриваемого набора данных исходы модели остаются аналогичными.\n",
    "- Модель относит запись к положительному классу → ложно положительный исход (false positive): модель «сказала», что пациент беремен, но на самом деле нет.\n",
    "- Модель определят запись как отрицательный класс → истинно отрицательный исход (true negative)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*КАРТИНКА [из L01](https://edunet.kea.su/repo/EduNet-content/dev-2.1/L01/out/1_2_errors.png)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy — доля правильных ответов алгоритма среди всех ответов (непоказательна в задачах с неравными классами):\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Допустим, мы хотим оценить работу спам-фильтра почты.\n",
    "\n",
    "📌 У нас есть 100 не-спам писем (отрицательный класс). 90 из них классификатор определил верно, остальные 10 – неверно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN = 90 # Место для вашего ответа\n",
    "FP = 10 # Место для вашего ответа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📌 Также есть 10 спам-писем (положительный класс). 6 из них классификатор определил верно, остальные 4 – неверно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = 6 # Место для вашего ответа\n",
    "FN = 4 # Место для вашего ответа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📌 Посчитаем accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (TP + TN) / (TP + TN + FP + FN) # Место для вашего кода\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📌 Что будет, если мы просто будем предсказывать все письма как не-спам?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN = 100 # Место для вашего ответа\n",
    "FP = 0 # Место для вашего ответа\n",
    "TP = 0 # Место для вашего ответа\n",
    "FN = 10 # Место для вашего ответа\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN) # Место для вашего кода\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При этом, наша модель совершенно не обладает никакой предсказательной силой, так как изначально мы хотели определять письма со спамом. Преодолеть это нам поможет переход с общей для всех классов метрики к отдельным показателям качества классов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае дисбаланса классов есть специальный аналог – метрика balanced accuracy.\n",
    "\n",
    "$$\\text{Balanced Accuracy} = \\dfrac{1}{2} (\\dfrac{TP}{TP + FN} + \\dfrac{TN}{TN + FP})$$\n",
    "\n",
    "📌 Посчитайте значение сбалансированной accuracy для модели, которая определяет все письма как не-спам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy = 0.5 * ((TP / (TP + FN)) + (TN / (TN + FP))) # Место для вашего кода\n",
    "balanced_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Точность, полнота, F-мера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Точность — доля объектов, названных классификатором положительными и при этом действительно являющимися положительными.\n",
    "\n",
    "$$\\text{Precision} = \\frac{TP}{TP + FP}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важность метрики определяется тем, насколько высока «цена» ложно положительного результата. Например, если стоимость дальнейшей проверки наличия беременности у пациента высока и мы не можем проверить все ложно положительные результаты, стоит максимизировать данную метрику."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полнота — доля объектов положительного класса, которые нашел алгоритм, из всех объектов положительного класса.\n",
    "\n",
    "$$\\text{Recall} = \\frac{TP}{TP + FN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо уделить особое внимание этой оценке, когда в поставленной задаче ошибка нераспознания положительного класса высока. Например, если у рассматриваемой группы пациенток беременность может протекать особенно тяжело."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*КАРТИНКА [из L01](https://edunet.kea.su/repo/EduNet-content/dev-2.1/L01/out/precision-recall.png)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В реальной жизни максимальная точность и полнота не достижимы одновременно и приходится искать некий баланс. Хотелось бы иметь некую метрику которая объединяла бы в себе информацию о точности и полноте нашего алгоритма.\n",
    "\n",
    "F-мера — среднее гармоническое точности и полноты:\n",
    "\n",
    "$$\\text{F-score}=2\\frac{\\text{Precision}×\\text{Recall}}{\\text{Precision}+\\text{Recall}}$$\n",
    "​\n",
    "Данная формула придает одинаковый вес точности и полноте, поэтому F-мера будет падать одинаково при уменьшении и точности и полноты. Можно рассчитать F-меру придав различный вес точности и полноте, если отдать приоритет одной из этих метрик при разработке алгоритма.\n",
    "\n",
    "$$\\text{F-score}=(β^2+1)\\frac{\\text{Precision}×\\text{Recall}}{β^2\\text{Precision}+\\text{Recall}}$$\n",
    "​\n",
    "$β$ принимает значения в диапазоне, $0<β<1$ если нужно отдать приоритет точности, а при $β>1$ приоритет отдается полноте. При $β=1$ формула сводится к предыдущей, что дает сбалансированную F-меру (также ее называют F1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Методы усреднения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В многоклассовых задачах подсчет качества сводится к вычислению одной из двухклассовых метрик.\n",
    "\n",
    "Пусть выборка состоит из  $K$  классов. Задача классификации ставится как $K$  задач об отделении класса  $i$  от остальных $(i=1,...,K)$. Для каждой из них можно посчитать свою матрицу ошибок.\n",
    "\n",
    "Выделяют три подхода:\n",
    "\n",
    "1. Микроусреднение\n",
    "\n",
    "Сначала элементы матрицы ошибок усредняются по всем классам. Например $TP = \\frac{1}{K}\\sum_{i=1}^KTP_i$. Затем по одной усреднённой матрице ошибок считаем точность, полноту, F-меру.\n",
    "\n",
    " $$\\text{Micro-precision} = \\frac{\\sum_{i=1}^KTP_i}{\\sum_{i=1}^KTP_i+\\sum_{i=1}^KFP_i}$$\n",
    "\n",
    " $$\\text{Micro-recall} = \\frac{\\sum_{i=1}^KTP_i}{\\sum_{i=1}^KTP_i+\\sum_{i=1}^KFN_i}$$\n",
    "\n",
    "2. Макрусреднение\n",
    "\n",
    "Сначала вычисляется итоговая метрика для каждого класса, а затем результаты усредняются по всем классам.\n",
    "\n",
    "$$\\text{Macro-precision} = \\frac{\\sum_{i=1}^K\\text{Precision}_i}{K}$$\n",
    "\n",
    "$$\\text{Macro-recall} = \\frac{\\sum_{i=1}^K\\text{Recall}_i}{K}$$\n",
    "\n",
    "3. Взвешивание\n",
    "\n",
    "Метрики для каждого класса умножаются на его вес, а затем складываются.\n",
    "\n",
    "$$\\text{Weighted-precision} = \\sum_{i=1}^K{w_i*\\text{Precision}_i}$$\n",
    "\n",
    "$$\\text{Weighted-recall} = \\sum_{i=1}^K{w_i*\\text{Recall}_i}$$\n",
    "\n",
    "$$w_i = \\frac{количество \\; объектов\\; класса\\; i}{общее \\; количество \\; объектов}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Библиотека scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для классификации будем использовать готовые методы из библиотеки [[doc] 🛠️ scikit-learn](https://scikit-learn.org/stable/) для машинного обучения.\n",
    "\n",
    " У них стандартные функции:\n",
    " - `fit` обучает модель на обучающей выборке\n",
    " - `predict` предсказывает классы на тестовой выборке\n",
    "\n",
    "Составление матрицы ошибок и подсчет всех метрик также может осуществляться инструментами sklearn.\n",
    "- матрица ошибок [🛠️[doc]](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)\n",
    "- точность [🛠️[doc]](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html)\n",
    "- полнота [🛠️[doc]](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)\n",
    "- F-мера [🛠️[doc]](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)\n",
    "\n",
    "Чтобы не считать все метрики по отдельности, можно сразу получить отчет о классификации [🛠️[doc]](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html):\n",
    "- метрики для каждого класса\n",
    "  - точность (precision)\n",
    "  - полнота (recall)\n",
    "  - F-мера (f1-score)\n",
    "  - количество объектов каждого класса (support)\n",
    "- усредненные метрики\n",
    "  - микроусредненные (micro avg)\n",
    "  - макроусредненные (macro avg)\n",
    "  - взвешенные (weighted avg)\n",
    "  \n",
    "Если микроусредненные точность, полнота и F-мера равны, выводится одно значение, равное также accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Методы векторизации текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый объект имеет некоторую числовую характеристику — признак. Совокупность всех признаков объекта называется его **признаковым описанием** и представляется в виде вектора.\n",
    "\n",
    "Если объектом является текст, в качестве признаков выступают слова, которые он содержит. Процесс преобразования текста в числа называется **векторизацией**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим два способа векторизации предложений из библиотеки scikit-learn. Для наглядности будем использовать небольшой корпус из трех предложений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "corpus = pd.Series(['She loves pizza, pizza is delicious.',\n",
    "                     'She is good person.',\n",
    "                     'Good people are the best.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Мешок слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мешок слов (bag of words) — представление текста, которое описывает вхождение слова в документ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{array}{c} \\hline\n",
    " & \\text{she} & \\text{loves} & \\text{pizza} & \\text{is} & \\text{delicious} & \\text{a} & \\text{good} & \\text{person} & \\text{people} & \\text{are} & \\text{the} & \\text{best} \\\\ \\hline\n",
    "\\text{She loves pizza, pizza is delicious.} & 1 & 1 & 2 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "\\text{She is a good person.} & 1 & 0 & 0 & 1 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 0 \\\\\n",
    "\\text{Good people are the best.} & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 1 & 1 & 1 & 1 \\\\ \\hline\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем векторизацию мешком слов с помощью класса `CountVectorizer` [🛠️[doc]](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). Метод `fit` собирает словарь, метод `transform` преобразует тексты в векторы на основе собранного словаря. Метод `fit_transform` выполняет все это сразу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bow = CountVectorizer()\n",
    "#bow.fit(corpus)\n",
    "#corpus_bow = bow.transform(corpus)\n",
    "corpus_bow = bow.fit_transform(corpus)\n",
    "corpus_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате мы получаем разрежённую (sparse) матрицу — это матрица с преимущественно нулевыми элементами. Если бо́льшая часть элементов матрицы ненулевая, она считается плотной (dense). Особенностью разреженных матриц является их компактность.\n",
    "\n",
    "[[blog] ✏️ Введение в разреженные матрицы](https://python-school.ru/blog/python/sparse-matrix/)\n",
    "\n",
    "Выведем результат для одного из предложений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bow_df = pd.DataFrame(corpus_bow.toarray(),\n",
    "                      columns = bow.get_feature_names_out(),\n",
    "                      index=corpus)\n",
    "bow_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По умолчанию в качестве признаков используются слова (униграммы). С помощью параметра `ngram_range` можно считать частоту встречаемости для *n*-грамм. Необходимо задать значения `min_n` и `max_n` (`default=(1, 1)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow1 = CountVectorizer(ngram_range=(2,3))\n",
    "corpus_bow1 = bow1.fit_transform(corpus)\n",
    "corpus_bow1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow1_df = pd.DataFrame(corpus_bow1.toarray(),\n",
    "                      columns = bow1.get_feature_names_out(),\n",
    "                      index=corpus)\n",
    "bow1_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметр `analyzer` определяет, какая единица предложения является признаком — целое слово или подслово. По умолчанию он принимает значение `‘word’`. Для использования символьных *n*-грамм нужно установить значение `‘char’` (границы слов включаются в *n*-граммы) или `‘char_wb’` (создает n-граммы символов только из текста внутри границ слов)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow2 = CountVectorizer(ngram_range=(4,6), analyzer='char_wb')\n",
    "corpus_bow2 = bow2.fit_transform(corpus)\n",
    "corpus_bow2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow2_df = pd.DataFrame(corpus_bow2.toarray(),\n",
    "                      columns = bow2.get_feature_names_out(),\n",
    "                      index=corpus)\n",
    "bow2_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$TF{\\text -}IDF$ ($TF$ — term frequency, $IDF$ — inverse document frequency)  — это способ векторизации текста, отражающий важность слова в документе, а не только частоту его появления.\n",
    "\n",
    "Частота слов ($TF$) — это мера частоты употребления слова $w$ в документе $d$. $TF$ определяется как отношение появления слова в документе к общему количеству слов в документе.\n",
    "\n",
    "$$TF(w,d) = \\frac{количество\\:вхождений\\:слова\\:w\\:в\\:документе\\:d}{общее\\:количество\\:слов\\:n\\:в\\:документе\\:d}$$\n",
    "\n",
    "Обратная частота документов ($IDF$) —  это мера важности слова. Некоторые слова могут присутствовать наиболее часто, но не имеют большого значения. $IDF$ присваивает вес каждому слову в зависимости от его частоты в корпусе $D$.\n",
    "\n",
    "$$IDF(w,D) = ln(\\frac{общее\\:количество\\:документов\\:N\\:в\\:корпусе\\:D}{количество\\:документов,\\:содержащих\\:слово\\:w})$$\n",
    "\n",
    "$TF{\\text -}IDF$ является произведением $TF$ и $IDF$.\n",
    "$$TF{\\text -}IDF(w,d,D)=TF(w,d)*IDF(w,D)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для векторизации воспользуемся классом `TfidfVectorizer` [🛠️[doc]](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). Применим метод `fit_transform` и посмотрим на результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "corpus_tfidf = tfidf.fit_transform(corpus)\n",
    "corpus_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame(corpus_tfidf.toarray(),\n",
    "                      columns = tfidf.get_feature_names_out(),\n",
    "                      index=corpus)\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно ограничить размер словаря и включать только слова, которые встречаются не реже N раз (`min_df`). Также можно убрать слова, которые встречаются слишком часто и являются стоп-словами в пределах данного корпуса (`max_df`). Оба параметра могут быть выражены целым числом либо числом с плавающей точкой в диапазоне [0.0, 1.0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf1 = TfidfVectorizer(min_df=2)\n",
    "corpus_tfidf1 = tfidf1.fit_transform(corpus)\n",
    "corpus_tfidf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf1_df = pd.DataFrame(corpus_tfidf1.toarray(),\n",
    "                      columns = tfidf1.get_feature_names_out(),\n",
    "                      index=corpus)\n",
    "tfidf1_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Библиотеки для анализа данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[doc] 🛠️ NumPy](https://numpy.org/) — это библиотека для поддержки больших многомерных массивов и быстрых математических функций для операций с этими массивами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Базовые функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основным объектом NumPy является массив чисел одного типа (`np.ndarray`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Один из способов создать массив — из обычных списков Python, используя функцию `numpy.array()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1, 2, 3])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда массив двумерный, он фактически становится матрицей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array([[1, 2, 3],\n",
    "              [4, 5, 6]])\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количество измерений массива можно узнать с помощью свойства `.ndim`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Размер массива можно узнать с помощью свойства `.shape`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = np.array([1, 2, 3])\n",
    "print(a1)\n",
    "print(a1.ndim)\n",
    "print(a1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2 = np.array([[1, 2, 3]])\n",
    "print(a2)\n",
    "print(a2.ndim)\n",
    "print(a2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a3 = np.array([[1],\n",
    "               [2],\n",
    "               [3]])\n",
    "print(a3)\n",
    "print(a3.ndim)\n",
    "print(a3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📌 Какое значение мы получим для массива `new_array`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_array = np.array([[1, 4, 6, 8],\n",
    "                      [2, 9, 3, 7],\n",
    "                      [5, 10, 6, 2]])\n",
    "# Место для вашего кода\n",
    "new_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме размерностей, можно также узнать тип элементов — в этом поможет свойство `.dtype`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_array.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё можно узнать количество элементов с помощью свойства  `.size`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_array.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Доступ к элементам массива"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно обращаться к отдельным элементам, строкам или столбцам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.array([[1, 2, 3, 4, 5, 6, 7],\n",
    "              [8, 9, 10, 11, 12, 13, 14]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы хотим достать их него элемент, который находится в первой строке на пятом месте. Сделать это можно с помощью специального оператора `[]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c[0, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если бы у нас был трёхмерный массив, обращение к его элементам было бы похожим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.array([[[1, 2, 3],\n",
    "               [4, 5, 6]],\n",
    "\n",
    "              [[7, 8, 9],\n",
    "              [10, 11, 12]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📌 Как в данном трехмерном массиве обратиться к элементу, который находится во втором двумерном массиве, во второй строке и втором столбце?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Место для вашего кода\n",
    "d[1, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме отдельных элементов, можно обратиться к целой строке или столбцу с помощью оператора среза `:`. Он позволяет выбрать все элементы указанной строки или столбца:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В первом случае мы выбрали всю первую строку, а во втором — первый столбец."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оператор `:` на самом деле представляет собой сокращённую форму конструкции начальный_индекс: конечный_индекс: шаг. Можно обращаться к любо выбранной последовательности элементов массива:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = np.array([[1, 2, 3, 4],\n",
    "              [5, 6, 7, 8]])\n",
    "e[0, 1:4:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы указали, что хотим выбрать первую строку, а затем уточнили, какие именно столбцы нам нужны: `1:4:2`.\n",
    "\n",
    "Первое число означает, что мы начинаем брать элементы с первого индекса — второго столбца.\n",
    "Второе число — что мы заканчиваем итерацию на четвёртом индексе, то есть проходим всю строку.\n",
    "Третье число указывает, с каким шагом мы идём по строке. В нашем примере — с шагом в два элемента. То есть мы пройдём по элементам 1, 3 и 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📌 В массиве `f` выведите все нечетные числа в последней строке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.array([[1, 2, 3, 4, 5],\n",
    "              [6, 7, 8, 9, 10],\n",
    "              [11, 12, 13, 14, 15]])\n",
    "# Место для вашего кода\n",
    "f[2, 0:5:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание специальных массивов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имеется несколько функций для того, чтобы создавать массивы с каким-то исходным содержимым (по умолчанию тип создаваемого массива — float64).\n",
    "\n",
    "Функция `zeros()` создает массив из нулей, а функция `ones()` — массив из единиц. Обе функции принимают кортеж с размерами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros((3, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ones((2, 2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для создания последовательностей чисел в NumPy имеется функция `arange()`. Она возвращает одномерный массив с равномерно разнесенными значениями внутри заданного интервала:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(3,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(0, 11, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(1, 11, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📌 Как создать массив из четных чисел от 2 до 10?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Место для вашего кода\n",
    "np.arange(2, 11, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Математические операции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Массивы можно складывать, умножать на число и на другой массив."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Одномерный массив = вектор\n",
    "v1 = np.array([7, 8, 9])\n",
    "v2 = np.array([3, 4, 5])\n",
    "# Двумерный массив = матрица\n",
    "m1 = np.array([[10, 11, 12], [13, 14, 15]])\n",
    "m2 = np.array([[1, 2, 3], [4, 5, 6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📌 Посчитайте сумму векторов `v1` и `v2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Место для вашего кода\n",
    "v1 + v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📌 Посчитайте произведение векторов `v1` и `v2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Место для вашего кода\n",
    "v1 * v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📌 Посчитайте сумму матриц `m1` и `m2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Место для вашего кода\n",
    "m1 + m2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📌 Посчитайте произведение вектора `v1` на число `n`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "# Место для вашего кода\n",
    "v1 * n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📌 Посчитайте произведение матрицы `m1` на число k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "# Место для вашего кода\n",
    "m1 * k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📌 Посчитайте произведение матрицы `m1` и вектора `v1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Место для вашего кода\n",
    "m1 * v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📌 Посчитайте произведение матриц `m1` и `m2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Место для вашего кода\n",
    "m1 * m2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напоминание про матричное умножение:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ AB= \\begin {pmatrix} 2 & 3\\\\ 4 & -1 \\end{pmatrix} \\times \\begin{pmatrix} 1 & 0\\\\ 5 & -2 \\end{pmatrix} =\\begin {pmatrix} 2+15 & 0-6 \\\\ 4-5 & 0+2 \\end{pmatrix} = \\begin{pmatrix} 17 & -6\\\\ -1 & 2 \\end{pmatrix}$\n",
    "\n",
    "$ BA= \\begin{pmatrix} 1 & 0\\\\ 5 & -2 \\end{pmatrix} \\times \\begin {pmatrix} 2 & 3\\\\ 4 & -1 \\end{pmatrix} =\\begin {pmatrix} 2+0 & 3+0 \\\\ 10-8 & 15+2 \\end{pmatrix} = \\begin{pmatrix} 2 & 3\\\\ 2 & 17 \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скалярное произведение векторов:\n",
    "$$\\vec{a}\\cdot\\vec{b}=|\\vec{a}|\\cdot|\\vec{b}|\\cdot\\cos(\\alpha)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(v1, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[doc] 🛠️ Pandas](https://pandas.pydata.org/) — библиотека для обработки и анализа табличных данных. В этой библиотеке используется NumPy для удобного хранения данных и вычислений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В библиотеке Pandas определены два класса объектов для работы с данными:\n",
    "\n",
    "> `Series` – одномерный массив, который может хранить значения любого типа данных.\n",
    "\n",
    "> `DataFrame` – двумерный массив (таблица), в котором столбцами являются объекты класса Series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создать объект класса `Series` можно следующим образом:\n",
    "\n",
    "`s = pd.Series(data, index=index)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве `data` могут выступать: массив `numpy`, словарь, число. В аргумент `index` передаётся список меток строк. Метка может быть числом или строкой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если `data` является массивом `numpy`, то `index` должен иметь такую же длину, как и `data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(np.arange(5), index=[\"a\", \"b\", \"c\", \"d\", \"e\"])\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если аргумент `index` не передаётся, то по умолчанию для `index` автоматически назначается список [0, ..., len(data) - 1]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(np.linspace(0, 1, 5))\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще один способ создать структуру `Series` – это использовать словарь для одновременного задания меток и значений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\"a\": 10, \"b\": 20, \"c\": 30, \"d\": 40}\n",
    "pd.Series(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для `Series` доступно взятие элемента по индексу, срезы, поэлементные математические операции аналогично массивам `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(np.arange(5), index=[\"a\", \"b\", \"c\", \"d\", \"e\"])\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выбор одного элемента\n",
    "s[\"a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выбор нескольких элементов\n",
    "s[[\"a\", \"d\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Срез\n",
    "s[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Поэлементное сложение\n",
    "s + s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для `Series` можно применять фильтрацию данных по условию, записанному в качестве индекса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(np.arange(5), index=[\"a\", \"b\", \"c\", \"d\", \"e\"])\n",
    "s[s > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объект класса `DataFrame` работает с двумерными табличными данными. Создать `DataFrame` проще всего из словаря Python следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students_marks_dict = {\"student\": [\"Студент_1\", \"Студент_2\", \"Студент_3\"],\n",
    "                       \"math\": [5, 3, 4],\n",
    "                       \"physics\": [4, 5, 5]}\n",
    "students = pd.DataFrame(students_marks_dict)\n",
    "students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " У объекта класса `DataFrame` есть индексы по строкам (`index`) и столбцам(`columns`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для индекса по строке по умолчанию задаётся числовое значение. Значения индекса можно заменить путем записи списка в атрибут `index`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students.index = [\"A\", \"B\", \"C\"]\n",
    "students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для доступа к записям таблицы по строковой метке используется атрибут `loc`. При использовании строковой метки доступна операция среза:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students.loc[\"B\":]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Импорт данных из файла"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычно табличные данные хранятся в файлах. Такие наборы данных принято называть датасетами. Файлы с датасетом могут иметь различный формат. Pandas поддерживает операции чтения и записи для CSV, Excel 2007+, SQL, HTML, JSON и др."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Несколько примеров, как получить датасет из файлов разных форматов:\n",
    "\n",
    "> CSV. Используется функция `read_csv()`. Аргумент `file` является строкой, в которой записан путь до файла с датасетом. Для записи данных из DataFrame в CSV-файл используется метод `to_csv(file)`.\n",
    "\n",
    "> Excel. Используется функция `read_excel()`. Для записи данных из `DataFrame` в Excel-файл используется метод `to_excel()`.\n",
    "\n",
    "> JSON. Используется функция `read_json()`. Для записи данных из `DataFrame` в JSON используется метод `to_json()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для работы с другими форматами файлов в Pandas есть функции, работающие аналогично рассмотренным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для дальнейшей работы загрузим файл с датасетом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/datasets/students_performance.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим датасет из CSV-файла с данными о студентах:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students = pd.read_csv(\"students_performance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полученный объект `students` относится к классу `DataFrame`.\n",
    "\n",
    "Для получения первых n строк датасета используется метод `head(n)`. По умолчанию возвращается 5 первых строк:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для получения последних n строк используется метод `tail(n)`. По умолчанию возвращается 5 последних строк:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📌 Как вывести строки 10-12 датасета?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Место для вашего кода\n",
    "students[10:13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве индекса можно использовать условия для фильтрации данных. Выберем 5 первых результатов теста по математике для студентов, прошедших подготовительный курс."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students[students[\"test preparation course\"] == \"completed\"][\"math score\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📌 Как вывести 10 последних результатов теста по чтению для студентов бакалавриата?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Место для вашего кода\n",
    "students[students[\"parental level of education\"] == \"bachelor's degree\"][\"reading score\"].tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Разведочный анализ данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Разведочный анализ данных/ Exploratory data analysis (EDA)** — анализ основных свойств данных, нахождение в них общих закономерностей, зачастую с использованием инструментов визуализации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачаем набор данных [[doc] 🛠️ SMS Spam Collection Dataset](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset). Он содержит смс-сообщения на английском языке, размеченные как «спам» (spam) и «не спам» (ham). О том, почему классы называются именно так, можно почитать в статье [[wiki] 📚 Spam (food)](https://simple.wikipedia.org/wiki/Spam_(food)#:~:text=The%20Hormel%20Foods%20Corporation%20once,%E2%80%9CSizzle%20Pork%20And%20Mmmm%E2%80%9D.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/datasets/SMSSpamCollection.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sms = pd.read_csv('SMSSpamCollection.csv', sep='\\t', # метки классов отделены от сообщений табуляцией\n",
    "                       header=None, names=['label', 'text']) # заголовка нет, даем названия столбцам\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим данные на наличие дублирующихся строк."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms.duplicated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалим повторяющиеся данные, сохранив первое вхождение для каждого дубля."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms.drop_duplicates(keep='first',inplace=True)\n",
    "sms.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, какое количество сообщений каждого класса представлено в датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sms.shape)\n",
    "print(sms['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Распределение классов можно визуализировать. Воспользуемся библиотекой [[doc] 🛠️ Matplotlib](https://matplotlib.org/) для рисования круговой диаграммы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.pie(sms['label'].value_counts(), labels=sms['label'].unique(), autopct='%.1f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим, насколько длинные сообщения встречаются в датасете. Разделим тексты по пробелам и посчитаем количество слов. Пословное деление предложений называется **токенизацией**. Токенизация может осуществляться не только на слова, но и на части слов (подслова). Об этом мы подробно поговорим в следующих лекциях.\n",
    "\n",
    "Для примера токенизируем одно сообщение и рассчитаем его длину."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = sms['text'][0].split()\n",
    "print(f'Word count in a sample message: {len(word_count)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем количество слов для всех сообщений и запишем результат в датафрейм в качестве нового столбца."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms['word_count'] = sms['text'].map(lambda x: len(x.split()))\n",
    "sms.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируем результат с помощью гистограммы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sms['word_count'])\n",
    "plt.title('Word count in messages')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно отдельно вывести минимальную, максимальную и среднюю длину сообщения, которые встретились в датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Minimum word count: {sms['word_count'].min()}\")\n",
    "print(f\"Maximum word count: {sms['word_count'].max()}\")\n",
    "print(f\"Mean word count: {sms['word_count'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем количество вхождений для каждого слова. Для этого создадим объект класса Counter(), он позволяет быстро посчитать количество появлений элементов в последовательности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "word_frequency = Counter(\" \".join(sms['text']).split())\n",
    "print(word_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим количество уникальных слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем в отдельную переменную топ-10 самых частотных слов и выведем результат в виде столбчатой диаграммы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10 = word_frequency.most_common(10)\n",
    "top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(y=[x[0] for x in top_10], width=[x[1] for x in top_10])\n",
    "plt.title('Top 10 most frequently occuring words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно заметить, что для слов разного регистра частотность считается отдельно (i vs. I). Также в топ-10 попало много служебных слов, которые вряд ли помогут понять, является ли сообщение спамом. Следовательно, прежде чем переходить к построении модели машинного обучения, необходимо осуществить подготовку и чистку данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо удалить стоп-слова — это часто используемые слова, которые не вносят никакой дополнительной информации в текст. В билиотеке [[doc] 🛠️ NLTK](https://www.nltk.org/) есть встроенный список стоп-слов, которым мы воспользуемся."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, входят ли наши самые частотные слова в этот список."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in top_10:\n",
    "    print(f\"'{item[0]}' in stopwords:\\\n",
    "    {item[0] in stopwords.words('english')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Слово I не вошло из-за того, что написано в верхнем регистре. Еще один этап предобработки текста — приведение слов к нижнему регистру. Это можно сделать с помощью метода `lower()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in top_10:\n",
    "    print(f\"'{item[0].lower()}' in stopwords: {item[0].lower() in stopwords.words('english')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Слово u отсутствует в исходном списке, но мы можем добавить его сами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = stopwords.words('english') + ['u']\n",
    "print(STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in top_10:\n",
    "    print(f\"'{item[0].lower()}' in stopwords: {item[0].lower() in STOPWORDS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь все частотные слова входят в список стоп-слов.\n",
    "\n",
    "Наконец, из текста нужно удалить знаки препинания. Для этого воспользуемся `string.punctuation` — это предварительно инициализированная строка,содержащая знаки препинания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для примера к каждому токену первого текста применим метод `strip()` и удалим знаки препинания, а затем через пробел соединим токены обратно к строку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sms['text'][0])\n",
    "print(' '.join([token.strip(punctuation).lower()\n",
    "for token in sms['text'][0].split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📌 Объедините все этапы предобработки в функции `text_preprocessing`. Она принимает на вход сырой текст и список стоп-слов. Функция токенизирует текст по словам, каждый токен приводит к нижнему регистру, удаляет знакки препинания и стоп-слова. На выходе мы должны получить строку из токенов после предобработки, разделенных пробелами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text, stopwords):\n",
    "    # Место для вашего кода\n",
    "    tokens = [token.strip(punctuation).lower() for token in text.split()\n",
    "    if token.lower() not in STOPWORDS]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sms['text'][0])\n",
    "print(text_preprocessing(sms['text'][0], STOPWORDS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осуществим предобработку всех текстов в датсете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms['preprocessed'] = sms['text'].apply(lambda x:\n",
    "                                        text_preprocessing(x, STOPWORDS))\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В следующем разделе обучим на наших данных модель для автоматической классификации спама."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Наивный байесовский классификатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наивный байесовский классификатор — это алгоритм классификации, основанный на теореме Байеса с допущением о независимости признаков. Другими словами, НБА предполагает, что наличие какого-либо признака в классе не связано с наличием какого-либо другого признака."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теорема Байеса позволяет рассчитать вероятность события $A$, основываясь на прошлом (априорном) знании условий $B$:\n",
    "\n",
    "$$P (A | B) = \\frac{P (B | A) × P(A)}{P(B)}$$\n",
    "\n",
    "$P(A$) – априорная (безусловная) вероятность события $A$;\n",
    "  - связана с некоторым случайным событием $A$;\n",
    "  - представляет степень уверенности в том, что данное событие произошло, в отсутствие любой другой информации, связанной с этим событием.\n",
    "\n",
    "Например, мы можем выдвинуть гипотезу H, что некоторый объект или наблюдение принадлежит классу C, безотносительно свойств этого объекта. Тогда P(H) будет априорной вероятностью данного события.\n",
    "\n",
    "$P(A | B)$ – апостериорная вероятность $A$ (вероятность события $A$\n",
    "при наступлении события $B$)\n",
    "- вероятность значения, принимаемого случайной переменной,\n",
    "-назначается после принятия во внимание некоторой новой, связанной с ней информации.\n",
    "  \n",
    "Иными словами, это вероятность события $A$ при условии, что произошло событие\n",
    "$B$.\n",
    "\n",
    "Например, при условии, что плод красный и круглый, мы с большой долей уверенности можем предположить, что это яблоко, чем в случае, если эта информация отсутствует, т.е. апостериорная вероятность данного события будет\n",
    "$P (\\text{яблоко|красный, круглый})$.\n",
    "\n",
    "$P(B)$ – априорная вероятность события $B$;\n",
    "\n",
    "$P(B | A)$ – апостериорная вероятность $B$ (вероятность наступления события $B$ при истинности события $A$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификация спама"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем использовать наивный байесовский классификатор для фильтрации спама.  В рамках данной задачи имеются:\n",
    "- Датасет из текстов сообщений с некоторым фиксированным словарём возможных слов.\n",
    "- Два класса сообщений: спам и нормальное.\n",
    "- Признаковое описание для каждого сообщения, характеризующее количество вхождений каждого из слов словаря в текст сообщения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого класса $c$ требуется найти $P(c|d)$ — вероятность класса $c$ для документа $d$. Она рассчитывается по формуле Байеса:\n",
    "\n",
    "$$P(c|d) = \\frac{P(d|c) P(c)} {P(d)}$$\n",
    "\n",
    "$P(d)$: вероятность документа $d$ одинакова для всех классов, поэтому её можно опустить.\n",
    "\n",
    "Получим:\n",
    "\n",
    "$$P(c|d) = P(d|c) P(c)$$\n",
    "\n",
    "$P(c)$: вероятность класса $c$ — это доля документов класса $c$ среди всех документов.\n",
    "\n",
    "$P(d|c)$: вероятность документа $d$ для класса $c$ зависит от слов $x_1, x_2, ..., x_n$, входящих в документ:\n",
    "\n",
    "$$P(d|c) = P(x_1,x_2,...,x_n|c) = P(x_1|c)P(x_2|c)...P(x_n|c) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть у нас есть датасет, где все письма состоят из слов $x_1, x_2, x_3, x_4$: **‘добрый’, ‘день’, ‘гости’, ‘деньги’**. Мы уже посчитали, сколько раз каждое слово встречается в каждом классе.\n",
    "\n",
    "Мы можем посчитать $P(x_1|c)$ — вероятность встретить слово **‘добрый’** в нормальном письме: берем количество слов **‘добрый’** и делим на количество слов во всех письмах. Аналогично для других слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*КАРТИНКА из блокнота [Naive_bayes_NLP](https://edunet.kea.su/repo/EduNet-additions/L02/naive_bayes_1.png). Необходимо исправить картинку: в знаменателе должно быть не количество слов во всех нормальных письмах, а количество нормальных писем, то есть 8.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Делаем то же самое для слов из спама."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*КАРТИНКА из блокнота [Naive_bayes_NLP](https://edunet.kea.su/repo/EduNet-additions/L02/naive_bayes_2.png).*\n",
    "\n",
    "*Мне кажется, стоит ее немного исправить. Более распространенный подход — в числителе считать количество нормальных писем с данным словом, а в знаменателе общее количество нормальных писем. См., например, [Байесовская фильтрация спама](https://ru.wikipedia.org/wiki/%D0%91%D0%B0%D0%B9%D0%B5%D1%81%D0%BE%D0%B2%D1%81%D0%BA%D0%B0%D1%8F_%D1%84%D0%B8%D0%BB%D1%8C%D1%82%D1%80%D0%B0%D1%86%D0%B8%D1%8F_%D1%81%D0%BF%D0%B0%D0%BC%D0%B0)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем $P(c)$ — вероятность того, что письмо не является спамом. Для этого количество нормальных писем делим на общее количество писем. Аналогично для спама."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*КАРТИНКА из блокнота [Naive_bayes_NLP](https://edunet.kea.su/repo/EduNet-additions/L02/naive_bayes_3.png).*\n",
    "\n",
    "*По этой картинке аналогичный комментарий.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем вычислить $P(d|c)$ для письма **‘добрый день’**. Для этого перемножим $P(x_1|d)$ — вероятность нормального письма со словом **‘добрый’** и и $P(x_2|d)$ — вероятность нормального письма со словом **‘день’**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*КАРТИНКА с формулами:*\n",
    "\n",
    "*p(добрый день|нормальное) = p(добрый|нормальное) × p(день|нормальное)*\n",
    "\n",
    "*p(добрый день|СПАМ) = p(добрый|СПАМ) × p(день|СПАМ)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось получить $P(c|d)$ — вероятность нормального письма с фразой **‘добрый день’** в «наивном» предположении. Нужно умножить $P(d|c)$ — вероятность нормального письма** ‘добрый день’** на $P(c)$  — вероятность того, что письмо не является спамом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*КАРТИНКА из блокнота [Naive_bayes_NLP](https://edunet.kea.su/repo/EduNet-additions/L02/naive_bayes_3_5.png)*\n",
    "\n",
    "*Эту картинку тоже нужно немного поменять. Вместо p(нормальное) должно быть p(нормальное|добрый день). Вместо p(СПАМ) — p(СПАМ|добрый день). Также в формулу нужно добавить умножение на p(нормальное) и p(СПАМ) соответственно.*\n",
    "\n",
    "\n",
    "\n",
    "*Итоговый вариант*:\n",
    "\n",
    "*p(нормальное|добрый день) = p(добрый день|нормальное) × p(нормальное)*\n",
    "\n",
    "*p(СПАМ|добрый день) = p(добрый день|СПАМ) × p(СПАМ)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение и тестирование модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем работать с уже известным нам набором данных [[doc] 🛠️ SMS Spam Collection Dataset](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset). Для удобства заменим словесные обозначения классов на числовые."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms['num_label'] = sms['label'].astype('category').cat.codes\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем в отдельные переменные предобработанные тексты `X` и метки классов `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = sms['preprocessed'], sms['num_label']\n",
    "X[:5], y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделить данные на обучающую и тестовую выборку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы подавать тексты в модель машинного обучения, необходимо представить каждое предложение в виде набора признаков — вектора. В качестве признаков будем подавать модели слова, встретившиеся в обучающей выборке. Те слова, которые встретятся в тестовой выборке, но отсутствуют в обучающей, не могут быть проинтерпретироованы моделью.\n",
    "\n",
    "Будем использовать модель векторизации «мешок слов». Словарь необходимо собирать на основе обучающей выборки (метод `fit`). При этом преобразование текстов в векторы на основе собранного словаря нужно осуществить для всего датасета (метод `transform`).\n",
    "\n",
    "📌 Попробуйте самостоятельно реализовать векторизацию обучающей и тестовой выборок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer()\n",
    "#vect.fit(X_train)\n",
    "#X_train_vect = vect.transform(X_train)\n",
    "X_train_vect = vect.fit_transform(X_train)\n",
    "X_test_vect = vect.transform(X_test)\n",
    "X_train_vect, X_test_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим модель для классификации — наивный байесовский классификатор. Осуществим обучение модели на обучающей выборке из предложений (`X_train_bow`) и меток (`y_train`) с помощью метода `fit`. Затем используем обученную модель для предсказания меток на основе предложений тестовой выборки (`X_test_bow`) с помощью метода `predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_vect, y_train)\n",
    "y_mnb = mnb.predict(X_test_vect)\n",
    "y_mnb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим качество классификации  с помощью метрики accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_mnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📌 Поскольку классы не сбалансированы, посчитайте также точность (precision), полноту (recall) и F-меру (f1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "# Место для вашего кода\n",
    "precision_score(y_test, y_mnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "# Место для вашего кода\n",
    "recall_score(y_test, y_mnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "# Место для вашего кода\n",
    "f1_score(y_test, y_mnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы получили довольно высокое качество. Но можно ли еще улучшить его? Например, за счет гиперпараметров векторизации?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подбор гиперпараметров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Для автоматического подбора параметров используется модуль `GridSearchCV`[🛠️[doc]](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). Он создает модель для каждой возможной комбинации параметров.\n",
    "\n",
    "Все этапы обработки — векторизацию и классификацию — объединим в `Pipeline`[🛠️[doc]](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html). Пайплайн в машинном обучении — это процесс обработки данных от начала до конца, включая ввод данных, их обработку, применение модели с установленными параметрами и вывод результата.\n",
    "\n",
    "Создаем словарь `parameters`, содержащий диапазон значений каждого параметра. Далее инициализируем объект `grid_search`, передавая ему пайплайн (векторизатор и модель). По умолчанию количество итераций равно 10 (`n_iter`), то есть будут сравниваться 10 разных моделей. Установим количество кросс-валидаций (`cv=5`).\n",
    "\n",
    "Кросс-валидация — перекрестная проверка.\n",
    "\n",
    "1. Фиксируется целое число $k$, меньшее числа примеров в датасете.\n",
    "2. Датасет разбивается на $k$ одинаковых частей.\n",
    "3. Происходит $k$ итераций, в каждой из которых одна часть выступает в роли тестового множества, а объединение остальных — в роли тренировочного.\n",
    "4. Финальный результат модели получается либо усреднением получившихся тестовых результатов, либо измеряется на отложенном тестовом множестве, не участвовавшем в кросс-валидации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*КАРТИНКА из [L01](https://edunet.kea.su/repo/EduNet-content/dev-2.1/L01/out/cross_validation_on_train_data.png)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим следующие параметры:\n",
    "- `ngram_range`:\n",
    "  - (1, 1) — только униграммы;\n",
    "  - (1, 2) — униграммы и биграммы.\n",
    "- `min_df`\n",
    "  -  0.0001 — исключаем токены, которые встретились в меньше чем 0,001% документов;\n",
    "  -  0.001 — исключаем токены, которые встретились в меньше чем 0,01% документов.\n",
    "- `max_df`:\n",
    "  - 0.7 — исключаем токены, которые встретились в больше чем 70% документов;\n",
    "  - 1.0 — исключаем токены, которые встретились в больше чем 100% документов.\n",
    "\n",
    "В качестве метрики для сравнения (`scoring`) будем использовать F1-меру. Выведем лучшие значения параметров (`best_params_`) и лучшее значение метрики (`best_score_`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipeline = Pipeline([\n",
    "           ('vect', CountVectorizer()),\n",
    "           ('clf', MultinomialNB()),\n",
    "])\n",
    "parameters = {\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "    'vect__min_df': (0.0001, 0.001),\n",
    "    'vect__max_df': (0.7, 1.0)\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, cv=5, scoring='f1')\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_, grid_search.best_score_.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "           ('vect', CountVectorizer(max_df=0.7, min_df=0.0001, ngram_range=(1, 2))),\n",
    "           ('clf', MultinomialNB()),\n",
    "])\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "f1_score(y_test, y_pred).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "За счет автоматического подбора гиперпараметров качество слегка возросло."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
