{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Введение в машинное обучение</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Два пути"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Допустим, у нас есть задача автоматически опеределить тональность (эмоциональную окраску) отзыва на некоторый товар или услугу: является он позитивным, негативным или нейтральным. К её решению можно подойти двумя способами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-content/L01/out/sentiment_task.png\" width=\"650\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вариант №1: подход на основе правил (rule-based)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать **вручную** заданные **правила** классификации и эмоционально размеченные **словари**. Эти правила рассчитывают класс текста на основе эмоциональных ключевых слов и их совместного использования с другими ключевыми словами.\n",
    "\n",
    "Несмотря на высокую эффективность в текстах какой-то определенной тематики, методы на основе правил плохо способны обобщать. Чем больше примеров мы будем анализировать, тем больше исключений будет появляться.\n",
    "\n",
    "Следовательно, потребуется добавлять новые правила и увеличивать размер словаря. Алгоритмическая сложность программы будет расти, ее будет сложнее поддерживать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-content/L01/out/sentiment_rules.png\" width=\"750\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вариант №2: машинное обучение (machine learning)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С появлением **машинного обучения** мы можем применить принципиально другой подход. Он заключается в том, чтобы обучить модель, которая сама будет извлекать шаблоны из данных.\n",
    "\n",
    "Зачастую пользователь не только пишет текст отзыва, но и ставит оценку от 1 до 10. Мы можем использовать ее в качестве разметки для данных: отзывы с оценкой от 1 до 3 будем считать негативными, от 4 до 7 — нейтральными, от 8 до 10 — позитивными.\n",
    "\n",
    "Соберем некоторое количество отзывов и оценок за определенное время. Загрузим данные в модель, и она обучится на этих данных. При достаточном количестве данных и адекватно подобранной модели мы сможем научить ее решать конкретные задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-content/L01/out/sentiment_model.png\" width=\"550\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По сути, модели всё равно, что сделать: проанализировать тональность отзыва, отфильтровать спам-письма, распределить новости по тематикам, определить язык текста и т.д. Нет необходимости писать под каждый пример отдельную программу: достаточно собрать данные, и мы сможем решить множество абсолютно разных задач.\n",
    "\n",
    "Важно лишь понимать, какую модель предпочтительнее выбрать. С этим мы будем разбираться в ходе курса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача курса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  AI, ML, ANN, DL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Место глубокого обучения и нейронных сетей в ИИ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-content/L01/out/ai_ml_ann_dl.png\" width=\"550\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Искусственный интеллект (ИИ)/ Artificial Intelligence (AI)**  — область компьютерных наук, связанная с моделированием интеллектуальных или творческих видов человеческой деятельности.\n",
    "\n",
    "**Машинное обучение/ Machine learning (ML)** — подраздел ИИ, связанный с разработкой алгоритмов и статистических моделей, которые компьютерные системы используют для выполнения задач без явных инструкций.\n",
    "\n",
    "**Искусственная нейронная сеть (ИНС)/  Artificial neural network (ANN)** — разновидность алгоритмов машинного обучения, математическая модель, построенная по принципу организации и функционирования биологических нейронных сетей. ИНС состоит из слоев «нейронов», которые связаны между собой. В простом случае это входной слой и выходной слой.\n",
    "\n",
    "**Глубокое обучение/ Deep Learning (DL)** — обучение «глубоких» ИНС. Помимо входного и выходного слоя, они состоят из сотен дополнительных «скрытых» слоев между видимыми слоями для ввода и вывода."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Области применения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В последнее время именно такого рода модели показывают высокую эффективность в тех областях, с которыми ранее могли справиться только люди.\n",
    "Алгоритмы машинного обучения могут обрабатывать данные различного типа:\n",
    "- Компьютерное зрение / Computer vision (CV) → изображения и видео\n",
    "- Обработка естественного языка / Natural language processing (NLP) → тексты\n",
    "- Распознавание и синтез речи / Automatic Speech Recognition (ASR) & Text to speech (TTS) → аудио"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обзор курса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<big>Лекция 1 Машинное обучение</big>\n",
    "\n",
    "**Зачем:**\n",
    "\n",
    "* Разобраться, как в целом подходить к задачам машинного обучения.\n",
    "\n",
    "**Что будет:**\n",
    "\n",
    "* Основные понятия, обучение с учителем и без учителя;\n",
    "* Инструменты (NumPy, Pandas, Scikit-learn, Matplotlib, Seaborn);\n",
    "* Разведывательный анализ, работа с данными;\n",
    "* Базовые метрики;\n",
    "* Методы векторизации: мешок слов, TF-IDF;\n",
    "* Методы машинного обучения: наивный байесовский классификатор, логистическая регрессия;\n",
    "* Построение классификатора и оценка качества.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L01/l01_meme.png\" width=\"400\" ></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<big>Лекция 2 Нейронные сети</big>\n",
    "\n",
    "**Зачем:**\n",
    "\n",
    "* Познакомиться с нейронными сетями — классом моделей машинного обучения, которые позволяют решать разнообразные задачи, начать их создавать и обучать.\n",
    "\n",
    "**Что будет:**\n",
    "\n",
    "* Интуиция, почему нейросети — очень выразительный класс моделей машинного обучения;\n",
    "* Основные «строительные блоки» нейросетей;\n",
    "* Основной метод обучения нейросетей — метод обратного распространения ошибки;\n",
    "* Знакомство с **PyTorch** — основной программной библиотекой глубокого обучения, которой будем пользоваться на курсе;\n",
    "* Построение векторных представлений слов на основе нейросетей;\n",
    "* Процесс создания и обучения нейронной сети для задачи классификации по тональности отзывов на фильмы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-content/L01/out/nn_fully_connected.png\" width=\"450\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задачи машинного обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-content/L01/out/machine_learning.png\" width=\"800\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По типу обучения можно выделить **обучение с учителем** и **обучение без учителем**. Это основные типы; по ходу курса мы познакомимся ещё с несколькими."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение с учителем (supervised learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом случае есть множество **объектов** $X$, множество **ответов** $y$. Каждый объект имеет некую числовую характеристику — **признак**. Совокупность всех признаков объекта называется его **признаковым описанием**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L01/features_target.png\" width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Требуется найти функциональную зависимость ответов от описаний объектов и построить алгоритм, принимающий на входе описание объекта и выдающий на выходе ответ.\n",
    "\n",
    "Имеющиеся данные разделяются на **обучающую** и **тестовую** выборку.\n",
    "- Обучающая выборка — это примеры, на основе которых алгоритм ищет зависимость ответов от описаний объектов и строит общую закономерность.\n",
    "- Тестовая выборка используется для оценки качества алгоритма на новых данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-content/L01/out/train_test.png\" width=\"900\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Классификация (classification)** — отнесение образца к одному из нескольких попарно не пересекающихся множеств. Множество допустимых ответов конечно. Их называют метками классов (class label)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-content/L01/out/classification.png\" width=\"650\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Примеры задач классификации:* фильтрация спама, анализ тональности, классификация по тематике, определение языка."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Регрессия (regression)** — соотнесение объекта с некоторым числом или числовым вектором. Отсутствуют жесткие ограничения на пространство ответов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-content/L01/out/regression.png\" width=\"650\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Примеры задач регрессии:* предсказание стоимости товара по текстовому описанию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение без учителя (unsupervised learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом случае ответы не задаются, и требуется искать зависимости между объектами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Кластеризация (clustering)** — разбиение множества входных данных на группы с учетом попарного сходства объектов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-content/L01/out/clustering.png\" width=\"650\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Пример задачи кластеризации:* распределение новостей по тематическим кластерам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# План исследования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Допустим, вы решили заняться разработкой классификатора спама. Как будет выглядеть план исследования?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L01/out/pipeline.png\" width=\"700\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сбор данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Где можно добыть данные?\n",
    "\n",
    "* Эксперименты в вашей лаборатории\n",
    "* [[doc] 🛠️ HuggingFace](https://huggingface.co/datasets)\n",
    "* [[doc] 🛠️ Соревнования Kaggle](https://www.kaggle.com/datasets)\n",
    "* [[doc] 🛠️ Google Datasets](https://datasetsearch.research.google.com/)\n",
    "* [[article] 🎓 Сайт Papers with Code](https://paperswithcode.com/)\n",
    "\n",
    "Пройдитесь по соседним лабораториям. Напишите письма авторам статей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если вы используете данные, скачанные из сети, проверьте, откуда они. Описаны ли они в статье? Если да, посмотрите на документ, убедитесь, что он был опубликован в авторитетном месте, и проверьте, упоминают ли авторы какие-либо ограничения на использованные датасеты.\n",
    "\n",
    "Если данные использовались в ряде работ, это еще не гарантирует высокое качество датасета. **Иногда данные используются только потому, что их легко достать**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Векторизация текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Методы машинного обучения работают с числами. Нам нужно найти способы, которые позволят представлять тексты в виде числовых данных.\n",
    "\n",
    "Если объектом является текст, в качестве признаков выступают слова, которые он содержит. Процесс преобразования текста в числа называется **векторизацией**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://monkeylearn.com/static/e7dd6511434a685cc7d20a7147e108d3/4394e/Text-to-vector-3_normal.webp\" width=\"600\"></center>\n",
    "\n",
    "\n",
    "\n",
    "<center><em>Источник: <a href=\"https://monkeylearn.com/blog/word-embeddings-transform-text-numbers/\">How to transform text into numbers</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим два способа векторизации предложений из библиотеки scikit-learn. Для наглядности будем использовать небольшой корпус из трех предложений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "corpus = pd.Series(['She loves pizza, pizza is delicious.',\n",
    "                     'She is good person.',\n",
    "                     'Good people are the best.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Мешок слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://i.ibb.co/jb4wrW9/bag.webp\" width=\"600\"></center>\n",
    "\n",
    "\n",
    "<center><em>Источник: <a href=\"https://bigdataschool.ru/blog/feature-extraction-text-data-preparation.html\">Оцифровываем текст</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мешок слов (bag of words) — представление текста, которое описывает вхождение слова в документ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем векторизацию мешком слов с помощью класса `CountVectorizer` [🛠️[doc]](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). Метод `fit` собирает словарь, метод `transform` преобразует тексты в векторы на основе собранного словаря. Метод `fit_transform` выполняет все это сразу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bow = CountVectorizer()\n",
    "#bow.fit(corpus)\n",
    "#corpus_bow = bow.transform(corpus)\n",
    "corpus_bow = bow.fit_transform(corpus)\n",
    "corpus_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате мы получаем разрежённую (sparse) матрицу — это матрица с преимущественно нулевыми элементами. Если бо́льшая часть элементов матрицы ненулевая, она считается плотной (dense). Особенностью разреженных матриц является их компактность.\n",
    "\n",
    "[[blog] ✏️ Введение в разреженные матрицы](https://python-school.ru/blog/python/sparse-matrix/)\n",
    "\n",
    "Выведем результат для всех предложений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bow_df = pd.DataFrame(corpus_bow.toarray(),\n",
    "                      columns = bow.get_feature_names_out(),\n",
    "                      index=corpus)\n",
    "bow_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По умолчанию в качестве признаков используются слова (униграммы). С помощью параметра `ngram_range` можно считать частоту встречаемости для *n*-грамм. Необходимо задать значения `min_n` и `max_n` (`default=(1, 1)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow1 = CountVectorizer(ngram_range=(2,3))\n",
    "corpus_bow1 = bow1.fit_transform(corpus)\n",
    "corpus_bow1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow1_df = pd.DataFrame(corpus_bow1.toarray(),\n",
    "                      columns = bow1.get_feature_names_out(),\n",
    "                      index=corpus)\n",
    "bow1_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметр `analyzer` определяет, какая единица предложения является признаком — целое слово или подслово. По умолчанию он принимает значение `‘word’`. Для использования символьных *n*-грамм нужно установить значение `‘char’` (границы слов включаются в *n*-граммы) или `‘char_wb’` (создает n-граммы символов только из текста внутри границ слов)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow2 = CountVectorizer(ngram_range=(4,6), analyzer='char_wb')\n",
    "corpus_bow2 = bow2.fit_transform(corpus)\n",
    "corpus_bow2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow2_df = pd.DataFrame(corpus_bow2.toarray(),\n",
    "                      columns = bow2.get_feature_names_out(),\n",
    "                      index=corpus)\n",
    "bow2_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://contentpowered-bc85.kxcdn.com/wp-content/uploads/2021/12/TF-IDF-Illustration.jpg.webp\" width=\"600\"></center>\n",
    "\n",
    "\n",
    "<center><em>Source: <a href=\"https://www.contentpowered.com/blog/tfidf-algorithm-content-seo/\">What Is The TF*IDF Algorithm?</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$TF{\\text -}IDF$ ($TF$ — term frequency, $IDF$ — inverse document frequency)  — это способ векторизации текста, отражающий важность слова в документе, а не только частоту его появления.\n",
    "\n",
    "Частота слов ($TF$) — это мера частоты употребления слова $w$ в документе $d$. $TF$ определяется как отношение появления слова в документе к общему количеству слов в документе.\n",
    "\n",
    "$$TF(w,d) = \\frac{количество\\:вхождений\\:слова\\:w\\:в\\:документе\\:d}{общее\\:количество\\:слов\\:n\\:в\\:документе\\:d}$$\n",
    "\n",
    "Обратная частота документов ($IDF$) —  это мера важности слова. Некоторые слова могут присутствовать наиболее часто, но не имеют большого значения. $IDF$ присваивает вес каждому слову в зависимости от его частоты в корпусе $D$.\n",
    "\n",
    "$$IDF(w,D) = ln(\\frac{общее\\:количество\\:документов\\:N\\:в\\:корпусе\\:D}{количество\\:документов,\\:содержащих\\:слово\\:w})$$\n",
    "\n",
    "$TF{\\text -}IDF$ является произведением $TF$ и $IDF$.\n",
    "$$TF{\\text -}IDF(w,d,D)=TF(w,d)*IDF(w,D)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для векторизации воспользуемся классом `TfidfVectorizer` [🛠️[doc]](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). Применим метод `fit_transform` и посмотрим на результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "corpus_tfidf = tfidf.fit_transform(corpus)\n",
    "corpus_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame(corpus_tfidf.toarray(),\n",
    "                      columns = tfidf.get_feature_names_out(),\n",
    "                      index=corpus)\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно ограничить размер словаря и включать только слова, которые встречаются не реже N раз(`min_df`). Также можно убрать слова, которые встречаются слишком часто и являются стоп-словами в пределах данного корпуса (`max_df`). Оба параметра могут быть выражены целым числом либо числом с плавающей точкой в диапазоне [0.0, 1.0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf1 = TfidfVectorizer(min_df=2)\n",
    "corpus_tfidf1 = tfidf1.fit_transform(corpus)\n",
    "corpus_tfidf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf1_df = pd.DataFrame(corpus_tfidf1.toarray(),\n",
    "                      columns = tfidf1.get_feature_names_out(),\n",
    "                      index=corpus)\n",
    "tfidf1_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разведочный анализ данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Разведочный анализ данных/ Exploratory data analysis (EDA)** — анализ основных свойств данных, нахождение в них общих закономерностей, зачастую с использованием инструментов визуализации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L01/eda.png\" width=\"450\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://chernobrovov.ru/articles/kak-naglyadno-pokazat-data-science-vizualizaciya-bolshih-dannyh.html\">Как наглядно показать Data Science</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примеры:\n",
    "\n",
    "* [[git] 🐾 Три блокнота с подробным анализом реального датасета](https://github.com/AleksandrIvchenko/machine-learning-project-walkthrough)\n",
    "* [[blog] ✏️ Как избежать «подводных камней» машинного обучения: руководство для академических исследователей](https://habr.com/ru/post/664102/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем использовать наивный байесовский классификатор для фильтрации спама.  В рамках данной задачи имеются:\n",
    "- Датасет из текстов сообщений с некоторым фиксированным словарём возможных слов.\n",
    "- Два класса сообщений: спам и нормальное.\n",
    "- Признаковое описание для каждого сообщения, характеризующее количество вхождений каждого из слов словаря в текст сообщения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого класса $c$ требуется найти $P(c|d)$ — вероятность класса $c$ для документа $d$. Она рассчитывается по формуле Байеса:\n",
    "\n",
    "$$P(c|d) = \\frac{P(d|c) P(c)} {P(d)}$$\n",
    "\n",
    "$P(d)$: вероятность документа $d$ одинакова для всех классов, поэтому её можно опустить.\n",
    "\n",
    "Получим:\n",
    "\n",
    "$$P(c|d) = P(d|c) P(c)$$\n",
    "\n",
    "$P(c)$: вероятность класса $c$ — это доля документов класса $c$ среди всех документов.\n",
    "\n",
    "$P(d|c)$: вероятность документа $d$ для класса $c$ зависит от слов $x_1, x_2, ..., x_n$, входящих в документ:\n",
    "\n",
    "$$P(d|c) = P(x_1,x_2,...,x_n|c) = P(x_1|c)P(x_2|c)...P(x_n|c) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть у нас есть датасет, где все письма состоят из слов $x_1, x_2, x_3, x_4$: **‘добрый’, ‘день’, ‘гости’, ‘деньги’**. Мы уже посчитали, сколько раз каждое слово встречается в каждом классе.\n",
    "\n",
    "Мы можем посчитать $P(x_1|c)$ — вероятность встретить слово **‘добрый’** в нормальном письме: берем количество нормальных писем со словом **‘добрый’** и делим на количество всех нормальных писем. Аналогично для других слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-content/L01/out/naive_bayes_spam_1.png\" width=\"700\" ></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Делаем то же самое для слов из спама."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-content/L01/out/naive_bayes_spam_2.png\" width=\"700\" ></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем $P(c)$ — вероятность того, что письмо не является спамом. Для этого количество нормальных писем делим на общее количество писем. Аналогично для спама."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-content/L01/out/naive_bayes_3.png\" width=\"700\" ></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем вычислить $P(d|c)$ для письма **‘добрый день’**. Для этого перемножим $P(x_1|d)$ — вероятность нормального письма со словом **‘добрый’** и $P(x_2|d)$ — вероятность нормального письма со словом **‘день’**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-content/L01/out/naive_bayes_spam_4.png\" width=\"700\" ></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось получить $P(c|d)$ — вероятность нормального письма с фразой **‘добрый день’** в «наивном» предположении. Нужно умножить $P(d|c)$ — вероятность нормального письма **‘добрый день’** на $P(c)$  — вероятность того, что письмо не является спамом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-content/L01/out/naive_bayes_spam_5.png\" width=\"700\" ></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение vs. применение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как будут выглядеть данные во время **инференса** модели?\n",
    "\n",
    "Не окажется ли, что при в обучающих данных модели содержатся примеры спама из электронной почты, а в тестовых — из смс-сообщений?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <table >\n",
    "     <tr>\n",
    "       <td>\n",
    "       \n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L01/email.png\" width=\"550\"></center>\n",
    "\n",
    "<em>Источник: электронная почта</em>\n",
    "\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L01/sms.png\" width=\"350\"></center>\n",
    "\n",
    "<em>Источник: смс-сообщения</em>\n",
    "\n",
    "</td>\n",
    "     </tr>\n",
    "    </table>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Что делать?**\n",
    "\n",
    "* Добавить целевые данные\n",
    "* Попробовать оценить смещение признаков данных и добавить это смещение к данным при обучении\n",
    "* Костыли и велосипеды\n",
    "\n",
    "Подробнее с этим вы познакомитесь в ходе курса.\n",
    "\n",
    "[[blog] ✏️ Обсуждение проблемы](https://stats.stackexchange.com/questions/362906/co-variate-shift-between-train-and-test-data-set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка качества классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как оценивать качество алгоритма? Допустим, мы хотим внести изменения в алгоритм. Как узнать, сделают ли эти изменения алгоритм лучше?\n",
    "\n",
    "В задачах машинного обучения для оценки качества моделей и сравнения  алгоритмов используются различные метрики. Мы рассмотрим некоторые из них вначале на примере бинарной классификации, а затем узнаем способы их усреднения для задач с несколькими классами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Матрица ошибок"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для подсчета метрик качества классификации используется матрица ошибок.\n",
    "\n",
    "Есть алгоритм, предсказывающий принадлежность каждого объекта одному из классов.\n",
    "- $\\hat y$ — предсказанный алгоритмом класс объекта\n",
    "- $y$ — истинный класс объекта\n",
    "\n",
    "Два класса делятся на положительный (1) и отрицательный (0 или –1).\n",
    "- Объекты, которые алгоритм относит к положительному классу, – положительные (Positive).\n",
    "- Те, которые на самом деле принадлежат к этому классу, – истинно положительные (True Positive).\n",
    "- Остальные – ложно положительные (False Positive).\n",
    "\n",
    "Аналогичная терминология для отрицательного (Negative) класса.\n",
    "\n",
    "Таким образом, ошибки классификации бывают двух видов: False Negative (FN) и False Positive (FP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{array}{|c|c|} \\hline\n",
    "& \\hat{y}=0 & \\hat{y}=1 \\\\ \\hline\n",
    "y=0 & \\text{True Negative (TN)} & \\text{False Positive (FP)}  \\\\ \\hline\n",
    "y=1 & \\text{False Negative (FN)} & \\text{True Positive (TP)} \\\\ \\hline\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждое письмо изначально относится к одному из классов:\n",
    "- является спамом → положительный класс (positive, $y=1$).\n",
    "- не является спамом → отрицательный класс (negative, $y=0$).\n",
    "\n",
    "Модель может «предсказать, является письмо спамом (true, $\\hat y = 1$) или нет (false, $\\hat y = 0$).\n",
    "\n",
    "Пусть какой-то набор слов характерен для спамового письма.\n",
    "- Модель верно определила и поставила положительный класс → истинно положительный исход (true positive).\n",
    "- Модель ставит отрицательную метку класса → ложно отрицательный исход (false negative): модель «сказала», что письмо является нормальным, но на самом деле оно является спамом.\n",
    "\n",
    "В случае, если письмо является нормальным, исходы модели остаются аналогичными.\n",
    "- Модель относит письмо к положительному классу → ложно положительный исход (false positive): модель «сказала», что письмо относится к спаму, но на самом деле нет.\n",
    "- Модель определят письмо как отрицательный класс → истинно отрицательный исход (true negative)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-content/L01/out/1_2_errors.png\" width=\"400\" ></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy — доля правильных ответов алгоритма среди всех ответов (непоказательна в задачах с неравными классами):\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Допустим, мы хотим оценить работу спам-фильтра почты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-content/L01/out/imbalanced_data.png\" width=\"800\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При этом модель 2 совершенно не обладает никакой предсказательной силой, так как изначально мы хотели определять письма со спамом. Преодолеть это поможет переход с общей для всех классов метрики к отдельным показателям качества классов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае дисбаланса классов есть специальный аналог – метрика balanced accuracy.\n",
    "\n",
    "$$\\text{Balanced Accuracy} = \\dfrac{1}{2} (\\dfrac{TP}{TP + FN} + \\dfrac{TN}{TN + FP})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-content/L01/out/balanced_accuracy.png\" width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Точность, полнота, F-мера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Точность — доля объектов, названных классификатором положительными и при этом действительно являющимися положительными.\n",
    "\n",
    "$$\\text{Precision} = \\frac{TP}{TP + FP}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важность метрики определяется тем, насколько высока «цена» ложно положительного результата. Например, если стоимость дальнейшей проверки наличия беременности у пациентки высока и мы не можем проверить все ложно положительные результаты, стоит максимизировать данную метрику."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полнота — доля объектов положительного класса, которые нашел алгоритм, из всех объектов положительного класса.\n",
    "\n",
    "$$\\text{Recall} = \\frac{TP}{TP + FN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо уделить особое внимание этой оценке, когда в поставленной задаче ошибка нераспознания положительного класса высока. Например, если у рассматриваемой группы пациенток беременность может протекать особенно тяжело."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-content/L01/out/precision-recall.png\" width=\"650\" ></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В реальной жизни максимальная точность и полнота не достижимы одновременно и приходится искать некий баланс. Хотелось бы иметь некую метрику которая объединяла бы в себе информацию о точности и полноте нашего алгоритма.\n",
    "\n",
    "F-мера — среднее гармоническое точности и полноты:\n",
    "\n",
    "$$\\text{F-score}=2\\frac{\\text{Precision}×\\text{Recall}}{\\text{Precision}+\\text{Recall}}$$\n",
    "​\n",
    "Данная формула придает одинаковый вес точности и полноте, поэтому F-мера будет падать одинаково при уменьшении и точности и полноты. Можно рассчитать F-меру придав различный вес точности и полноте, если отдать приоритет одной из этих метрик при разработке алгоритма.\n",
    "\n",
    "$$\\text{F-score}=(β^2+1)\\frac{\\text{Precision}×\\text{Recall}}{β^2\\text{Precision}+\\text{Recall}}$$\n",
    "​\n",
    "$β$ принимает значения в диапазоне, $0<β<1$ если нужно отдать приоритет точности, а при $β>1$ приоритет отдается полноте. При $β=1$ формула сводится к предыдущей, что дает сбалансированную F-меру (также ее называют F1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инструменты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ходе курса мы будем опираться на существующие методы, реализованные в следующих основных библиотеках:\n",
    "\n",
    "* [[doc] 🛠️ NumPy](https://numpy.org/) — поддержка больших многомерных массивов и быстрых математических функций для операций с этими массивами.\n",
    "* [[doc] 🛠️ Scikit-learn](https://scikit-learn.org/stable/) — ML алгоритмы, \"toy\"-датасеты.\n",
    "* [[doc] 🛠️ Pandas](https://pandas.pydata.org/) — удобная работа с табличными данными.\n",
    "* [[doc] 🛠️ **PyTorch**](https://pytorch.org/) — основной фреймворк машинного обучения, который будет использоваться на протяжении всего курса.\n",
    "* [[doc] 🛠️ Matplotlib](https://matplotlib.org/) — основная библиотека для визуализации. Вывод различных графиков.\n",
    "* [[doc] 🛠️ Seaborn](https://seaborn.pydata.org/) — удобная библиотека для визуализации статистик. Прямо из коробки вызываются и гистограммы, и тепловые карты, и визуализация статистик по датасету, и многое другое."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L01/sns.png\" width=\"850\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://www.acte.in/what-is-seaborn-in-python-article/\">What is Seaborn in Python? A Complete Guide For Beginners & REAL-TIME Examples</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пример работы с данными и моделью"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачаем набор данных [[doc] 🛠️ SMS Spam Collection Dataset](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset). Он содержит смс-сообщения на английском языке, размеченные как «спам» (spam) и «не спам» (ham). О том, почему классы называются именно так, можно почитать в статье [[wiki] 📚 Spam (food)](https://simple.wikipedia.org/wiki/Spam_(food)#:~:text=The%20Hormel%20Foods%20Corporation%20once,%E2%80%9CSizzle%20Pork%20And%20Mmmm%E2%80%9D.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/datasets/SMS_Spam_Collection.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sms = pd.read_csv('SMS_Spam_Collection.csv', sep='\\t',\n",
    "                  # texts and labels are separated by tabs\n",
    "                  header=None, names=['label', 'text'])\n",
    "                  # give names to the columns\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим данные на наличие дублирующихся строк."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms.duplicated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалим повторяющиеся данные, сохранив первое вхождение для каждого дубля."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms.drop_duplicates(keep='first',inplace=True)\n",
    "sms.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимые инструменты для предобработки текстов есть в билиотеке [[doc] 🛠️ NLTK](https://www.nltk.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также нам понадобится строка из знаков препинания, которые мы будем удалять."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для примера к каждому токену одного из сообщений применим метод `strip()` и удалим знаки препинания, а затем через пробел соединим токены обратно к строку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sms['text'][4])\n",
    "print(' '.join([ps.stem(token.strip(punctuation).lower())\n",
    "for token in sms['text'][4].split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим лемматизацию токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "print(sms['text'][4])\n",
    "print(' '.join([lemmatizer.lemmatize(token.strip(punctuation).lower())\n",
    "for token in sms['text'][4].split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалим стоп-слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = stopwords.words('english')\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sms['text'][4])\n",
    "print(' '.join([lemmatizer.lemmatize(token.strip(punctuation).lower())\n",
    "for token in sms['text'][4].split()\n",
    "if token.lower() not in STOPWORDS]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объединим все этапы предобработки в функции `text_preprocessing`. Она принимает на вход сырой текст, токенизирует его по словам, каждый токен приводит к нижнему регистру, лемматизирует, удаляет знаки препинания и стоп-слова. На выходе мы должны получить строку из токенов после предобработки, разделенных пробелами.\n",
    "\n",
    "Осуществим предобработку всех текстов в датсете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    STOPWORDS = stopwords.words('english')\n",
    "    tokens = [lemmatizer.lemmatize(token.strip(punctuation).lower())\n",
    "    for token in text.split()\n",
    "    if token.lower() not in STOPWORDS]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "sms['preprocessed'] = sms['text'].apply(lambda x:\n",
    "                                        text_preprocessing(x))\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms.dropna(subset=['preprocessed'], inplace=True)\n",
    "sms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В следующем разделе обучим на наших данных модель для автоматической классификации спама."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разведочный анализ данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, какое количество сообщений каждого класса представлено в датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Распределение классов можно визуализировать. Воспользуемся библиотекой [[doc] 🛠️ Matplotlib](https://matplotlib.org/) для рисования круговой диаграммы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.pie(sms['label'].value_counts(), labels=sms['label'].unique(), autopct='%.1f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим, насколько длинные сообщения встречаются в датасете.\n",
    "\n",
    "Посчитаем количество слов для всех сообщений и запишем результат в датафрейм в качестве нового столбца."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms['word_count'] = sms['preprocessed'].map(lambda x: len(x.split()))\n",
    "sms.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируем результат с помощью гистограммы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sms['word_count'], bins=60)\n",
    "plt.title('Word count in messages')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно отдельно вывести минимальную, максимальную и среднюю длину сообщения, которые встретились в датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Minimum word count: {sms['word_count'].min()}\")\n",
    "print(f\"Maximum word count: {sms['word_count'].max()}\")\n",
    "print(f\"Mean word count: {sms['word_count'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем количество вхождений для каждого слова. Для этого создадим объект класса Counter(), он позволяет быстро посчитать количество появлений элементов в последовательности. Определим количество уникальных слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "word_frequency = Counter(\" \".join(sms['preprocessed']).split())\n",
    "print(word_frequency)\n",
    "print(len(word_frequency))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем в отдельную переменную топ-10 самых частотных слов и выведем результат в виде столбчатой диаграммы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10 = word_frequency.most_common(10)\n",
    "plt.barh(y=[x[0] for x in top_10], width=[x[1] for x in top_10])\n",
    "plt.title('Top 10 most frequently occuring words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение и тестирование модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Продолжим работу с набором данных [[doc] 🛠️ SMS Spam Collection Dataset](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset). Для удобства заменим словесные обозначения классов на числовые."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms['num_label'] = sms['label'].astype('category').cat.codes\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем в отдельные переменные предобработанные тексты `X` и метки классов `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = sms['preprocessed'], sms['num_label']\n",
    "X[:5], y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим данные на обучающую и тестовую выборку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы подавать тексты в модель машинного обучения, необходимо представить каждое предложение в виде набора признаков — вектора. В качестве признаков будем подавать модели слова, встретившиеся в обучающей выборке. Те слова, которые встретятся в тестовой выборке, но отсутствуют в обучающей, не могут быть проинтерпретироованы моделью.\n",
    "\n",
    "Будем использовать модель векторизации «мешок слов». Словарь необходимо собирать на основе обучающей выборки (метод `fit`). При этом преобразование текстов в векторы на основе собранного словаря нужно осуществить для всего датасета (метод `transform`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://i.ibb.co/FWW9w3K/fit-transform.png\" width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осуществим векторизацию обучающей и тестовой выборок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vect = TfidfVectorizer()\n",
    "#vect.fit(X_train)\n",
    "#X_train_vect = vect.transform(X_train)\n",
    "X_train_vect = vect.fit_transform(X_train)\n",
    "X_test_vect = vect.transform(X_test)\n",
    "X_train_vect, X_test_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим модель для классификации — наивный байесовский классификатор. Осуществим обучение модели на обучающей выборке из предложений (`X_train_bow`) и меток (`y_train`) с помощью метода `fit`. Затем используем обученную модель для предсказания меток на основе предложений тестовой выборки (`X_test_bow`) с помощью метода `predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_vect, y_train)\n",
    "y_mnb = mnb.predict(X_test_vect)\n",
    "y_mnb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим качество классификации  с помощью метрики accuracy, а также точности (precision), полноты (recall) и F-меры (f1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "report = pd.DataFrame(\n",
    "    [accuracy_score(y_test, y_mnb),\n",
    "     recall_score(y_test, y_mnb),\n",
    "     precision_score(y_test, y_mnb),\n",
    "     f1_score(y_test, y_mnb)]).round(2)\n",
    "report = report.rename(columns={0: 'Naïve bayes'})\n",
    "report = report.rename(index={0: \"accuracy\", 1: \"recall\", 2: \"precision\", 3: \"f1_score\", 4: \"accuracy\"})\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы получили довольно высокое качество. Но можно ли еще улучшить его? Например, за счет гиперпараметров векторизации?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подбор гиперпараметров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Для автоматического подбора параметров используется модуль `GridSearchCV`[🛠️[doc]](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). Он создает модель для каждой возможной комбинации параметров.\n",
    "\n",
    "Все этапы обработки — векторизацию и классификацию — объединим в `Pipeline`[🛠️[doc]](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html). Пайплайн в машинном обучении — это процесс работы с данными от начала до конца, включая ввод данных, их обработку, применение модели с установленными параметрами и вывод результата.\n",
    "\n",
    "Создаем словарь `parameters`, содержащий диапазон значений каждого параметра. Далее инициализируем объект `grid_search`, передавая ему пайплайн (векторизатор и модель). По умолчанию количество итераций равно 10 (`n_iter`), то есть будут сравниваться 10 разных моделей. Установим количество кросс-валидаций (`cv=5`).\n",
    "\n",
    "Кросс-валидация — перекрестная проверка.\n",
    "\n",
    "1. Фиксируется целое число $k$, меньшее числа примеров в датасете.\n",
    "2. Датасет разбивается на $k$ одинаковых частей.\n",
    "3. Происходит $k$ итераций, в каждой из которых одна часть выступает в роли тестового множества, а объединение остальных — в роли тренировочного.\n",
    "4. Финальный результат модели получается либо усреднением получившихся тестовых результатов, либо измеряется на отложенном тестовом множестве, не участвовавшем в кросс-валидации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-content/L01/out/cross_validation_on_train_data.png\" width=\"600\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим следующие параметры:\n",
    "- `ngram_range`:\n",
    "  - (1, 1) — только униграммы;\n",
    "  - (1, 2) — униграммы и биграммы.\n",
    "- `min_df`\n",
    "  -  0.0001 — исключаем токены, которые встретились в меньше чем 0,01% документов;\n",
    "  -  0.001 — исключаем токены, которые встретились в меньше чем 0,1% документов.\n",
    "- `max_df`:\n",
    "  - 0.7 — исключаем токены, которые встретились в больше чем 70% документов;\n",
    "  - 1.0 — исключаем токены, которые встретились в больше чем 100% документов.\n",
    "\n",
    "В качестве метрики для сравнения (`scoring`) будем использовать F1-меру. Выведем лучшие значения параметров (`best_params_`) и лучшее значение метрики (`best_score_`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipeline = Pipeline([\n",
    "           ('vect', TfidfVectorizer()),\n",
    "           ('clf', MultinomialNB()),\n",
    "])\n",
    "parameters = {\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "    'vect__min_df': (0.0001, 0.001),\n",
    "    'vect__max_df': (0.7, 1.0)\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, cv=5, scoring='f1')\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_, grid_search.best_score_.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "           ('vect',\n",
    "            TfidfVectorizer(max_df=grid_search.best_params_['vect__max_df'],\n",
    "                            min_df=grid_search.best_params_['vect__min_df'],\n",
    "                            ngram_range=grid_search.best_params_['vect__ngram_range'])),\n",
    "           ('clf', MultinomialNB()),\n",
    "])\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "f1_score(y_test, y_pred).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "За счет автоматического подбора гиперпараметров качество возросло."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
