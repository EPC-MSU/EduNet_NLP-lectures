{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">–ú–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–µ–∫–æ–¥–µ—Ä–∞ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —ç–Ω–∫–æ–¥–µ—Ä–∞ –∏ –¥–µ–∫–æ–¥–µ—Ä–∞.\n",
    "\n",
    "–ù–∞ –æ—Å–Ω–æ–≤–µ —ç–Ω–∫–æ–¥–µ—Ä–∞ –±—ã–ª–∞ —Å–æ–∑–¥–∞–Ω–∞ –º–æ–¥–µ–ª—å BERT –∏ –ø–æ–¥–æ–±–Ω—ã–µ –µ–π (RoBERTa, ALBERT). –û–Ω–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —Å–ª–æ–≤ –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π. –ó–∞—Ç–µ–º –º–æ–¥–µ–ª–∏ –¥–æ–æ–±—É—á–∞—é—Ç—Å—è –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–¥–∞—á–∏, —á—Ç–æ —Ç—Ä–µ–±—É–µ—Ç –Ω–∞–ª–∏—á–∏—è —Ä–∞–∑–º–µ—á–µ–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö.\n",
    "\n",
    "–ù–∞ –æ—Å–Ω–æ–≤–µ —ç–Ω–∫–æ–¥–µ—Ä–∞ –±—ã–ª–∞ —Å–æ–∑–¥–∞–Ω–∞ –º–æ–¥–µ–ª—å GPT ‚Äî Generative Pretrained Transformer. –ú–æ–¥–µ–ª—å GPT –±—ã–ª–∞ –æ–±—É—á–µ–Ω–∞ –Ω–∞ –∑–∞–¥–∞—á–µ —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è, —Ç–æ –µ—Å—Ç—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏) —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–ª–æ–≤–∞ –≤ —Ç–µ–∫—Å—Ç–µ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ß–µ—Ç—ã—Ä–µ –ø–æ–∫–æ–ª–µ–Ω–∏—è GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é GPT –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Å–ª–µ–¥—É—é—â–µ–µ:\n",
    "\n",
    "1. –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç—Å—è –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —á–∏—Å–µ–ª (—Ç–æ–∫–µ–Ω–æ–≤).\n",
    "2. –°–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤ –ø—Ä–æ—Ö–æ–¥–∏—Ç —á–µ—Ä–µ–∑ Embedding layer (–ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π) –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç—Å—è –≤ —Å–ø–∏—Å–æ–∫ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.\n",
    "3. –ö –∫–∞–∂–¥–æ–º—É —ç–º–±–µ–¥–¥–∏–Ω–≥—É –ø—Ä–∏–±–∞–≤–ª—è–µ—Ç—Å—è **positional embedding**.\n",
    "4. –°–ø–∏—Å–æ–∫ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –ø—Ä–æ—Ö–æ–¥–∏—Ç —á–µ—Ä–µ–∑ –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö –±–ª–æ–∫–æ–≤ (Transformer Decoder Block).\n",
    "5. –ü–æ—Å–ª–µ —Ç–æ–≥–æ, –∫–∞–∫ —Å–ø–∏—Å–æ–∫ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –ø—Ä–æ–π–¥—ë—Ç —á–µ—Ä–µ–∑ –ø–æ—Å–ª–µ–¥–Ω–∏–π –±–ª–æ–∫, —ç–º–±–µ–¥–¥–∏–Ω–≥, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –ø–æ—Å–ª–µ–¥–Ω–µ–º—É —Ç–æ–∫–µ–Ω—É, –º–∞—Ç—Ä–∏—á–Ω–æ —É–º–Ω–æ–∂–∞–µ—Ç—Å—è –Ω–∞ –≤—Å—ë —Ç–æ—Ç –∂–µ –≤—Ö–æ–¥–Ω–æ–π, –Ω–æ —É–∂–µ —Ç—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π Embedding Layer, –∏ –ø–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è SoftMax –ø–æ–ª—É—á–∞–µ—Ç—Å—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞.\n",
    "6. –ò–∑ —ç—Ç–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è —Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å –ø–æ–º–æ—â—å—é argmax).\n",
    "7. –ü–æ–ª—É—á–µ–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –∫ –≤—Ö–æ–¥–Ω–æ–º—É —Å–ø–∏—Å–∫—É —Ç–æ–∫–µ–Ω–æ–≤, —à–∞–≥–∏ 1‚Äì6 –ø–æ–≤—Ç–æ—Ä—è—é—Ç—Å—è."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L07/gpt3.gif\" width=\"800\"></center>\n",
    "\n",
    "<center><em>–ò—Å—Ç–æ—á–Ω–∏–∫: <a href=\"https://jalammar.github.io/how-gpt3-works-visualizations-animations/\">How GPT3 Works ‚Äî Visualizations and Animations</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[paper] üéì Improving Language Understanding by Generative Pre-Training (2018)](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)\n",
    "\n",
    "- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: 117 –º–∏–ª–ª–∏–æ–Ω–æ–≤.\n",
    "- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤: 12.\n",
    "- –†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö: 5 –ì–ë.\n",
    "- –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: 512 —Ç–æ–∫–µ–Ω–æ–≤.\n",
    "\n",
    "–ü–µ—Ä–≤–∞—è –≤–µ—Ä—Å–∏—è —ç—Ç–æ–π –º–æ–¥–µ–ª–∏ –±—ã–ª–∞ –æ–±—É—á–µ–Ω–∞ –Ω–∞ 7000 –∫–Ω–∏–≥.\n",
    "\n",
    "–ö–∞–∫ —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –æ–Ω–∞ —Ä–∞–±–æ—Ç–∞–ª–∞ –Ω–µ –æ—á–µ–Ω—å —Ö–æ—Ä–æ—à–æ, –Ω–æ –ø—Ä–∏ –¥–æ–æ–±—É—á–µ–Ω–∏–∏ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –ø–æ–∫–∞–∑—ã–≤–∞–ª–∞ –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ.\n",
    "\n",
    "–û–¥–Ω–∞–∫–æ SOTA —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã GPT-1 –¥–µ—Ä–∂–∞–ª–∏—Å—å –Ω–µ–¥–æ–ª–≥–æ, —Ç–∞–∫ –∫–∞–∫ –ø–æ—è–≤–∏–ª—Å—è BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L07/gpt1.jpg\" width=\"800\"></center>\n",
    "\n",
    "<center><em>–ò—Å—Ç–æ—á–Ω–∏–∫: <a href=\"https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf/\">Improving Language Understanding by Generative Pre-Training</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[paper] üéì Language Models are Unsupervised Multitask Learners (2019)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "\n",
    "- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: 1.5 –º–∏–ª–ª–∏–∞—Ä–¥–∞.\n",
    "- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤: 48.\n",
    "- –†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö: 40 –ì–ë.\n",
    "- –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: 1024 —Ç–æ–∫–µ–Ω–æ–≤.\n",
    "\n",
    "\n",
    "–ù–æ–≤–∞—è GPT-2 –Ω–µ —Å–æ–¥–µ—Ä–∂–∞–ª–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã—Ö –Ω–æ–≤–æ–≤–≤–µ–¥–µ–Ω–∏–π, –æ—Å–Ω–æ–≤–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –¥–æ—Å—Ç–∏–≥–∞–ª–∏—Å—å –∑–∞ —Å—á–µ—Ç —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö (–∫ –∫–Ω–∏–≥–∞–º –¥–æ–±–∞–≤–∏–ª–∏ 8 –º–∏–ª–ª–∏–æ–Ω–æ–≤ —Å–∞–π—Ç–æ–≤).\n",
    "\n",
    "GPT-2 –Ω–∞—É—á–∏–ª–∞—Å—å –ø–∏—Å–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ —Å–≤—è–∑–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L07/gpt2.png\" width=\"800\"></center>\n",
    "\n",
    "<center><em>–ò—Å—Ç–æ—á–Ω–∏–∫: <a href=\"https://en.wikipedia.org/wiki/GPT-2\">GPT-2</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–∞–∫–∂–µ GPT-2 —É–∂–µ –º–æ–≥–ª–∞ –±–µ–∑ —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≤—ã–ø–æ–ª–Ω—è—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∑–∞–¥–∞—á–∏, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ –±—ã–ª–æ —è–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ù–∞–ø—Ä–∏–º–µ—Ä, –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–æ–ø–∏—Å–∞—Ç—å `TL;DR` –ø–æ—Å–ª–µ —Ç–µ–∫—Å—Ç–∞."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i.postimg.cc/nL19wfYL/tldr.jpg\" width=\"850\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[paper] üéì Language Models are Few-Shot Learners (2020)](https://arxiv.org/pdf/2005.14165.pdf)\n",
    "\n",
    "- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: 175 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤.\n",
    "- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤: 96.\n",
    "- –†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö: 45 –¢–ë (—Ç.–µ. 45 000 –ì–ë).\n",
    "- –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: 2048 —Ç–æ–∫–µ–Ω–æ–≤.\n",
    "\n",
    "GPT-3 –æ–±—É—á–µ–Ω–∞ –Ω–∞ –µ—â—ë –±–æ–ª—å—à–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –¥–∞–Ω–Ω—ã—Ö ‚Äî –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞, –≤—Å—è –∞–Ω–≥–ª–∏–π—Å–∫–∞—è –í–∏–∫–∏–ø–µ–¥–∏—è —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–æ–ª—å–∫–æ 0,6% –∏–∑ –Ω–∏—Ö.\n",
    "\n",
    "–ú–æ–¥–µ–ª—å –Ω–∞—É—á–∏–ª–∞—Å—å —Ä–µ—à–∞—Ç—å –º–Ω–æ–≥–æ NLP-–∑–∞–¥–∞—á –±–µ–∑ –¥–æ–æ–±—É—á–µ–Ω–∏—è."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <table >\n",
    "     <tr>\n",
    "       <td>\n",
    "       \n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L07/gpt3_gec.jpg\" width=\"440\"></center>\n",
    "\n",
    "<em>–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫</em>\n",
    "\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L07/gpt3_sa.jpg\" width=\"450\"></center>\n",
    "\n",
    "<em>–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏</em>\n",
    "\n",
    "\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L07/gpt3_mt.jpg\" width=\"500\"></center>\n",
    "\n",
    "<em>–ú–∞—à–∏–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥</em>\n",
    "\n",
    "\n",
    "</td>\n",
    "     </tr>\n",
    "    </table>\n",
    "    </div>\n",
    "\n",
    "<center><em>–ò—Å—Ç–æ—á–Ω–∏–∫: <a href=\"https://beta.openai.com/examples\">GPT-3 beta</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-3 —Ç–∞–∫–∂–µ —Å–ø–æ—Å–æ–±–Ω–∞ –ø–∏—Å–∞—Ç—å –∏ –ø–æ–Ω–∏–º–∞—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º–Ω—ã–π –∫–æ–¥."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <table >\n",
    "     <tr>\n",
    "       <td>\n",
    "       \n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L07/gpt3_code_promt.jpg\" width=\"300\"></center>\n",
    "\n",
    "<em>–ó–∞–ø—Ä–æ—Å</em>\n",
    "\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L07/gpt3_code_response.jpg\" width=\"400\"></center>\n",
    "\n",
    "<em>–ü—Ä–∏–º–µ—Ä –æ—Ç–≤–µ—Ç–∞</em>\n",
    "\n",
    "\n",
    "</td>\n",
    "     </tr>\n",
    "    </table>\n",
    "    </div>\n",
    "\n",
    "<center><em>–ò—Å—Ç–æ—á–Ω–∏–∫: <a href=\"https://beta.openai.com/examples\">GPT-3 beta</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°—Ç–∞—Ç—å—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç.\n",
    "\n",
    "- –ù–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø—Ä–æ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏.\n",
    "- –†–∞–±–æ—Ç–∞–µ—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –¥–æ 8192 —Ç–æ–∫–µ–Ω–æ–≤. –ï—Å—Ç—å –≤–µ—Ä—Å–∏—è, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∞—è 32 768 —Ç–æ–∫–µ–Ω–æ–≤ (50 —Å—Ç—Ä–∞–Ω–∏—Ü).\n",
    "\n",
    "–ü–æ–º–∏–º–æ —Ç–µ–∫—Å—Ç–æ–≤, —É–º–µ–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏, —è–≤–ª—è–µ—Ç—Å—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L07/gpt4.jpg\" width=\"550\"></center>\n",
    "\n",
    "<center><em>–ò—Å—Ç–æ—á–Ω–∏–∫: <a href=\"https://thecymes.com/article/the-game-changing-features-of-openais-chatgpt-4\">The game-changing features of OpenAI's GPT 4</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —Ä–∞–±–æ—Ç—É –º–æ–¥–µ–ª–∏ ruGPT-3 –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞. –û–Ω–∞ –¥–æ—Å—Ç—É–ø–Ω–∞ –≤ —Ç—Ä–µ—Ö –≤–∞—Ä–∏–∞–Ω—Ç–∞—Ö:\n",
    "- small: [ai-forever/rugpt3small_based_on_gpt2 üõ†Ô∏è[doc]](https://huggingface.co/ai-forever/rugpt3small_based_on_gpt2)\n",
    "- medium: [ai-forever/rugpt3medium_based_on_gpt2 üõ†Ô∏è[doc]](https://huggingface.co/ai-forever/rugpt3medium_based_on_gpt2)\n",
    "- large: [ai-forever/rugpt3large_based_on_gpt2 üõ†Ô∏è[doc]](https://huggingface.co/ai-forever/rugpt3large_based_on_gpt2)\n",
    "\n",
    "–î–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è –∫–ª–∞—Å—Å `AutoTokenizer` [üõ†Ô∏è[doc]](https://huggingface.co/docs/transformers/v4.45.2/en/model_doc/auto#transformers.AutoTokenizer), –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ ‚Äî –∫–ª–∞—Å—Å `AutoModelForCausalLM` [üõ†Ô∏è[doc]](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM).\n",
    "\n",
    "–í—ã–±–µ—Ä–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—É—é –º–æ–¥–µ–ª—å. API –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π, –¥–ª—è –ø–æ–¥–º–µ–Ω—ã –º–æ–¥–µ–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ `model_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"ai-forever/rugpt3large_based_on_gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ –º—ã —Ö–æ—Ç–∏–º –ø—Ä–∏ –ø–æ–º–æ—â–∏ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å: **¬´–°–∫–æ–ª—å–∫–æ –±—É–¥–µ—Ç 2+2?¬ª**, —Ç–æ –º–æ–∂–µ–º –ø–æ–¥–∞—Ç—å –Ω–∞ –≤—Ö–æ–¥ –º–æ–¥–µ–ª–∏ —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç:\\\n",
    "`¬´–í–æ–ø—Ä–æ—Å: –°–∫–æ–ª—å–∫–æ –±—É–¥–µ—Ç 2+2? –û—Ç–≤–µ—Ç: ‚Ä¶ ¬ª`\\\n",
    "–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ–º —Ç–∞–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –±—É–¥–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å, –ø–æ—ç—Ç–æ–º—É –º–æ–¥–µ–ª—å –¥–æ–ø–∏—à–µ—Ç `¬´4¬ª`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"–í–æ–ø—Ä–æ—Å: '–°–∫–æ–ª—å–∫–æ –±—É–¥–µ—Ç 2+2?'\\n–û—Ç–≤–µ—Ç:\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "out = model.generate(input_ids, do_sample=False, max_length=20, pad_token_id=20)\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–†–∞—Å—Å–º–æ—Ç—Ä–∏–º –ø—Ä–∏–º–µ—Ä —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏. –ö–∞–∫ –∏ BERT-like –º–æ–¥–µ–ª–∏, GPT –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –Ω–∞ –ø–æ–¥—Å–ª–æ–≤–∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"–¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–π –º–µ–Ω—è\" # raw text\n",
    "tokens = tokenizer.encode(text, add_special_tokens=False) # applying ruGPT-3 tokenizer\n",
    "decoded_tokens = [tokenizer.decode([token]) for token in tokens] # converting ids to tokens\n",
    "\n",
    "print(\"text:\", text)\n",
    "print(\"tokens: \", tokens)\n",
    "print(\"decoded tokens: \", decoded_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–Ø–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –≤—ã–¥–∞—ë—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞, —ç—Ç—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –º–æ–∂–Ω–æ –ø–æ-—Ä–∞–∑–Ω–æ–º—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞.\n",
    "\n",
    "–î–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏ –ø—Ä–∏–º–µ–Ω–∏–º –≤—Å–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è —Å–ª–µ–¥—É—é—â–µ–≥–æ: '–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: \"–Ø–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å\" - —ç—Ç–æ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "text = '–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: \"–Ø–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å\" ‚Äî —ç—Ç–æ'\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ñ–∞–¥–Ω—ã–π –ø–æ–∏—Å–∫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–∞–º—ã–π –ø—Ä–æ—Å—Ç–æ–π —Å–ø–æ—Å–æ–± ‚Äî –∂–∞–¥–Ω—ã–π –ø–æ–∏—Å–∫ (greedy search): –∫–∞–∂–¥—ã–π —Ä–∞–∑ –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è —Ç–æ–∫–µ–Ω, —É –∫–æ—Ç–æ—Ä–æ–≥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å.\n",
    "\n",
    "–ü—Ä–∏ —Ç–∞–∫–æ–º —Å–ø–æ—Å–æ–±–µ –º—ã –Ω–µ –ø–æ–ª—É—á–∏–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ –∑–∞–ø—Ä–æ—Å, –∏, —á—Ç–æ –µ—â—ë —Ö—É–∂–µ, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –º–æ–∂–µ—Ç –∑–∞—Å—Ç—Ä–µ–≤–∞—Ç—å –≤ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–∏–Ω–∏–º—É–º–∞—Ö –∏ –≤—ã–¥–∞–≤–∞—Ç—å –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã, –Ω–∞–ø—Ä–∏–º–µ—Ä `the the the the ...`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L07/greedy_seacrh.png\" width=\"500\"></center>\n",
    "\n",
    "<center><em>–ò—Å—Ç–æ—á–Ω–∏–∫: <a href=\"https://habr.com/ru/articles/589663/\">–ö—Ä–∞—Ç–∫–∏–π —ç–∫—Å–∫—É—Ä—Å –≤ ruGPT-3</a></em></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of greedy search\n",
    "out = model.generate(input_ids,\n",
    "                     do_sample=False,\n",
    "                     max_length=30)\n",
    "\n",
    "# Decoding\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –õ—É—á–µ–≤–æ–π –ø–æ–∏—Å–∫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "–ß—É—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–π –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Å–ø–æ—Å–æ–± ‚Äî —ç—Ç–æ –ª—É—á–µ–≤–æ–π –ø–æ–∏—Å–∫ (beam search). –ù–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è –Ω–µ —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —Å–∞–º—ã–π –≤–µ—Ä–æ—è—Ç–Ω—ã–π —Ç–æ–∫–µ–Ω, –∞ —Å—Ä–∞–∑—É –Ω–µ—Å–∫–æ–ª—å–∫–æ (–∑–∞ –∏—Ö –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—á–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä `num_beams`). –î–∞–ª—å—à–µ –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç—Å—è –ø–æ–∏—Å–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤. –ü—É—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–∞–∑–≤–µ—Ç–≤–ª—è—é—Ç—Å—è, —á—Ç–æ –¥–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.\n",
    "\n",
    "–û–±—ã—á–Ω–æ —ç—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –≤—ã—Å–æ–∫–æ–π —Å–≤—è–∑–Ω–æ—Å—Ç–∏ (–∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏), –Ω–æ –ø—Ä–∏ —ç—Ç–æ–º –∫ —Å—É—Ö–æ—Å—Ç–∏ –∏ —Å–∫—É—á–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞. –¢–∞–∫–∂–µ —ç—Ç–æ –Ω–µ —Ä–µ—à–∞–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—Ä–æ–±–ª–µ–º—É —Å –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è–º–∏ –∫—É—Å–æ—á–∫–æ–≤ —Ç–µ–∫—Å—Ç–∞."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L07/beam_search.png\" width=\"500\"></center>\n",
    "\n",
    "<center><em>–ò—Å—Ç–æ—á–Ω–∏–∫: <a href=\"https://habr.com/ru/articles/589663/\">–ö—Ä–∞—Ç–∫–∏–π —ç–∫—Å–∫—É—Ä—Å –≤ ruGPT-3</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–∞–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–±–ª–∞–¥–∞–µ—Ç —Ö–æ—Ä–æ—à–µ–π –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å—é (—Å–≤—è–∑–Ω–æ—Å—Ç—å—é), –Ω–æ –æ–±—ã—á–Ω–æ —É –Ω–∏—Ö –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç \"—á–µ–ª–æ–≤–µ—á–Ω–æ—Å—Ç–∏\", –æ–Ω–∏ –∫–∞–∂—É—Ç—Å—è —Å—É—Ö–∏–º–∏ –∏ —Å–∫—É—á–Ω—ã–º–∏. –¢–∞–∫–∂–µ —ç—Ç–æ –Ω–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of beam search\n",
    "out = model.generate(input_ids,\n",
    "                     do_sample=False,\n",
    "                     num_beams=5,\n",
    "                     max_length=40)\n",
    "\n",
    "# Decoding\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú–æ–∂–Ω–æ –≤—ã–≤–µ—Å—Ç–∏ –Ω–µ —Ç–æ–ª—å–∫–æ –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –Ω–æ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –¥—Ä—É–≥–∏–º –ª—É—á–∞–º."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set return_num_sequences > 1\n",
    "out = model.generate(input_ids,\n",
    "                     do_sample=False,\n",
    "                     num_beams=5,\n",
    "                     max_length=40,\n",
    "                     num_return_sequences=5)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(out):\n",
    "  print(\"{}: {}\\n\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–æ–≤ –º–æ–∂–Ω–æ —É–º–µ–Ω—å—à–∏—Ç—å, –µ—Å–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–∞–ø—Ä–µ—Ç –Ω–∞ –ø–æ–≤—Ç–æ—Ä *n*-–≥—Ä–∞–º–º."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can prohibit repeating bigrams\n",
    "out = model.generate(input_ids,\n",
    "                     do_sample=False,\n",
    "                     num_beams=5,\n",
    "                     max_length=40,\n",
    "                     num_return_sequences=5,\n",
    "                     no_repeat_ngram_size=2)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(out):\n",
    "  print(\"{}: {}\\n\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –°—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–æ–π"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ß—Ç–æ–±—ã –¥–æ–±–∞–≤–∏—Ç—å —Ç–µ–∫—Å—Ç—É –Ω–µ–ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ—Å—Ç–∏ –∏ —á–µ–ª–æ–≤–µ—á–Ω–æ—Å—Ç–∏, –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ–µ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–æ–π. –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –Ω–µ —Å–∞–º—ã–π –≤–µ—Ä–æ—è—Ç–Ω—ã–π —Ç–æ–∫–µ–Ω, –∞ —Å–ª—É—á–∞–π–Ω—ã–π, —Å —É—á—ë—Ç–æ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π.\n",
    "\n",
    "–ü–∞—Ä–∞–º–µ—Ç—Ä —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å —Å—Ç–µ–ø–µ–Ω—å —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏. –ü—Ä–∏ –Ω—É–ª–µ–≤–æ–π —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–µ –º–µ—Ç–æ–¥ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∂–∞–¥–Ω—ã–º —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ–º, –ø—Ä–∏  –±–æ–ª—å—à–æ–π —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–µ —Ç–æ–∫–µ–Ω—ã –±—É–¥—É—Ç –≤—ã–±–∏—Ä–∞—Ç—å—Å—è –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–ª—É—á–∞–π–Ω–æ. –û–±—ã—á–Ω–æ —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ `0.8‚Äì2.0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–§–æ—Ä–º—É–ª–∞ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–∞ –Ω–∞ —Ñ–æ—Ä–º—É–ª—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ë–æ–ª—å—Ü–º–∞–Ω–∞: —á–µ–º –≤—ã—à–µ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ —Å–∏—Å—Ç–µ–º—ã, —Ç–µ–º –±–æ–ª—å—à–µ \"—Ä–∞–∑–º–∞–∑—ã–≤–∞–µ—Ç—Å—è\" —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –µ—ë –≤–æ–∑–º–æ–∂–Ω—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π, –æ—Ç—Å—é–¥–∞ —Å–ª–æ–≤–æ \"—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞\".\n",
    "\n",
    "$$\\large p=\\text{softmax}(\\log(p)/t)$$\n",
    "\n",
    "–°—Ç–æ–∏—Ç –æ—Ç–º–µ—Ç–∏—Ç—å, —á—Ç–æ —Å–ª—É—á–∞–π–Ω–∞—è –ø—Ä–∏—Ä–æ–¥–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±—É–¥–µ—Ç –∏–Ω–æ–≥–¥–∞ –ø—Ä–∏–≤–æ–¥–∏—Ç—å –∫ –ø–æ–ª–Ω–æ—Å—Ç—å—é –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling eith temperature\n",
    "out = model.generate(input_ids, do_sample=True, temperature=1.3, max_length=30)\n",
    "\n",
    "# Decoding\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –°—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ top-k –∏ top-p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–ª—è –∑–∞–ø—Ä–µ—Ç–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–∞–ª–æ–≤–µ—Ä–æ—è—Ç–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤–≤–æ–¥—è—Ç `top-k` –∏–ª–∏ `top-p` –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è. –í —ç—Ç–æ–º —Å–ª—É—á–∞–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–æ–∂–µ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Å–ª—É—á–∞–π–Ω—ã–º –æ–±—Ä–∞–∑–æ–º, –Ω–æ –∑–∞—Ä–∞–Ω–µ–µ –æ—Ç—Å–µ–∫–∞—é—Ç—Å—è –≤—Å–µ –º–∞–ª–æ–≤–µ—Ä–æ—è—Ç–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í —Å–ª—É—á–∞–µ `top-k` –∑–∞–Ω—É–ª—è—é—Ç—Å—è –≤—Å–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∫—Ä–æ–º–µ `k` —Å–∞–º—ã—Ö –≤–µ—Ä–æ—è—Ç–Ω—ã—Ö.\n",
    "\n",
    "–£—Å—Ç–∞–Ω–æ–≤–∏–≤ $k=6$, –º—ã –±—É–¥–µ–º –≤—ã–±–∏—Ä–∞—Ç—å —Å–ª–µ–¥—É—é—â–µ–µ —Å–ª–æ–≤–æ —Ç–æ–ª—å–∫–æ –∏–∑ 6 —Å–ª–æ–≤ —Å –Ω–∞–∏–±–æ–ª—å—à–µ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é:\n",
    "- *nice, dog, car, woman, guy, man* –Ω–∞ 1-–º —à–∞–≥–µ\n",
    "- *drives, is, turns, stops, down, a* –Ω–∞ 2-–º —à–∞–≥–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L07/top-k.png\" width=\"800\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í —Å–ª—É—á–∞–µ `top-p` –æ—Å—Ç–∞–µ—Ç—Å—è –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä —Ç–æ–∫–µ–Ω–æ–≤, —á—Ç–æ–±—ã —Å—É–º–º–∞ –∏—Ö –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –±—ã–ª–∞ –Ω–µ –±–æ–ª—å—à–µ `p`. –°–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤, –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö –º—ã –≤—ã–±–∏—Ä–∞–µ–º, –º–æ–∂–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –º–µ–Ω—è—Ç—å—Å—è (—É–≤–µ–ª–∏—á–∏–≤–∞—Ç—å—Å—è –∏ —É–º–µ–Ω—å—à–∞—Ç—å—Å—è).\n",
    "\n",
    "–£—Å—Ç–∞–Ω–æ–≤–∏–º $p=0.92$ –∏ –±—É–¥–µ–º –≤—ã–±–∏—Ä–∞—Ç—å –∏–∑:\n",
    "- 9 —Å–ª–æ–≤ –Ω–∞ 1-–º —à–∞–≥–µ\n",
    "- 3 —Å–ª–æ–≤ –Ω–∞ 2-–º —à–∞–≥–µ\n",
    "\n",
    "–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ `top-p` –Ω–∞–∑—ã–≤–∞—é—Ç —è–¥–µ—Ä–Ω—ã–º —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ–º (nucleus sampling)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L07/top-p.png\" width=\"700\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random state\n",
    "torch.random.manual_seed(42)\n",
    "\n",
    "# Sampling with top-k and top-p restrictions\n",
    "out = model.generate(input_ids,\n",
    "                     do_sample=True,\n",
    "                     temperature=1.0,\n",
    "                     top_k=20,\n",
    "                     top_p=0.9,\n",
    "                     max_length=30,\n",
    "                    )\n",
    "# Decoding\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ó–∞—Ç—Ä–∞–≤–æ—á–Ω–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í –æ—Ç–ª–∏—á–∏–µ –æ—Ç BERT-like –º–æ–¥–µ–ª–µ–π, –¥–ª—è GPT —ç—Ç–∞–ø –¥–æ–æ–±—É—á–µ–Ω–∏—è –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω. –î–ª—è —Ç–æ–≥–æ —á—Ç–æ–±—ã –¥–æ–ø–∏—Å—ã–≤–∞—Ç—å —Ç–µ–∫—Å—Ç, –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –æ—á–µ–Ω—å —Ö–æ—Ä–æ—à–æ –ø–æ–Ω–∏–º–∞—Ç—å –µ–≥–æ —Å–º—ã—Å–ª –∏ –∏–º–µ—Ç—å –∑–Ω–∞–Ω–∏—è –æ —Ä–µ–∞–ª—å–Ω–æ–º –º–∏—Ä–µ. –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –∑–Ω–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –º–æ–∂–Ω–æ –ø–æ–ø—ã—Ç–∞—Ç—å—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å, –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É—è –ª–µ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ç–µ–∫—Å—Ç–∞. –ü–æ—Å–∫–æ–ª—å–∫—É GPT —Å–ø–æ—Å–æ–±–Ω–∞ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç, –º—ã –º–æ–∂–µ–º —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –ª—é–±—É—é NLP-–∑–∞–¥–∞—á—É –∫–∞–∫ –∑–∞–¥–∞—á—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è) —Ç–µ–∫—Å—Ç–∞. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ä–µ—à–∞—Ç—å –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∑–∞–¥–∞—á —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º –∏–ª–∏ –±–µ–∑ –Ω–µ–≥–æ.\n",
    "\n",
    "–ü–æ–¥–±–æ—Ä –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–π —Ç–µ–∫—Å—Ç–∞ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è \"–∑–∞—Ç—Ä–∞–≤–æ—á–Ω–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ\" (prompt engineering). –°—É—Ç—å –∑–∞—Ç—Ä–∞–≤–æ—á–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –≤ —Ç–æ–º, —á—Ç–æ–±—ã –ø–æ–¥–æ–±—Ä–∞—Ç—å —Ç–∞–∫–∏–µ –∑–∞–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∏ –Ω–∞—á–∞–ª–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, —á—Ç–æ–±—ã –¥–∞–ª—å—à–µ –º–æ–¥–µ–ª—å –≤—ã–¥–∞–≤–∞–ª–∞ —Ä–æ–≤–Ω–æ —Ç–æ, —á—Ç–æ –Ω–∞–¥–æ. –ü–æ–¥–±–∏—Ä–∞—è \"–∑–∞—Ç—Ä–∞–≤–∫–∏\" –∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ (—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞, `top-k`, `top-p`), –º–æ–∂–Ω–æ –¥–æ–±–∏—Ç—å—Å—è —Ö–æ—Ä–æ—à–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤ —Ç–æ–ª—å–∫–æ –∑–∞ —Å—á–µ—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.\n",
    "\n",
    "–°—É—â–µ—Å—Ç–≤—É–µ—Ç –¥–≤–∞ –ø–æ–¥—Ö–æ–¥–∞ –¥–ª—è –∑–∞—Ç—Ä–∞–≤–æ—á–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è:\n",
    "- zero-shot: —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞–¥–∞—á—É –∏ –ø–µ—Ä–µ–¥–∞—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é;\n",
    "- few-shot: —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞–¥–∞—á—É, –ø–æ–¥–∞—Ç—å –≤ –º–æ–¥–µ–ª—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –∏ –∑–∞—Ç–µ–º –ø–µ—Ä–µ–¥–∞—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"ai-forever/rugpt3large_based_on_gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero-shot learning\n",
    "text = \"–° –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –Ω–∞ —Ä—É—Å—Å–∫–∏–π 'cat' –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—Å—è –∫–∞–∫ '\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "out = model.generate(input_ids, do_sample=False,max_length=15)\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# few-shot learning\n",
    "text = \"–° –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –Ω–∞ —Ä—É—Å—Å–∫–∏–π 'cat' –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—Å—è –∫–∞–∫ '–∫–æ—à–∫–∞', 'dog' –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—Å—è –∫–∞–∫ '—Å–æ–±–∞–∫–∞', –∞ 'bird' –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—Å—è –∫–∞–∫ '\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "out = model.generate(input_ids, do_sample=False,max_length=35)\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–¥–µ–ª–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—É—é —Å–∏—Å—Ç–µ–º—É –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è: –±—É–¥–µ–º –ø–æ–¥–∞–≤–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –≤–∏–¥–µ —Ç–µ–∫—Å—Ç–∞ –∏ —Å–º–æ—Ç—Ä–µ—Ç—å, –∫–∞–∫ –º–æ–¥–µ–ª—å –ø—Ä–æ–¥–æ–ª–∂–∏—Ç –µ–≥–æ. –î–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –±—É–¥–µ–º –ø–æ–¥–∞–≤–∞—Ç—å —Ç–µ–∫—Å—Ç —Å –Ω–µ–∑–∞–∫—Ä—ã—Ç–æ–π –∫–∞–≤—ã—á–∫–æ–π, —á—Ç–æ–±—ã –≤—ã–Ω—É–¥–∏—Ç—å –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏–º–µ–Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(1)\n",
    "\n",
    "# Film recomendation\n",
    "text = \"–Ø –ª—é–±–ª—é —Å–æ–≤–µ—Ç—Å–∫–∏–µ –∫–æ–º–µ–¥–∏–∏: ‚Äú–ü—Ä–∏–∫–ª—é—á–µ–Ω–∏—è –®—É—Ä–∏–∫–∞‚Äù, ‚Äú\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "out = model.generate(input_ids,\n",
    "                     do_sample=True,\n",
    "                     temperature=1.4,\n",
    "                     top_k=20,\n",
    "                     top_p=0.8,\n",
    "                     max_length=46,\n",
    "                    )\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(1)\n",
    "\n",
    "# Music recomendation\n",
    "text = \"–¢–µ–º –ª—é–¥—è–º, –∫–æ–º—É –Ω—Ä–∞–≤–∏—Ç—Å—è ‚Äú–ê–ª–∏—Å–∞‚Äù, —Ç–∞–∫–∂–µ –ø–æ–Ω—Ä–∞–≤—è—Ç—Å—è –≥—Ä—É–ø–ø—ã ‚Äú\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "out = model.generate(input_ids,\n",
    "                     do_sample=True,\n",
    "                     temperature=1.1,\n",
    "                     top_k=10,\n",
    "                     top_p=0.7,\n",
    "                     max_length=35,\n",
    "                    )\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–†–∞—Å—Å–º–æ—Ç—Ä–∏–º –±–∏–Ω–∞—Ä–Ω—É—é –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –ø–æ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –±–µ–∑ –¥–æ–æ–±—É—á–µ–Ω–∏—è.\n",
    "\n",
    "–î–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ—Ä—É –ø–µ—Ä–ø–ª–µ–∫—Å–∏–∏. –í —Ç–µ–æ—Ä–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –ø–µ—Ä–ø–ª–µ–∫—Å–∏—è ‚Äì –º–µ—Ä–∞ —Ç–æ–≥–æ, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –≤—ã–±–æ—Ä–∫—É. –ì–æ–≤–æ—Ä—è –ø—Ä–æ—Å—Ç—ã–º —è–∑—ã–∫–æ–º, –ø–µ—Ä–ø–ª–µ–∫—Å–∏—è ‚Äì –º–µ—Ä–∞ \"—É–¥–∏–≤–ª—ë–Ω–Ω–æ—Å—Ç–∏\" –º–æ–¥–µ–ª–∏.\n",
    "\n",
    "–ù–∞ –≤—Ö–æ–¥ –º–æ–¥–µ–ª–∏ –º—ã –ø–æ–¥–∞—ë–º –∑–∞—Ç—Ä–∞–≤–∫—É –≤ —Ñ–æ—Ä–º–∞—Ç–µ <–º–µ—Ç–∫–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏> + <–æ—Ç–∑—ã–≤>. –î–∞–ª–µ–µ –º—ã —Å—á–∏—Ç–∞–µ–º –ø–µ—Ä–ø–ª–µ–∫—Å–∏—é –∫–∞–∂–¥–æ–≥–æ –∏–∑ –Ω–∏—Ö. –í—ã–±–∏—Ä–∞–µ–º –Ω–∞–∏–º–µ–Ω—å—à—É—é –ø–µ—Ä–ø–ª–µ–∫—Å–∏—é –∏–∑ –¥–≤—É—Ö –∏ –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º –º–µ—Ç–∫—É –æ—Ç–∑—ã–≤—É."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_perplexity(sentence, model, tokenizer):\n",
    "    # Add tone lable to sentence\n",
    "    sentence_positive = '–¥–æ–≤–æ–ª–µ–Ω:'+sentence\n",
    "    sentence_negative = '–Ω–µ–¥–æ–≤–æ–ª–µ–Ω:'+sentence\n",
    "    list_sent = [sentence_positive, sentence_negative]\n",
    "    ppl_values = []\n",
    "\n",
    "    for sentence in list_sent:\n",
    "      # Tokenize sentence\n",
    "      encodings = tokenizer(sentence, return_tensors='pt')\n",
    "      input_ids = encodings.input_ids.to(device)\n",
    "      # Apply model\n",
    "      with torch.no_grad():\n",
    "          outputs = model(input_ids=input_ids, labels=input_ids)\n",
    "      loss = outputs.loss\n",
    "      # Count perplexity\n",
    "      ppl = math.exp(loss.item() * input_ids.size(1))\n",
    "      ppl_values.append(ppl)\n",
    "\n",
    "    # Choose sentence with lower perplexity\n",
    "    if ppl_values[0] > ppl_values[1]:\n",
    "      return '–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π'\n",
    "    else:\n",
    "      return '–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = '—è —á—É—Ç—å –Ω–µ –∑–∞—Å–Ω—É–ª –≤–æ –≤—Ä–µ–º—è —Ñ–∏–ª—å–º–∞'\n",
    "print(f'–û—Ç–∑—ã–≤ \"{negative}\": {calculate_perplexity(negative, model, tokenizer)}')\n",
    "positive = '—Å—é–∂–µ—Ç –Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–æ –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–π'\n",
    "print(f'–û—Ç–∑—ã–≤ \"{positive}\": {calculate_perplexity(positive, model, tokenizer)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –§–∞–π–Ω—Ç—é–Ω–∏–Ω–≥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è –º–æ–¥–µ–ª—å—é –º–µ–Ω—å—à–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞, —á—Ç–æ–±—ã –æ–Ω–∞ –ø–æ–º–µ—Å—Ç–∏–ª–∞—Å—å –Ω–∞ GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"ai-forever/rugpt3small_based_on_gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–±—É—á–∞—é—â–∏–π —Ç–µ–∫—Å—Ç –Ω–∞—Ä–µ–∑–∞–µ—Ç—Å—è –Ω–∞ —Å–ª—É—á–∞–π–Ω—ã–µ –±–ª–æ–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ—Å—Ç–∞–≤–ª—è—é—Ç—Å—è –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–∑ 1024 (2048 —É GPT-3) —Ç–æ–∫–µ–Ω–æ–≤, —Ä–∞–∑–¥–µ–ª—è—è—Å—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º `<|endoftext|>` —Å–∏–º–≤–æ–ª–æ–º. –í–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å (–∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å) –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –æ–¥–∏–Ω –∑–∞ –¥—Ä—É–≥–∏–º –ø—Ä–∏ –ø–æ–º–æ—â–∏ Cross-Entropy Loss.\n",
    "\n",
    "–¢–∞–∫ –∫–∞–∫ –≤—Ö–æ–¥–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤—Å–µ–≥–¥–∞ –∑–∞–ø–æ–ª–Ω–µ–Ω–∞ –¥–æ –∫–æ–Ω—Ü–∞, padding –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è. –ù–æ –≤–æ –≤—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –¥–ª–∏–Ω–∞ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–π, –ø–æ—ç—Ç–æ–º—É –Ω–∞–¥–æ —è–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞—Ç—å, —á–µ–º –¥–æ–ø–æ–ª–Ω—è—Ç—å –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –ø–æ–∑–∏—Ü–∏–∏. –ü–æ –¥–µ—Ñ–æ–ª—Ç—É –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ—Ç –∂–µ `<|endoftext|>`.\n",
    "\n",
    "–í –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –≤–µ—Ä—Å–∏—è—Ö GPT –≤—ã—à–µ—Å–∫–∞–∑–∞–Ω–Ω–æ–µ –º–æ–∂–µ—Ç –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å—Å—è. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ ruGPT3 –≥–æ—Ä–∞–∑–¥–æ –±–æ–ª—å—à–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤: `<s\\>`, `<s>`, `<pad>`, `<unk>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–û–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ**\n",
    "\n",
    "–ë—É–¥–µ–º —É—á–∏—Ç—å GPT –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å—Ç–∏—Ö–∏ –ú–∞—è–∫–æ–≤—Å–∫–æ–≥–æ. –í –∫–∞—á–µ—Å—Ç–≤–µ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –≤–æ–∑—å–º—ë–º –≤—Å–µ–≥–æ –ª–∏—à—å –æ–¥–∏–Ω —Å—Ç–∏—Ö."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"–î—ã–º —Ç–∞–±–∞—á–Ω—ã–π –≤–æ–∑–¥—É—Ö –≤—ã–µ–ª.\n",
    "–ö–æ–º–Ω–∞—Ç–∞ ‚Äî\n",
    "–≥–ª–∞–≤–∞ –≤ –∫—Ä—É—á–µ–Ω—ã—Ö–æ–≤—Å–∫–æ–º –∞–¥–µ.\n",
    "–í—Å–ø–æ–º–Ω–∏ ‚Äî\n",
    "–∑–∞ —ç—Ç–∏–º –æ–∫–Ω–æ–º\n",
    "–≤–ø–µ—Ä–≤—ã–µ\n",
    "—Ä—É–∫–∏ —Ç–≤–æ–∏, –∏—Å—Å—Ç—É–ø–ª–µ–Ω–Ω—ã–π, –≥–ª–∞–¥–∏–ª.\n",
    "–°–µ–≥–æ–¥–Ω—è —Å–∏–¥–∏—à—å –≤–æ—Ç,\n",
    "—Å–µ—Ä–¥—Ü–µ –≤ –∂–µ–ª–µ–∑–µ.\n",
    "–î–µ–Ω—å –µ—â–µ ‚Äî\n",
    "–≤—ã–≥–æ–Ω–∏—à—å,\n",
    "–º–æ–∂–µ—Ç –±—ã—Ç—å, –∏–∑—Ä—É–≥–∞–≤.\n",
    "–í –º—É—Ç–Ω–æ–π –ø–µ—Ä–µ–¥–Ω–µ–π –¥–æ–ª–≥–æ –Ω–µ –≤–ª–µ–∑–µ—Ç\n",
    "—Å–ª–æ–º–∞–Ω–Ω–∞—è –¥—Ä–æ–∂—å—é —Ä—É–∫–∞ –≤ —Ä—É–∫–∞–≤.\n",
    "–í—ã–±–µ–≥—É,\n",
    "—Ç–µ–ª–æ –≤ —É–ª–∏—Ü—É –±—Ä–æ—à—É —è.\n",
    "–î–∏–∫–∏–π,\n",
    "–æ–±–µ–∑—É–º–ª—é—Å—å,\n",
    "–æ—Ç—á–∞—è–Ω—å–µ–º –∏—Å—Å–µ—á–∞ÃÅ—Å—å.\n",
    "–ù–µ –Ω–∞–¥–æ —ç—Ç–æ–≥–æ,\n",
    "–¥–æ—Ä–æ–≥–∞—è,\n",
    "—Ö–æ—Ä–æ—à–∞—è,\n",
    "–¥–∞–π –ø—Ä–æ—Å—Ç–∏–º—Å—è —Å–µ–π—á–∞—Å.\n",
    "–í—Å–µ —Ä–∞–≤–Ω–æ\n",
    "–ª—é–±–æ–≤—å –º–æ—è ‚Äî\n",
    "—Ç—è–∂–∫–∞—è –≥–∏—Ä—è –≤–µ–¥—å ‚Äî\n",
    "–≤–∏—Å–∏—Ç –Ω–∞ —Ç–µ–±–µ,\n",
    "–∫—É–¥–∞ –Ω–∏ –±–µ–∂–∞–ª–∞ –±.\n",
    "–î–∞–π –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–º –∫—Ä–∏–∫–µ –≤—ã—Ä–µ–≤–µ—Ç—å\n",
    "–≥–æ—Ä–µ—á—å –æ–±–∏–∂–µ–Ω–Ω—ã—Ö –∂–∞–ª–æ–±.\n",
    "–ï—Å–ª–∏ –±—ã–∫–∞ —Ç—Ä—É–¥–æ–º —É–º–æ—Ä—è—Ç ‚Äî\n",
    "–æ–Ω —É–π–¥–µ—Ç,\n",
    "—Ä–∞–∑–ª—è–∂–µ—Ç—Å—è –≤ —Ö–æ–ª–æ–¥–Ω—ã—Ö –≤–æ–¥–∞—Ö.\n",
    "–ö—Ä–æ–º–µ –ª—é–±–≤–∏ —Ç–≤–æ–µ–π,\n",
    "–º–Ω–µ\n",
    "–Ω–µ—Ç—É –º–æ—Ä—è,\n",
    "–∞ —É –ª—é–±–≤–∏ —Ç–≤–æ–µ–π –∏ –ø–ª–∞—á–µ–º –Ω–µ –≤—ã–º–æ–ª–∏—à—å –æ—Ç–¥—ã—Ö.\n",
    "–ó–∞—Ö–æ—á–µ—Ç –ø–æ–∫–æ—è —É—Å—Ç–∞–≤—à–∏–π —Å–ª–æ–Ω ‚Äî\n",
    "—Ü–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–π –ª—è–∂–µ—Ç –≤ –æ–ø–æ–∂–∞—Ä–µ–Ω–Ω–æ–º –ø–µ—Å–∫–µ.\n",
    "–ö—Ä–æ–º–µ –ª—é–±–≤–∏ —Ç–≤–æ–µ–π,\n",
    "–º–Ω–µ\n",
    "–Ω–µ—Ç—É —Å–æ–ª–Ω—Ü–∞,\n",
    "–∞ —è –∏ –Ω–µ –∑–Ω–∞—é, –≥–¥–µ —Ç—ã –∏ —Å –∫–µ–º.\n",
    "–ï—Å–ª–∏ –± —Ç–∞–∫ –ø–æ—ç—Ç–∞ –∏–∑–º—É—á–∏–ª–∞,\n",
    "–æ–Ω\n",
    "–ª—é–±–∏–º—É—é –Ω–∞ –¥–µ–Ω—å–≥–∏ –± –∏ —Å–ª–∞–≤—É –≤—ã–º–µ–Ω—è–ª,\n",
    "–∞ –º–Ω–µ\n",
    "–Ω–∏ –æ–¥–∏–Ω –Ω–µ —Ä–∞–¥–æ—Å—Ç–µ–Ω –∑–≤–æ–Ω,\n",
    "–∫—Ä–æ–º–µ –∑–≤–æ–Ω–∞ —Ç–≤–æ–µ–≥–æ –ª—é–±–∏–º–æ–≥–æ –∏–º–µ–Ω–∏.\n",
    "–ò –≤ –ø—Ä–æ–ª–µ—Ç –Ω–µ –±—Ä–æ—à—É—Å—å,\n",
    "–∏ –Ω–µ –≤—ã–ø—å—é —è–¥–∞,\n",
    "–∏ –∫—É—Ä–æ–∫ –Ω–µ —Å–º–æ–≥—É –Ω–∞–¥ –≤–∏—Å–∫–æ–º –Ω–∞–∂–∞—Ç—å.\n",
    "–ù–∞–¥–æ –º–Ω–æ—é,\n",
    "–∫—Ä–æ–º–µ —Ç–≤–æ–µ–≥–æ –≤–∑–≥–ª—è–¥–∞,\n",
    "–Ω–µ –≤–ª–∞—Å—Ç–Ω–æ –ª–µ–∑–≤–∏–µ –Ω–∏ –æ–¥–Ω–æ–≥–æ –Ω–æ–∂–∞.\n",
    "–ó–∞–≤—Ç—Ä–∞ –∑–∞–±—É–¥–µ—à—å,\n",
    "—á—Ç–æ —Ç–µ–±—è –∫–æ—Ä–æ–Ω–æ–≤–∞–ª,\n",
    "—á—Ç–æ –¥—É—à—É —Ü–≤–µ—Ç—É—â—É—é –ª—é–±–æ–≤—å—é –≤—ã–∂–µ–≥,\n",
    "–∏ —Å—ÉÃÅ–µ—Ç–Ω—ã—Ö –¥–Ω–µ–π –≤–∑–º–µ—Ç–µ–Ω–Ω—ã–π –∫–∞—Ä–Ω–∞–≤–∞–ª\n",
    "—Ä–∞—Å—Ç—Ä–µ–ø–ª–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã –º–æ–∏—Ö –∫–Ω–∏–∂–µ–∫‚Ä¶\n",
    "–°–ª–æ–≤ –º–æ–∏—Ö —Å—É—Ö–∏–µ –ª–∏—Å—Ç—å—è –ª–∏\n",
    "–∑–∞—Å—Ç–∞–≤—è—Ç –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è,\n",
    "–∂–∞–¥–Ω–æ –¥—ã—à–∞?\n",
    "–î–∞–π —Ö–æ—Ç—å\n",
    "–ø–æ—Å–ª–µ–¥–Ω–µ–π –Ω–µ–∂–Ω–æ—Å—Ç—å—é –≤—ã—Å—Ç–µ–ª–∏—Ç—å\n",
    "—Ç–≤–æ–π —É—Ö–æ–¥—è—â–∏–π —à–∞–≥..\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í –±–∏–±–ª–∏–æ—Ç–µ–∫–µ transformers –µ—Å—Ç—å –≥–æ—Ç–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ –∑–∞–≥—Ä—É–∑—á–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö. –ù–∞ –≤—Ö–æ–¥ –Ω—É–∂–µ–Ω –≤—Å–µ–≥–æ –ª–∏—à—å –æ–¥–∏–Ω `.txt` —Ñ–∞–π–ª —Å –æ–±—É—á–∞—é—â–∏–º —Ç–µ–∫—Å—Ç–æ–º."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save text train data as .txt file\n",
    "train_path = \"train_dataset.txt\"\n",
    "with open(train_path, mode=\"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], padding=\"max_length\", truncation=False, max_length=64\n",
    "    )\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"text\", data_files={\"train\": train_path})\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# –°rop the text into optimal length pieces\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–û–±—É—á–µ–Ω–∏–µ**\n",
    "\n",
    "–î–ª—è —Ñ–∞–π–Ω—Ç—é–Ω–∏–Ω–≥–∞ –Ω–∞–º –Ω–µ–æ–±—Ö–æ–¥–∏–º –æ–±—ä–µ–∫—Ç –∫–ª–∞—Å—Å–∞ Trainer, –∫–æ—Ç–æ—Ä—ã–π —Å–¥–µ–ª–∞–µ—Ç –≤—Å—é —Ä–∞–±–æ—Ç—É –∑–∞ –Ω–∞—Å. –î–∞–ª–µ–µ –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –≤—Å–µ–≥–æ –ª–∏—à—å –∑–∞–ø—É—Å—Ç–∏—Ç—å `trainer.train()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finetuned\",  # The output directory\n",
    "    overwrite_output_dir=True,  # overwrite the content of the output directory\n",
    "    num_train_epochs=200,  # number of training epochs\n",
    "    per_device_train_batch_size=32,  # batch size for training\n",
    "    per_device_eval_batch_size=32,  # batch size for evaluation\n",
    "    warmup_steps=10,  # number of warmup steps for learning rate scheduler\n",
    "    gradient_accumulation_steps=16,  # to make \"virtual\" batch size larger\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    optimizers=(\n",
    "        torch.optim.AdamW(model.parameters(), lr=1e-5),\n",
    "        None,\n",
    "    ),  # Optimizer and learnig rate scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–†–µ–∑—É–ª—å—Ç–∞—Ç —Ñ–∞–π–Ω—Ç—é–Ω–∏–Ω–≥–∞**\n",
    "\n",
    "–ì–æ—Ç–æ–≤–æ! –¢–µ–ø–µ—Ä—å –¥–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º, —á—Ç–æ –∂–µ —Å–æ—á–∏–Ω–∏—Ç GPT –≤ —Å—Ç–∏–ª–µ –ú–∞—è–∫–æ–≤—Å–∫–æ–≥–æ, –µ—Å–ª–∏ –Ω–∞ –≤—Ö–æ–¥ –ø–æ–¥–∞—Ç—å —Ç–∞–∫—É—é —Å—Ç—Ä–æ—á–∫—É:\n",
    "\n",
    "\"–ö–∞–∫ –∂–µ —Å–ª–æ–∂–Ω–æ —É—á–∏—Ç—å –º–∞—Ç–∞–Ω–∞–ª–∏–∑!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability sampling with limit example\n",
    "text = \"–ö–∞–∫ –∂–µ —Å–ª–æ–∂–Ω–æ —É—á–∏—Ç—å –º–∞—Ç–∞–Ω–∞–ª–∏–∑!\\n\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True,\n",
    "        num_beams=3,\n",
    "        temperature=1.9,\n",
    "        top_p=0.9,\n",
    "        max_length=200,\n",
    "        pad_token_id=512,\n",
    "    )\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –û—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏-–¥–µ–∫–æ–¥–µ—Ä—ã"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú–æ–¥–µ–ª–∏, —É –∫–æ—Ç–æ—Ä—ã—Ö –±–æ–ª–µ–µ —á–µ–º –º–∏–ª–ª–∏–∞—Ä–¥ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –æ–±—ã—á–Ω–æ –Ω–∞–∑—ã–≤–∞—é—Ç –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (–ë–Ø–ú) –∏–ª–∏ large language models (LLM). –≠—Ç–æ—Ç —Ç–µ—Ä–º–∏–Ω –æ—Ç–Ω–æ—Å–∏—Ç—Å—è —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ –∫ –º–æ–¥–µ–ª—è–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–µ–∫–æ–¥–µ—Ä–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞. –ü—Ä–∏ —ç—Ç–æ–º –≤ –¥–∞–ª—å–Ω–µ–π—à–µ–º –æ–¥–Ω–∞ –∏ —Ç–∞ –∂–µ –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –±—ã—Ç—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω–æ–º –∏–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ.\n",
    "\n",
    "–ö LLM –æ—Ç–Ω–æ—Å—è—Ç—Å—è GPT, –Ω–∞—á–∏–Ω–∞—è —Å 3-–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è. –°—É—â–µ—Å—Ç–≤—É—é—Ç –∏ –¥—Ä—É–≥–∏–µ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ –æ—Ç–∫—Ä—ã—Ç–æ–º –¥–æ—Å—Ç—É–ø–µ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLaMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Ä–∞–∑–≤–∏—Ç–∏—è –¥–µ–∫–æ–¥–µ—Ä–æ–≤. –ö–æ–Ω—Ü–µ–ø—Ü–∏—è Large Language Model Meta AI –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –æ–±—É—á–µ–Ω–∏–∏ –º–µ–Ω—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –±–û–ª—å—à–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –¥–∞–Ω–Ω—ã—Ö.\n",
    "\n",
    "[[paper] üéì LLaMA: Open and Efficient Foundation Language Models (2023)](https://arxiv.org/abs/2302.13971)\n",
    "\n",
    "LLaMA-13B –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç GPT-3 –ø–æ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤—É —Ç–µ—Å—Ç–æ–≤, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Ç–æ, —á—Ç–æ –æ–Ω–∞ –≤ 10 —Ä–∞–∑ –º–µ–Ω—å—à–µ.\n",
    "\n",
    "* Pre-normalization [GPT3]. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∫–∞–∂–¥–æ–≥–æ –ø–æ–¥—Å–ª–æ—è –≤–º–µ—Å—Ç–æ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. RMSNorm.\n",
    "* –ê–∫—Ç–∏–≤–∞—Ü–∏—è SwiGLU [PaLM]. –ü—Ä–∏—à–ª–∞ –Ω–∞ –º–µ—Å—Ç–æ ReLU.\n",
    "* Rotary Embeddings [GPTNeo]. –í–º–µ—Å—Ç–æ –∞–±—Å–æ–ª—é—Ç–Ω—ã—Ö –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤–≤–æ–¥—è—Ç—Å—è –Ω–æ–≤—ã–µ.\n",
    "\n",
    "–ú–æ–¥–µ–ª–∏ —Ç–∏–ø–∞ LLaMA-Chat –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π. –¢–∞–∫–∏–µ –º–æ–¥–µ–ª–∏ –æ–±—É—á–∞—é—Ç—Å—è —á–µ—Ä–µ–∑  Reinforcement Learning from Human Feedback (**RLHF**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-content/L07/out/llama.png\" width=\"400\"></center>\n",
    "\n",
    "<center><em>–ú–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–∏ LLaMA</em></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://arxiv.org/pdf/2302.13971.pdf\"> LLaMA: Open and Efficient Foundation Language Models</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LLaMA2**\n",
    "\n",
    "[[paper] üéì Llama 2: Open Foundation and Fine-Tuned Chat Models (2023)](https://arxiv.org/abs/2307.09288)\n",
    "\n",
    "–ü–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø–µ—Ä–≤–æ–π –≤–µ—Ä—Å–∏–µ–π:\n",
    "\n",
    "* +40% –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è,\n",
    "* –∫–æ–Ω—Ç–µ–∫—Å—Ç 4096 —Ç–æ–∫–µ–Ω–æ–≤ (—Ö2 LLAMA),\n",
    "* –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è Grouped-query.\n",
    "\n",
    "**GQA** ‚Äî –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —É–∂–µ –≤–∏–¥–µ–Ω–Ω–æ–≥–æ –Ω–∞–º–∏ Self-Attention, –≤ –∫–æ—Ç–æ—Ä–æ–π –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –¥–µ–ª–∏—Ç—Å—è –Ω–∞ –≥—Ä—É–ø–ø—ã, –≤–Ω—É—Ç—Ä–∏ –∫–æ—Ç–æ—Ä—ã—Ö Key –∏ Value –æ–±—â–∏–µ.\n",
    "\n",
    "[[paper] üéì GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints (2023)](https://arxiv.org/abs/2305.13245)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L07/gqa.png\" width=\"800\"></center>\n",
    "\n",
    "<center><em>–ò—Å—Ç–æ—á–Ω–∏–∫: <a href=\"https://arxiv.org/abs/2305.13245\"> GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints\"</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —É–¥–µ–ª–µ–Ω–æ –ø—Ä–æ—Ü–µ–¥—É—Ä–µ –æ–±—É—á–µ–Ω–∏—è –∏ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–µ –æ—Ç–≤–µ—Ç–æ–≤ –ø–æ —à–∏—Ä–æ–∫–æ–º—É –ø–µ—Ä–µ—á–Ω—é –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤, –æ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –æ—Ç–≤–µ—Ç–æ–≤ –ø—Ä–µ–¥–º–µ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏ –¥–æ –∏—Ö —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏, –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏ –∏ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—é –ø–æ —Ç–æ–º—É –∏–ª–∏ –∏–Ω–æ–º—É –∫—Ä–∏—Ç–µ—Ä–∏—é."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LLaMA3**\n",
    "\n",
    "–î–≤–∞ —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏:\n",
    "- base ‚Äì 8B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "- large ‚Äì 70B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "\n",
    "–û–±–µ –º–æ–¥–µ–ª–∏ –≤ –¥–≤—É—Ö –≤–µ—Ä—Å–∏—è—Ö (pre-trained & instruct tuned). –î–æ—Å—Ç—É–ø–Ω–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É.\n",
    "\n",
    "–û—Ç–ª–∏—á–∏—è –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –≤–µ—Ä—Å–∏–π:\n",
    "- —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è —É–≤–µ–ª–∏—á–µ–Ω –¥–æ 128,256 —Ç–æ–∫–µ–Ω–æ–≤ (VS 32K –≤\n",
    "LLaMA 2)\n",
    "- —Ä–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ —É–≤–µ–ª–∏—á–µ–Ω –≤ 8 —Ä–∞–∑ (–¥–æ 15T\n",
    "—Ç–æ–∫–µ–Ω–æ–≤)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–µ–∑–∞–¥–æ–ª–≥–æ –¥–æ —Ä–µ–ª–∏–∑–∞ LLaMA Meta —Å—Ç–∞–ª–∫–∏–≤–∞–ª–∞—Å—å —Å –∫—Ä–∏—Ç–∏–∫–æ–π –∑–∞ —Å–≤–æ—é –º–æ–¥–µ–ª—å Galactica, –∫–æ—Ç–æ—Ä–∞—è –±—ã–ª–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∞ –Ω–∞ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç—å—è—Ö. –°–Ω–∞—á–∞–ª–∞ –æ–Ω–∏ –≤—ã–ª–æ–∂–∏–ª–∏ –µ—ë –≤ –æ—Ç–∫—Ä—ã—Ç—ã–π –¥–æ—Å—Ç—É–ø, –Ω–æ –±—ã—Å—Ç—Ä–æ –æ–∫–∞–∑–∞–ª–∞—Å—å, —á—Ç–æ –æ–Ω–∞ –º–æ–∂–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Å–µ–≤–¥–æ–Ω–∞—É—á–Ω—ã–µ –∏ –ª–∂–µ–Ω–∞—É—á–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –∏ Meta –±—ã—Å—Ç—Ä–æ –∑–∞–∫—Ä—ã–ª–∞ –¥–æ—Å—Ç—É–ø –∫ —ç—Ç–æ–π –º–æ–¥–µ–ª–∏. –ü–æ—ç—Ç–æ–º—É –º–æ–¥–µ–ª—å LLaMA —Å—Ç—Ä–æ–≥–æ –≥–æ–≤–æ—Ä—è –Ω–µ –≤—ã–ª–æ–∂–µ–Ω–∞ –≤ –æ—Ç–∫—Ä—ã—Ç—ã–π –¥–æ—Å—Ç—É–ø –∏ –∏–º–µ–µ—Ç –Ω–µ–∫–æ–º–µ—Ä—á–µ—Å–∫—É—é –ª–∏—Ü–µ–Ω–∑–∏—é. –ß—Ç–æ–±—ã —Å–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å, –Ω—É–∂–Ω–æ –∑–∞–ø–æ–ª–Ω—è—Ç—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é —Ñ–æ—Ä–º—É –∏ –∂–¥–∞—Ç—å –ø–æ–∫–∞ –µ–µ –æ–¥–æ–±—Ä—è—Ç. –ù–æ, –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ –ª—é–¥–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–ª—É—á–∏–ª–∏ –¥–æ—Å—Ç—É–ø –∫ –º–æ–¥–µ–ª–∏ –Ω–∞—á–∞–ª–∏ –≤—ã–∫–ª–∞–¥—ã–≤–∞—Ç—å –µ–µ –≤ –æ—Ç–∫—Ä—ã—Ç—ã–π –¥–æ—Å—Ç—É–ø.\n",
    "\n",
    "LLaMA 2 —Ç–∞–∫–∂–µ –∏–º–µ–µ—Ç –Ω–µ–∫–æ–º–µ—Ä—á–µ—Å–∫—É—é –ª–∏—Ü–µ–Ω–∑–∏—é –∏ —Ç—Ä–µ–±—É–µ—Ç –∑–∞—è–≤–∫–∏ –Ω–∞ –ª–∏—Ü–µ–Ω—Ü–∏—é, –æ–¥–Ω–∞–∫–æ –æ–¥–æ–±—Ä–µ–Ω–∏–µ –ª–∏—Ü–µ–Ω–∑–∏–∏ –∑–∞–Ω–∏–º–∞–µ—Ç –Ω–µ–±–æ–ª—å—à–æ–µ –≤—Ä–µ–º—è, –ø–æ—ç—Ç–æ–º—É –º–æ–¥–µ–ª—å —É–∂–µ –≤–æ—Å–ø—Ä–∏–Ω–∏–º–∞–µ—Ç—Å—è –∫–∞–∫ –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ç–∫—Ä—ã—Ç–∞—è."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sliding Window Attention‚Äî –æ–±—É—á–µ–Ω–∏–µ —Å –¥–ª–∏–Ω–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ 4,096 –∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —Ä–∞–∑–º–µ—Ä–æ–º –∫—ç—à–∞, —Å —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–º –æ–±—ä–µ–º–æ–º –≤–Ω–∏–º–∞–Ω–∏—è –≤ 128k —Ç–æ–∫–µ–Ω–æ–≤.\n",
    "- GQA (Grouped Query Attention) ‚Äî –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä—ã–π inference –∏\n",
    "–º–µ–Ω—å—à–∏–π —Ä–∞–∑–º–µ—Ä –∫—ç—à–∞.\n",
    "- Byte-fallback BPE tokenizer ‚Äî –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ —Å–∏–º–≤–æ–ª—ã –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –±—É–¥—É—Ç —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω—ã —Å —Ç–æ–∫–µ–Ω–∞–º–∏, –Ω–∞—Ö–æ–¥—è—â–∏–º–∏—Å—è –∑–∞ –ø—Ä–µ–¥–µ–ª–∞–º–∏ —Å–ª–æ–≤–∞—Ä—è."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–ö–∞–∫–∏–µ –µ—â—ë –µ—Å—Ç—å –ë–Ø–ú?**\n",
    "\n",
    "Hugging Face [–ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø–æ–∏—Å–∫–æ–≤–∏–∫ üõ†Ô∏è[doc]](https://huggingface.co/models), –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –≤—ã–±—Ä–∞—Ç—å –ë–Ø–ú –ø–æ–¥ –∑–∞–¥–∞—á—É –∏ —è–∑—ã–∫, –≤–∫–ª—é—á–∞—è –º–æ–¥–µ–ª–∏ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º.\n",
    "\n",
    "–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤–∏–∫, –∞–≥—Ä–µ–≥–∏—Ä—É—é—â–∏–π –º–æ–¥–µ–ª–∏ —Å–æ –≤—Å–µ–π —Å–µ—Ç–∏, —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –≤—ã–±–æ—Ä–∞ —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏, –Ω–∞–ª–∏—á–∏—è –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è –∏ –ø—Ä–æ—á–µ–≥–æ, –Ω–∞—Ö–æ–¥–∏—Ç—Å—è [—Ç—É—Ç üõ†Ô∏è[doc]](https://llm.extractum.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –î–æ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è—Ö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–¥–∏–Ω –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –±—ã—Å—Ç—Ä–æ —Ä–∞–∑–≤–∏–≤–∞—é—â–∏—Ö—Å—è —Ç—Ä–µ–Ω–¥–æ–≤ –≤ NLP ‚Äî —Ä–µ—à–∞—Ç—å –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∑–∞–¥–∞—á –æ–¥–Ω–æ–π –æ–±—â–µ–π –º–æ–¥–µ–ª—å—é.\n",
    "\n",
    "- –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –±–æ–ª—å—à–æ–º –æ–±—ä–µ–º–µ –Ω–µ—Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∑–∞–∫–ª–∞–¥—ã–≤–∞–µ—Ç —à–∏—Ä–æ–∫–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —è–∑—ã–∫–∞\n",
    "- –î–æ–æ–±—É—á–µ–Ω–∏–µ –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –∑–∞–¥–∞—á—É —Ç—Ä–µ–±—É–µ—Ç –Ω–∞–ª–∏—á–∏—è —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –∫–æ—Ä–ø—É—Å–æ–≤ –≥–æ—Ä–∞–∑–¥–æ –º–µ–Ω—å—à–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞\n",
    "- –ü—Ä–∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞–≤–∫–∞—Ö (–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è—Ö) –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç —Ä–µ—à–∞—Ç—å –∑–∞–¥–∞—á–∏, –∫–æ—Ç–æ—Ä—ã–µ –æ–Ω–∏ –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –≤–∏–¥–µ–ª–∏\n",
    "- –í—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ NLP –∑–∞–¥–∞—á–∏ –º–æ–∂–Ω–æ —Å–≤–µ—Å—Ç–∏ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–µ—Ä–µ–≤–æ–¥–∞ –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–∞ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ–ø–µ—á–∞—Ç–æ–∫, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–º–µ–Ω–æ–≤–∞–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π. –ì–ª–∞–≤–Ω–æ–µ ‚Äî –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞—Ç—Ä–∞–≤–∫—É –∏–ª–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://1.bp.blogspot.com/-o4oiOExxq1s/Xk26XPC3haI/AAAAAAAAFU8/NBlvOWB84L0PTYy9TzZBaLf6fwPGJTR0QCLcBGAsYHQ/s640/image3.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[paper] üéì Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (2019)](https://arxiv.org/abs/1910.10683)\n",
    "\n",
    "–≠—Ç–æ –æ—á–µ–Ω—å –±–æ–ª—å—à–∞—è —Å—Ç–∞—Ç—å—è, –≤ –∫–æ—Ç–æ—Ä–æ–π –ø–æ–¥—Ä–æ–±–Ω–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–ª–∞—Å—å —É–Ω–∏—Ñ–∏–∫–∞—Ü–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö NLP –∑–∞–¥–∞—á –≤ –∑–∞–¥–∞—á—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –ê —Ç–∞–∫–∂–µ –ø–æ–ø—Ä–æ–±–æ–≤–∞–ª–∏ –º–Ω–æ–≥–æ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –∫ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—é. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –º–æ–¥–µ–ª–∏ –¢5, –∏ –≤—Å–µ –∏—Ö –æ–Ω–∏ –≤—ã–ª–æ–∂–∏–ª–∏ –≤ –æ—Ç–∫—Ä—ã—Ç—ã–π –¥–æ—Å—Ç—É–ø.\n",
    "\n",
    "–ï—â–µ –≤ —Å—Ç–∞—Ç—å–µ –æ–Ω–∏ –ø—Ä–æ–±–æ–≤–∞–ª–∏ —Ç—é–Ω–∏—Ç—å –º–æ–¥–µ–ª—å –ø–æ–¥ —Ä–∞–∑–Ω—ã–µ –∑–∞–¥–∞—á–∏, –Ω–æ –ø–æ –±–æ–ª—å—à–µ–π —á–∞—Å—Ç–∏ –≤—Å–µ –µ—â–µ –ø–æ –æ—Ç–¥–µ–ª—å–Ω–æ—Å—Ç–∏. –ï—Å–ª–∏ –≤—ã –ø—Ä–æ—á–∏—Ç–∞–µ—Ç–µ —Å—Ç–∞—Ç—å—é –∏–ª–∏ —Ö–æ—Ç—è –±—ã –æ–ø–∏—Å–∞–Ω–∏–µ fine-tuning —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤, —Ç–æ —É–≤–∏–¥–∏—Ç–µ, —á—Ç–æ –Ω–∞ —Ç–æ—Ç –º–æ–º–µ–Ω—Ç –ø–∞—Ä–∞–¥–∏–≥–º–∞ (1 –º–æ–¥–µ–ª—å - 1 –∑–∞–¥–∞—á–∞) –µ—â–µ –Ω–µ –∏–∑–º–µ–Ω–∏–ª–∞—Å—å. –í —Å–≤–æ–∏—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ö –æ–Ω–∏ –ø—Ä–æ–±–æ–≤–∞–ª–∏ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞—Ç—å—Å—è —Å—Ä–∞–∑—É –ø–æ–¥ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–∞–¥–∞—á, –Ω–æ —É –Ω–∏—Ö –±—ã–ª–æ –Ω–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –º–Ω–æ–≥–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á –∏ –≤ –∏—Ç–æ–≥–µ –æ–±—â–∞—è –º–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–ª–∞ —Ö—É–∂–µ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, —á–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –º–æ–¥–µ–ª–∏.\n",
    "\n",
    "–ù–æ –≤ –æ—Ç–∫—Ä—ã—Ç—ã–π –¥–æ—Å—Ç—É–ø –æ–Ω–∏ –≤—ã–ª–æ–∂–∏–ª–∏ –≤ —Ç–æ–º —á–∏—Å–ª–µ –∏ –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –¥–æ–æ–±—É—á–µ–Ω—ã –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö. –ó–∞–¥–∞—á–∞ –≤ –º–æ–¥–µ–ª—å –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ –ø—Ä–µ—Ñ–∏–∫—Å (–ø–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –Ω–∞ –Ω–∞—á–∞–ª–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –Ω–∏–∂–µ). –≠—Ç–∏ –º–æ–¥–µ–ª–∏ –µ—Å—Ç—å –Ω–∞ huggingface, –¥–∞–≤–∞–π—Ç–µ –ø–æ–ø—Ä–æ–±—É–µ–º –≤–∑—è—Ç—å –∫–∞–∫—É—é-—Ç–æ –º–æ–¥–µ–ª—å –∏ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Å—Ö–æ–¥—É —Ä–µ—à–∏—Ç—å –∑–∞–¥–∞—á—É —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_NAME = 't5-large'\n",
    "MODEL_NAME = 't5-base'\n",
    "# MODEL_NAME = 't5-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, model_max_length=512)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í–æ–∑—å–º–µ–º –∫–∞–∫–æ–π-–Ω–∏–±—É–¥—å —Ç–µ–∫—Å—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_prefix = \"summarize: {}\"\n",
    "\n",
    "text = \"\"\"\n",
    "Badgers burrowing under rail tracks have halted trains in the northern and southern Netherlands, forcing lengthy cancellations on at least two lines.\n",
    "All trains were halted Tuesday afternoon on a busy line between the southern cities of Den Bosch and Boxtel after the animals dug into a dike carrying rails. The national railway company said the line would be out of service for at least a week.\n",
    "The digging means \"the rails can subside and then the safety of train traffic can no longer be guaranteed,\" ProRail, the company that maintains the Dutch rail network said in a statement.\n",
    "Earlier this month, badgers also burrowed under tracks near the northern village of Molkwerum in Friesland province, knocking a line out of service until next month while workers seek permission to shift the animals.\n",
    "Badgers are protected animals in the Netherlands, so rail operators have to get permission to move them or disturb their habitat before repairs can begin.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ–ø—Ä–æ–±—É–µ–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–∞–º–º–∞—Ä–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([task_prefix.format(text)],\n",
    "                    return_tensors=\"pt\", padding=True)\n",
    "\n",
    "output_sequences = model.generate(\n",
    "    # this parameters are also important but you can read about them in the docs and just try changing them\n",
    "    num_beams=5,\n",
    "    max_length=100,\n",
    "    no_repeat_ngram_size=3,\n",
    "#     repetition_penalty= 5.0,\n",
    "#     length_penalty=0.01,\n",
    "#     early_stopping=True,\n",
    "#     do_sample=True,\n",
    "#     top_k=30,\n",
    "#     top_p=0.8,\n",
    "    early_stopping=True,\n",
    "#     num_return_sequences=3,\n",
    "    num_return_sequences= 1,\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    do_sample=False,  # disable sampling to test if batching affects output\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = tokenizer.batch_decode(output_sequences, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–†–∞–±–æ—Ç–∞–µ—Ç –Ω–µ–ø–ª–æ—Ö–æ, –Ω–æ, –∫–æ–Ω–µ—á–Ω–æ, –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –Ω—É–∂–Ω–æ —Ç—é–Ω–∏—Ç—å –º–æ–¥–µ–ª—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FLAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://1.bp.blogspot.com/-_kPdaMrcRWI/YV2b-XFoRxI/AAAAAAAAIMw/KDjg0IfuoK8hjpSXNODoV46D8Rb5rK8hgCLcBGAsYHQ/w640-h178/image3.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[paper] üéì Finetuned Language Models Are Zero-Shot Learners (2021)](https://arxiv.org/abs/2109.01652)\n",
    "\n",
    "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ —É–∂–µ –∑–∞–º–µ—Ç–µ–Ω —Å–¥–≤–∏–≥ –≤ —Å—Ç–æ—Ä–æ–Ω—É –æ–±—â–∏—Ö –º–æ–¥–µ–ª–µ–π –∏ —É–∂–µ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–ª—Å—è –ø–æ–¥—Ö–æ–¥ –∫ —Ç–∞–∫–æ–º—É –æ–±—É—á–µ–Ω–∏—é —á–µ—Ä–µ–∑ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏. –û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è –≤ —Å—Ç–∞—Ç—å–µ - –ø–µ—Ä–µ–¥–µ–ª–∞—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ NLP –¥–∞—Ç–∞—Å–µ—Ç—ã –≤ –±–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π (–æ–Ω–∏ —Å–¥–µ–ª–∞–ª–∏ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–µ–º–ø–ª–µ–π—Ç—ã –Ω–∞ –ø—Ä–∞–≤–∏–ª–∞—Ö –∏ –ø—Ä–æ–≥–Ω–∞–ª–∏ –∏—Ö —á–µ—Ä–µ–∑ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã) –∏ –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å —Ä–µ—à–∞—Ç—å —Å—Ä–∞–∑—É –≤—Å—ë. –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø—Ä–∏ —ç—Ç–æ–º —ç—Ç–æ –Ω–µ –∫–∞–∫–∏–µ-—Ç–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ–≥–∏ –∫–∞–∫ –≤ T5, –∞ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏.\n",
    "\n",
    "–ü—Ä–∏–º–µ—Ä—ã –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π:\n",
    "- Please translate this sentence to French: 'The dog\n",
    "runs.'\n",
    "- What is the sentiment of this text? Options: Negative, Positive, Neutral.\n",
    "\n",
    "–ü—Ä–∏ —Ç–∞–∫–æ–º –ø–æ–¥—Ö–æ–¥–µ –æ–Ω–∏ –∑–∞–º–µ—Ç–∏–ª–∏, —á—Ç–æ –º–æ–¥–µ–ª—å –Ω–∞—á–∏–Ω–∞–µ—Ç –æ–±–æ–±—â–∞—Ç—å—Å—è –Ω–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã—Ö –æ–Ω–∞ –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –≤–∏–¥–µ–ª–∞ - —Ç–∞–∫ –∫–∞–∫ –º–æ–¥–µ–ª—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∞ –Ω–∞ –±–æ–ª—å—à–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ —Ç–µ–∫—Å—Ç–æ–≤, –æ–Ω–∞ —É–∂–µ —Ö–æ—Ä–æ—à–æ –ø–æ–Ω–∏–º–∞–µ—Ç —è–∑—ã–∫ –∏ —ç–∫—Å—Ç—Ä–∞–ø–æ–ª–∏—Ä—É–µ—Ç –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏–∑ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏, –∏—Å–ø–æ–ª—å–∑—É—è —Å–≤–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —è–∑—ã–∫–∞). –ò —á–µ–º –±–æ–ª—å—à–µ —Ç–∞–∫–∏—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π, —Ç–µ–º –ª—É—á—à–µ –ø–æ–ª—É—á–∞–ª–æ—Å—å.\n",
    "\n",
    "–û–Ω–∏ –ø–æ–ø—Ä–æ–±–æ–≤–∞–ª–∏ —Ç–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ —Å —Ä–∞–∑–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (T5, PALM) –∏ –≤–µ–∑–¥–µ –ø–æ–ª—É—á–∞–ª–æ—Å—å —Ö–æ—Ä–æ—à–æ —Ä–µ—à–∞—Ç—å –Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FLAN –≤–∞—Ä–∏–∞–Ω—Ç—ã –º–æ–¥–µ–ª–µ–π —Ç–∞–∫–∂–µ –¥–æ—Å—Ç—É–ø–Ω—ã –Ω–∞ huggingface. –î–∞–≤–∞–π—Ç–µ –ø–æ–ø—Ä–æ–±—É–µ–º —Å —Ç–∞–∫–∏–º –∂–µ —Ç–µ–∫—Å—Ç–æ–º."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'google/flan-t5-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, model_max_length=512)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –º–æ–¥–µ–ª–∏ –º–æ–∂–Ω–æ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å –≤ —Å–≤–æ–±–æ–¥–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ, –ø–æ—ç—Ç–æ–º—É —Å–¥–µ–ª–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é, —á—Ç–æ–±—ã —É–¥–æ–±–Ω–µ–µ –±—ã–ª–æ –ø—Ä–æ–±–æ–≤–∞—Ç—å —Ä–∞–∑–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_instruction(instruction, text, model):\n",
    "\n",
    "\n",
    "    inputs = tokenizer([instruction.format(text)],\n",
    "                        return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    output_sequences = model.generate(\n",
    "        # this parameters are also important but you can read about them in the docs and just try changing them\n",
    "        num_beams=5,\n",
    "        max_length=100,\n",
    "        no_repeat_ngram_size=3,\n",
    "    #     repetition_penalty= 5.0,\n",
    "    #     length_penalty=0.01,\n",
    "    #     early_stopping=True,\n",
    "    #     do_sample=True,\n",
    "    #     top_k=30,\n",
    "    #     top_p=0.8,\n",
    "        early_stopping=True,\n",
    "    #     num_return_sequences=3,\n",
    "        num_return_sequences= 1,\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        do_sample=False,  # disable sampling to test if batching affects output\n",
    "    )\n",
    "    summaries = tokenizer.batch_decode(output_sequences, skip_special_tokens=True)\n",
    "    return summaries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Badgers burrowing under rail tracks have halted trains in the northern and southern Netherlands, forcing lengthy cancellations on at least two lines.\n",
    "All trains were halted Tuesday afternoon on a busy line between the southern cities of Den Bosch and Boxtel after the animals dug into a dike carrying rails. The national railway company said the line would be out of service for at least a week.\n",
    "The digging means \"the rails can subside and then the safety of train traffic can no longer be guaranteed,\" ProRail, the company that maintains the Dutch rail network said in a statement.\n",
    "Earlier this month, badgers also burrowed under tracks near the northern village of Molkwerum in Friesland province, knocking a line out of service until next month while workers seek permission to shift the animals.\n",
    "Badgers are protected animals in the Netherlands, so rail operators have to get permission to move them or disturb their habitat before repairs can begin.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Give a summary of this text: {}\"\n",
    "predict_for_instruction(instruction, text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Give a very short summary of this text: {}\"\n",
    "predict_for_instruction(instruction, text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Write a title for the following text:{}\"\n",
    "predict_for_instruction(instruction, text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Suggest keywords for this text. Text: {}\"\n",
    "predict_for_instruction(instruction, text, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RLHF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " –û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑—å—é –æ—Ç —á–µ–ª–æ–≤–µ–∫–∞ (Reinforcement Learning from Human Feedback) ‚Äî –ø–æ–¥—Ö–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª–∏–ª –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è—Ö. –û–Ω –±—ã–ª –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ InstructGPT –∏ –æ–ø–∏—Å–∞–Ω –≤ —Å—Ç–∞—Ç—å–µ  [[paper] üéì Training language models to follow instructions with human feedback (2022)](https://arxiv.org/abs/2203.02155).\n",
    "\n",
    "\n",
    "\n",
    " –ú–æ–¥–µ–ª—å InstructGPT ‚Äî \"–º–ª–∞–¥—à–∏–π –±—Ä–∞—Ç\" ChatGPT. –û–Ω–∞ –Ω–µ –ø—Ä–∏–≤–ª–µ–∫–ª–∞ —Å—Ç–æ–ª—å–∫–æ –≤–Ω–∏–º–∞–Ω–∏—è –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏, –Ω–æ –±–æ–ª–µ–µ –ø–æ–ª–µ–∑–Ω–∞ –Ω–∞–º –∫–∞–∫ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è–º, –ø–æ—Å–∫–æ–ª—å–∫—É –ø—Ä–æ—Ü–µ—Å—Å –µ–µ –æ–±—É—á–µ–Ω–∏—è –æ–ø–∏—Å–∞–Ω –≤ —Å—Ç–∞—Ç—å–µ. –û–¥–Ω–∞–∫–æ —Å–∞–º–∞ –∏–¥–µ—è –æ–±—É—á–µ–Ω–∏—è –ø—Ä–∏–º–µ–Ω–∏–º–∞ –∏ –∫ ChatGPT.\n",
    "\n",
    " –ò—Ç–∞–∫, –∫–∞–∫ –ø—Ä–µ–≤—Ä–∞—Ç–∏—Ç—å (—É–∂–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —É–º–Ω—É—é) GPT-3 –≤ (–µ—â–µ –±–æ–ª–µ–µ —É–º–Ω—É—é) InstructGPT?\n",
    "\n",
    "- –®–∞–≥ 1: —Å–æ–±—Ä–∞—Ç—å –Ω–∞–±–æ—Ä –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –∏ –¥–æ–æ–±—É—á–∏—Ç—å –Ω–∞ –Ω–µ–º GPT-3.\n",
    "\n",
    "–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–æ–ª–∂–Ω—ã –≤–∫–ª—é—á–∞—Ç—å –Ω–µ —Ç–æ–ª—å–∫–æ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ NLP-–∑–∞–¥–∞—á–∏, –Ω–æ –∏ –±–æ–ª–µ–µ \"—Ç–≤–æ—Ä—á–µ—Å–∫–∏–µ\": –ø—Ä–∏–¥—É–º–∞—Ç—å –∏—Å—Ç–æ—Ä–∏—é, –Ω–∞–ø–∏—Å–∞—Ç—å —Å–ø–∏—Å–æ–∫ —á–µ–≥–æ-–Ω–∏–±—É–¥—å, –ø–æ—Å–æ–≤–µ—Ç–æ–≤–∞—Ç—å —á—Ç–æ-—Ç–æ. –ö–∞—á–µ—Å—Ç–≤–æ –≤–∞–∂–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞: –±–æ–ª—å—à–∞—è –¥–æ–ª—è —Ä—É—á–Ω–æ–≥–æ —Ç—Ä—É–¥–∞, –≤—ã—Å–æ–∫–æ–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ –∫ –∫–∞—á–µ—Å—Ç–≤—É –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π.\n",
    "\n",
    "–ü–æ—Å–ª–µ –¥–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ GPT-3 –ø–æ–ª—É—á–∏–º SFT-–º–æ–¥–µ–ª—å (supervised fine-tuned).\n",
    "\n",
    "- –®–∞–≥ 2: –æ—Ü–µ–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–æ–¥–µ–ª–∏ SFT –∏ –æ–±—É—á–∏—Ç—å reward-–º–æ–¥–µ–ª—å.\n",
    "\n",
    "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã SFT-–º–æ–¥–µ–ª–∏ –æ—Ü–µ–Ω–∏–≤–∞—é—Ç—Å—è —Å –ø–æ–º–æ—â—å—é —Ä–∞–∑–º–µ—Ç—á–∏–∫–æ–≤. –ë–µ—Ä–µ—Ç—Å—è —Ç–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, —Å –ø–æ–º–æ—â—å—é SFT-–º–æ–¥–µ–ª–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –æ—Ç–≤–µ—Ç–∞. –î–∞–ª–µ–µ —Ä–∞–∑–º–µ—Ç—á–∏–∫–∏ —Ä–∞–Ω–∂–∏—Ä—É—é—Ç –æ—Ç–≤–µ—Ç—ã –º–æ–¥–µ–ª–∏ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É: –ø–æ–ø–∞—Ä–Ω–æ —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç —ç—Ç–∏ –æ—Ç–≤–µ—Ç—ã –∏ –≥–æ–≤–æ—Ä—è—Ç, –∫–∞–∫–æ–π –∏–∑ –Ω–∏—Ö –ª—É—á—à–µ.\n",
    "\n",
    "–û—Ç–≤–µ—Ç—ã —Ä–∞–∑–º–µ—Ç—á–∏–∫–æ–≤ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è reward-–º–æ–¥–µ–ª–∏. –û–Ω–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Å–ø–æ—Å–æ–±–Ω–∞ –∏–º–∏—Ç–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç —Ä–∞–∑–º–µ—Ç—á–∏–∫–∞ –∏ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç—ã SFT-–º–æ–¥–µ–ª–∏.\n",
    "\n",
    "- –®–∞–≥ 3: –¥–æ–æ–±—É—á–∏—Ç—å SFT-–º–æ–¥–µ–ª—å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º reward-–º–æ–¥–µ–ª–∏\n",
    "\n",
    "SFT-–º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç—ã-–∫–∞–Ω–¥–∏–¥–∞—Ç—ã. –û–Ω–∏ –æ—Ü–µ–Ω–∏–≤–∞—é—Ç—Å—è —Å –ø–æ–º–æ—â—å—é reward-–º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω–æ–π –Ω–∞ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –æ—Ç –ª—é–¥–µ–π. –û—Ü–µ–Ω–∫–∞ reward-–º–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤–µ—Å–æ–≤ SFT-–º–æ–¥–µ–ª–∏. –û–Ω–∞ –¥–æ–æ–±—É—á–∞–µ—Ç—Å—è —Ç–∞–∫, —á—Ç–æ–±—ã –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –ø–æ–ª—É—á–∞–ª–∏ –≤—ã—Å–æ–∫–∏–µ –æ—Ü–µ–Ω–∫–∏.\n",
    "\n",
    "–î–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–µ—Ç–æ–¥ Proximal Policy Optimization (PPO). –ú–æ–¥–µ–ª—å –Ω–µ –ø—Ä–æ—Å—Ç–æ —É—á–∏—Ç—Å—è –º–∞–∫—Å–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –æ—Ü–µ–Ω–∫—É –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤, –Ω–æ –µ—â–µ —Å—Ç–∞—Ä–∞–µ—Ç—Å—è –º–∏–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L07/rlhf_steps.png\" width=\"800\"></center>\n",
    "\n",
    "<center><em>–ò—Å—Ç–æ—á–Ω–∏–∫: <a href=\"https://arxiv.org/abs/2203.02155\">Training language models to follow instructions with human feedback </a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RLHF –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø—Ä–∞–≤–∏—Ç—å —Ç–æ, —á—Ç–æ –±—ã–ª–æ –∑–∞–ª–æ–∂–µ–Ω–æ –∫–∞–∫ –≤–æ –≤—Ä–µ–º—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è, —Ç–∞–∫ –∏ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –º–µ—Ç–æ–¥–æ–º SFT. –≠—Ç–æ –º–æ–≥—É—Ç –±—ã—Ç—å –æ—à–∏–±–∫–∏ –∏–ª–∏ –Ω–µ–∂–µ—Ç–∞–ª—å–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L07/prediction_vs_reward_model.png\" width=\"800\"></center>\n",
    "\n",
    "<center><em>–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å—Ç—É–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞ –∏ –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è</em></center>\n",
    "\n",
    "<center><em>–ò—Å—Ç–æ—á–Ω–∏–∫: <a href=\"https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up#footnote-anchor-9-135824233\"> CAMERON R. WOLFE, \"LLaMA-2 from the Ground Up\"</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∏–º–µ–µ—Ç —Ç—É –∂–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –∏ –≤–µ—Å–∞, —á—Ç–æ –∏ –æ—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å. –†–∞–∑–Ω–∏—Ü–∞ –≤ —Ç–æ–º, —á—Ç–æ —Å–ª–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞) –∑–∞–º–µ–Ω—ë–Ω –Ω–∞ —Å–ª–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏.\n",
    "\n",
    "–ú–æ–¥–µ–ª—å —à—Ç—Ä–∞—Ñ—É–µ—Ç –∑–∞ —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—ã–º —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –æ—Ç–≤–µ—Ç–æ–º –∏ –∏–º–µ—é—â–∏–º—Å—è –º–∞—à–∏–Ω–Ω—ã–º. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –≤ —ç—Ç–æ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è —Å–º–µ—â–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑—É–µ—Ç—Å—è –≤—ã–±—Ä–∞–Ω–Ω–æ–π —á–µ–ª–æ–≤–µ–∫–æ–º –æ—Ü–µ–Ω–∫–æ–π –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–∞.\n",
    "\n",
    "$$\\large L_{\\text{ranking}} = - \\log (œÉ(r_{Œ∏}(x, y_c))-œÉ(r_{Œ∏}(x, y_r))-m(r))$$\n",
    "\n",
    "–°—Ç–æ–∏—Ç —É—á–µ—Å—Ç—å, —á—Ç–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –æ–±—É—á–∞–µ—Ç—Å—è –º–Ω–æ–∂–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –æ—Ç–≤–µ—á–∞—é—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º –æ—Ç–≤–µ—Ç–æ–≤ –º–æ–¥–µ–ª–∏: –æ—Ç –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –¥–æ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpaca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–∏–µ –∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –≤–ª–∏—è–µ—Ç –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏, –Ω–æ —Å–æ–∑–¥–∞–≤–∞—Ç—å —Ç–∞–∫–∏–µ –¥–∞—Ç–∞—Å–µ—Ç—ã —Å–ª–æ–∂–Ω–æ –∏ –¥–æ—Ä–æ–≥–æ, –∞ –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏ –Ω–µ –¥–µ–ª—è—Ç—Å—è. –ü–æ—ç—Ç–æ–º—É –º–Ω–æ–≥–∏–µ —Ä–∞–±–æ—Ç—ã –≤ –æ–±—É—á–µ–Ω–∏–∏ –Ω–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è—Ö –ø–æ—Å–≤—è—â–µ–Ω—ã —Å–ø–æ—Å–æ–±–∞–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ, –Ω–æ –∫–∞–∫ –º–æ–∂–Ω–æ –±–æ–ª–µ–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã. –ó–Ω–∞—á–∏–º–∞—è —Ä–∞–±–æ—Ç–∞ –≤ —ç—Ç–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ - Stanford Alpaca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L07/alpaca.jpg\" width=\"800\"></center>\n",
    "\n",
    "\n",
    "<center><em>–ò—Å—Ç–æ—á–Ω–∏–∫: <a href=\"https://crfm.stanford.edu/2023/03/13/alpaca.html\">Alpaca: A Strong, Replicable Instruction-Following Model\"</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[–ö–æ–¥ –∏ –¥–∞—Ç–∞—Å–µ—Ç üõ†Ô∏è[doc]](https://github.com/tatsu-lab/stanford_alpaca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ê–≤—Ç–æ—Ä—ã –ê–ª—å–ø–∞–∫–∏ –¥–æ–æ–±—É—á–∏–ª–∏ –º–æ–¥–µ–ª—å LLaMA (7 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤) –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π, –∫–æ—Ç–æ—Ä—ã–π –æ–Ω–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–∏ —Å –ø–æ–º–æ—â—å—é OpenAI API, –∏ –ø–æ–ª—É—á–∏–ª–∞—Å—å –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–∞ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –Ω–∞ —Å–∞–º—É –º–æ–¥–µ–ª—å –æ—Ç OpenAI.\n",
    "\n",
    "–î–∞—Ç–∞—Å–µ—Ç –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π Alpaca —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç—å–∏:\n",
    "\n",
    "[[paper] üéì Self-Instruct: Aligning Language Models with Self-Generated Instructions (2022)](https://arxiv.org/abs/2212.10560)\n",
    "\n",
    "\n",
    "\n",
    "–ò –∫–∞–∫ –æ–Ω–∏ –≥–æ–≤–æ—Ä—è—Ç —É –Ω–∏—Ö —É—à–ª–æ –æ–∫–æ–ª–æ 500$ –Ω–∞ –≤—Å–µ, —á—Ç–æ –≤ —Ç—ã—Å—è—á–∏ —Ä–∞–∑ –¥–µ—à–µ–≤–ª–µ —Ç–æ–≥–æ, —á—Ç–æ, –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, –ø–æ—Ç—Ä–∞—Ç–∏–ª —Å–∞–º OpenAI –Ω–∞ —Å–≤–æ–∏ –º–æ–¥–µ–ª–∏. –ù–æ OpenAI –∑–∞–ø—Ä–µ—â–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–≤–æ–∏ –º–æ–¥–µ–ª–∏ –≤ —Ç–∞–∫–∏—Ö —Ü–µ–ª—è—Ö –∏ –ø–æ—ç—Ç–æ–º—É –∏—Ç–æ–≥–æ–≤—É—é –º–æ–¥–µ–ª—å Alpaca –æ–Ω–∏ –ø–æ–∫–∞ –Ω–µ –≤—ã–∫–ª–∞–¥—ã–≤–∞—é—Ç.\n",
    "\n",
    "–ù–æ –æ–Ω–∏ –≤—ã–ª–æ–∂–∏–ª–∏ –≤ –æ—Ç–∫—Ä—ã—Ç—ã–π –¥–æ—Å—Ç—É–ø –¥–∞—Ç–∞—Å–µ—Ç –∏ –º–æ–∂–Ω–æ —Å–∞–º–æ–º—É –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –¥–æ–æ–±—É—á–∏—Ç—å –∫–∞–∫—É—é-—Ç–æ –æ—Ç–∫—Ä—ã—Ç—É—é –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å. –ö–æ–º–º–µ—Ä—á–µ—Å–∫–∞—è –ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å —Ç–∞–∫–æ–π –º–æ–¥–µ–ª–∏, –æ–¥–Ω–∞–∫–æ, –≤—Å–µ –µ—â–µ –ø–æ–¥ –≤–æ–ø—Ä–æ—Å–æ–º."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–∫–∞—á–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, Sequence\n",
    "import json\n",
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import Trainer\n",
    "\n",
    "# import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_alpaca = json.load(open('alpaca_data.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_alpaca[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í –Ω–µ–º –∫–∞–∂–¥—ã–π –ø—Ä–∏–º–µ—Ä —ç—Ç–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è, –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –æ—Ç–≤–µ—Ç.\n",
    "–î–ª—è –º–æ–¥–µ–ª–∏ —ç—Ç–∏ –ø—Ä–∏–º–µ—Ä—ã –µ—â–µ –æ–±–æ—Ä–∞—á–∏–≤–∞—é—Ç—Å—è –≤ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç, –∫–æ—Ç–æ—Ä—ã–π –≥–æ–≤–æ—Ä–∏—Ç –º–æ–¥–µ–ª–∏, —á—Ç–æ –æ–Ω–∞ –¥–æ–ª–∂–Ω–∞ —Å–ª–µ–¥–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"</s>\"\n",
    "DEFAULT_UNK_TOKEN = \"</s>\"\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "    ),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–∞–≤–∞–π—Ç–µ –ø–æ–ø—Ä–æ–±—É–µ–º –¥–æ–æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –æ—Ç facebook - opt (–æ–Ω–∞ –æ—Ç–∫—Ä—ã—Ç—ã—è –∏ —É—Å—Ç—Ä–æ–µ–Ω–∞ –∫–∞–∫ LLama –∏ GPT - —ç—Ç–æ –¥–µ–∫–æ–¥–µ—Ä –æ–Ω–ª–∏ –º–æ–¥–µ–ª—å)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tokenize_fn(strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "    \"\"\"Tokenize a list of strings.\"\"\"\n",
    "    tokenized_list = [\n",
    "        tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "        )\n",
    "        for text in strings\n",
    "    ]\n",
    "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "    input_ids_lens = labels_lens = [\n",
    "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "    ]\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=labels,\n",
    "        input_ids_lens=input_ids_lens,\n",
    "        labels_lens=labels_lens,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(\n",
    "    sources: Sequence[str],\n",
    "    targets: Sequence[str],\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    ") -> Dict:\n",
    "    \"\"\"Preprocess the data by tokenizing.\"\"\"\n",
    "    examples = [s + t for s, t in zip(sources, targets)]\n",
    "    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in (examples, sources)]\n",
    "    input_ids = examples_tokenized[\"input_ids\"]\n",
    "    labels = copy.deepcopy(input_ids)\n",
    "    for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "        label[:source_len] = IGNORE_INDEX\n",
    "    return dict(input_ids=input_ids, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–∞–ª–µ–µ —ç—Ç–æ –æ–±–æ—Ä–∞—á–∏–≤–∞–µ—Ç—Å—è –∫ –∫–ª–∞—Å—Å—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç –¥–∞–Ω–Ω—ã–µ –∫ —Ñ–æ—Ä–º–∞—Ç—É huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str, tokenizer: transformers.PreTrainedTokenizer):\n",
    "        super(SupervisedDataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "        list_data_dict = json.load(open(data_path))\n",
    "\n",
    "        logging.warning(\"Formatting inputs...\")\n",
    "        prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]\n",
    "        sources = [\n",
    "            prompt_input.format_map(example) if example.get(\"input\", \"\") != \"\" else prompt_no_input.format_map(example)\n",
    "            for example in list_data_dict\n",
    "        ]\n",
    "        targets = [f\"{example['output']}{tokenizer.eos_token}\" for example in list_data_dict]\n",
    "\n",
    "        logging.warning(\"Tokenizing inputs... This may take some time...\")\n",
    "        data_dict = preprocess(sources, targets, tokenizer)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'facebook/opt-350m'\n",
    "model_name = \"facebook/opt-125m\"\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        max_length=512,\n",
    "        cache_dir=\"huggingface_cache\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=\"huggingface_cache\",\n",
    "    model_max_length=512,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=\"alpaca_data.json\")\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ó–∞–¥–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = transformers.TrainingArguments(learning_rate=1e-5,\n",
    "                 num_train_epochs=1,\n",
    "                 per_device_train_batch_size=2,\n",
    "                 gradient_accumulation_steps=1,\n",
    "                 evaluation_strategy='no',\n",
    "                 weight_decay=0.,\n",
    "                 warmup_ratio=0.03,\n",
    "                 lr_scheduler_type=\"cosine\",\n",
    "                 save_strategy='no',\n",
    "                 logging_steps=1000,\n",
    "                 output_dir=\"opt125_instruct_ft\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –≤–≤–µ—Å—Ç–∏ api key. –í —Ü–µ–ª—è—Ö –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –æ–Ω –Ω–µ –≤–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –≤ –±–ª–æ–∫–Ω–æ—Ç, –Ω–æ –µ–≥–æ –ø–æ–ª—É—á–∏—Ç—å –¥–æ–≤–æ–ª—å–Ω–æ –ø—Ä–æ—Å—Ç–æ –ø–æ —Å—Å—ã–ª–∫–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                 tokenizer=tokenizer,\n",
    "                 args=train_args,\n",
    "                 train_dataset=train_dataset,\n",
    "                 eval_dataset=None,\n",
    "                 data_collator=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–æ—Ö—Ä–∞–Ω–∏–º –º–æ–¥–µ–ª—å."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('opt125_ft_02')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò –ø–æ–ø—Ä–æ–±—É–µ–º –µ–µ –Ω–∞ —Ç–æ–º –∂–µ —Ç–µ–∫—Å—Ç–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'opt125_ft_02'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, model_max_length=512, max_length=512)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_instruction(instruction, text, model):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    prompt = (\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "              \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "              f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{text}\\n\\n### Response:\")\n",
    "\n",
    "    inputs = tokenizer([prompt],\n",
    "                        return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    output_sequences = model.generate(\n",
    "        # this parameters are also important but you can read about them in the docs and just try changing them\n",
    "        num_beams=1,\n",
    "#         temperature=0.4,\n",
    "#         max_length=100,\n",
    "        max_new_tokens=20,\n",
    "#         no_repeat_ngram_size=3,\n",
    "    #     repetition_penalty= 5.0,\n",
    "    #     length_penalty=0.01,\n",
    "    #     early_stopping=True,\n",
    "    #     do_sample=True,\n",
    "    #     top_k=30,\n",
    "    #     top_p=0.8,\n",
    "        early_stopping=True,\n",
    "    #     num_return_sequences=3,\n",
    "        num_return_sequences= 1,\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        do_sample=False,  # disable sampling to test if batching affects output\n",
    "    )\n",
    "    summaries = tokenizer.batch_decode(output_sequences[:,len(inputs[0]):], skip_special_tokens=True)\n",
    "    return summaries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Badgers burrowing under rail tracks have halted trains in the northern and southern Netherlands, forcing lengthy cancellations on at least two lines.\n",
    "All trains were halted Tuesday afternoon on a busy line between the southern cities of Den Bosch and Boxtel after the animals dug into a dike carrying rails. The national railway company said the line would be out of service for at least a week.\n",
    "The digging means \"the rails can subside and then the safety of train traffic can no longer be guaranteed,\" ProRail, the company that maintains the Dutch rail network said in a statement.\n",
    "Earlier this month, badgers also burrowed under tracks near the northern village of Molkwerum in Friesland province, knocking a line out of service until next month while workers seek permission to shift the animals.\n",
    "Badgers are protected animals in the Netherlands, so rail operators have to get permission to move them or disturb their habitat before repairs can begin.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Give a summary of this text.\"\n",
    "predict_for_instruction(instruction, text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Give a very short summary of this text.\"\n",
    "predict_for_instruction(instruction, text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Write a headline for the following text.\"\n",
    "predict_for_instruction(instruction, text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Suggest a headline for this text.\"\n",
    "predict_for_instruction(instruction, text, model)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
