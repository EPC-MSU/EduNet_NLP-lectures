{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">–ú–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–µ–∫–æ–¥–µ—Ä–∞ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —ç–Ω–∫–æ–¥–µ—Ä–∞ –∏ –¥–µ–∫–æ–¥–µ—Ä–∞.\n",
    "\n",
    "–ù–∞ –æ—Å–Ω–æ–≤–µ —ç–Ω–∫–æ–¥–µ—Ä–∞ –±—ã–ª–∞ —Å–æ–∑–¥–∞–Ω–∞ –º–æ–¥–µ–ª—å BERT –∏ –ø–æ–¥–æ–±–Ω—ã–µ –µ–π (RoBERTa, ALBERT). –û–Ω–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —Å–ª–æ–≤ –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π. –ó–∞—Ç–µ–º –º–æ–¥–µ–ª–∏ –¥–æ–æ–±—É—á–∞—é—Ç—Å—è –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–¥–∞—á–∏, —á—Ç–æ —Ç—Ä–µ–±—É–µ—Ç –Ω–∞–ª–∏—á–∏—è —Ä–∞–∑–º–µ—á–µ–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö.\n",
    "\n",
    "–ù–∞ –æ—Å–Ω–æ–≤–µ —ç–Ω–∫–æ–¥–µ—Ä–∞ –±—ã–ª–∞ —Å–æ–∑–¥–∞–Ω–∞ –º–æ–¥–µ–ª—å GPT ‚Äî Generative Pretrained Transformer. –ú–æ–¥–µ–ª—å GPT –±—ã–ª–∞ –æ–±—É—á–µ–Ω–∞ –Ω–∞ –∑–∞–¥–∞—á–µ —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è, —Ç–æ –µ—Å—Ç—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏) —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–ª–æ–≤–∞ –≤ —Ç–µ–∫—Å—Ç–µ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ß–µ—Ç—ã—Ä–µ –ø–æ–∫–æ–ª–µ–Ω–∏—è GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é GPT –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Å–ª–µ–¥—É—é—â–µ–µ:\n",
    "\n",
    "1. –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç—Å—è –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —á–∏—Å–µ–ª (—Ç–æ–∫–µ–Ω–æ–≤).\n",
    "2. –°–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤ –ø—Ä–æ—Ö–æ–¥–∏—Ç —á–µ—Ä–µ–∑ Embedding layer (–ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π) –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç—Å—è –≤ —Å–ø–∏—Å–æ–∫ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.\n",
    "3. –ö –∫–∞–∂–¥–æ–º—É —ç–º–±–µ–¥–¥–∏–Ω–≥—É –ø—Ä–∏–±–∞–≤–ª—è–µ—Ç—Å—è **positional embedding**.\n",
    "4. –°–ø–∏—Å–æ–∫ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –ø—Ä–æ—Ö–æ–¥–∏—Ç —á–µ—Ä–µ–∑ –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö –±–ª–æ–∫–æ–≤ (Transformer Decoder Block).\n",
    "5. –ü–æ—Å–ª–µ —Ç–æ–≥–æ, –∫–∞–∫ —Å–ø–∏—Å–æ–∫ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –ø—Ä–æ–π–¥—ë—Ç —á–µ—Ä–µ–∑ –ø–æ—Å–ª–µ–¥–Ω–∏–π –±–ª–æ–∫, —ç–º–±–µ–¥–¥–∏–Ω–≥, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –ø–æ—Å–ª–µ–¥–Ω–µ–º—É —Ç–æ–∫–µ–Ω—É, –º–∞—Ç—Ä–∏—á–Ω–æ —É–º–Ω–æ–∂–∞–µ—Ç—Å—è –Ω–∞ –≤—Å—ë —Ç–æ—Ç –∂–µ –≤—Ö–æ–¥–Ω–æ–π, –Ω–æ —É–∂–µ —Ç—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π Embedding Layer, –∏ –ø–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è SoftMax –ø–æ–ª—É—á–∞–µ—Ç—Å—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞.\n",
    "6. –ò–∑ —ç—Ç–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è —Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å –ø–æ–º–æ—â—å—é argmax).\n",
    "7. –ü–æ–ª—É—á–µ–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –∫ –≤—Ö–æ–¥–Ω–æ–º—É —Å–ø–∏—Å–∫—É —Ç–æ–∫–µ–Ω–æ–≤, —à–∞–≥–∏ 1‚Äì6 –ø–æ–≤—Ç–æ—Ä—è—é—Ç—Å—è."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L07/gpt3.gif\" width=\"800\"></center>\n",
    "\n",
    "<center><em>–ò—Å—Ç–æ—á–Ω–∏–∫: <a href=\"https://jalammar.github.io/how-gpt3-works-visualizations-animations/\">How GPT3 Works ‚Äî Visualizations and Animations</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[paper] üéì Improving Language Understanding by Generative Pre-Training (2018)](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)\n",
    "\n",
    "- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: 117 –º–∏–ª–ª–∏–æ–Ω–æ–≤.\n",
    "- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤: 12.\n",
    "- –†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö: 5 –ì–ë.\n",
    "- –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: 512 —Ç–æ–∫–µ–Ω–æ–≤.\n",
    "\n",
    "–ü–µ—Ä–≤–∞—è –≤–µ—Ä—Å–∏—è —ç—Ç–æ–π –º–æ–¥–µ–ª–∏ –±—ã–ª–∞ –æ–±—É—á–µ–Ω–∞ –Ω–∞ 7000 –∫–Ω–∏–≥.\n",
    "\n",
    "–ö–∞–∫ —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –æ–Ω–∞ —Ä–∞–±–æ—Ç–∞–ª–∞ –Ω–µ –æ—á–µ–Ω—å —Ö–æ—Ä–æ—à–æ, –Ω–æ –ø—Ä–∏ –¥–æ–æ–±—É—á–µ–Ω–∏–∏ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –ø–æ–∫–∞–∑—ã–≤–∞–ª–∞ –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ.\n",
    "\n",
    "–û–¥–Ω–∞–∫–æ SOTA —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã GPT-1 –¥–µ—Ä–∂–∞–ª–∏—Å—å –Ω–µ–¥–æ–ª–≥–æ, —Ç–∞–∫ –∫–∞–∫ –ø–æ—è–≤–∏–ª—Å—è BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L07/gpt1.jpg\" width=\"800\"></center>\n",
    "\n",
    "<center><em>–ò—Å—Ç–æ—á–Ω–∏–∫: <a href=\"https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf/\">Improving Language Understanding by Generative Pre-Training</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[paper] üéì Language Models are Unsupervised Multitask Learners (2019)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "\n",
    "- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: 1.5 –º–∏–ª–ª–∏–∞—Ä–¥–∞.\n",
    "- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤: 48.\n",
    "- –†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö: 40 –ì–ë.\n",
    "- –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: 1024 —Ç–æ–∫–µ–Ω–æ–≤.\n",
    "\n",
    "\n",
    "–ù–æ–≤–∞—è GPT-2 –Ω–µ —Å–æ–¥–µ—Ä–∂–∞–ª–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã—Ö –Ω–æ–≤–æ–≤–≤–µ–¥–µ–Ω–∏–π, –æ—Å–Ω–æ–≤–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –¥–æ—Å—Ç–∏–≥–∞–ª–∏—Å—å –∑–∞ —Å—á–µ—Ç —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö (–∫ –∫–Ω–∏–≥–∞–º –¥–æ–±–∞–≤–∏–ª–∏ 8 –º–∏–ª–ª–∏–æ–Ω–æ–≤ —Å–∞–π—Ç–æ–≤).\n",
    "\n",
    "GPT-2 –Ω–∞—É—á–∏–ª–∞—Å—å –ø–∏—Å–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ —Å–≤—è–∑–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L07/gpt2.png\" width=\"800\"></center>\n",
    "\n",
    "<center><em>–ò—Å—Ç–æ—á–Ω–∏–∫: <a href=\"https://en.wikipedia.org/wiki/GPT-2\">GPT-2</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–∞–∫–∂–µ GPT-2 —É–∂–µ –º–æ–≥–ª–∞ –±–µ–∑ —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≤—ã–ø–æ–ª–Ω—è—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∑–∞–¥–∞—á–∏, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ –±—ã–ª–æ —è–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ù–∞–ø—Ä–∏–º–µ—Ä, –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–æ–ø–∏—Å–∞—Ç—å `TL;DR` –ø–æ—Å–ª–µ —Ç–µ–∫—Å—Ç–∞."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i.postimg.cc/nL19wfYL/tldr.jpg\" width=\"850\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[paper] üéì Language Models are Few-Shot Learners (2020)](https://arxiv.org/pdf/2005.14165.pdf)\n",
    "\n",
    "- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: 175 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤.\n",
    "- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤: 96.\n",
    "- –†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö: 45 –¢–ë (—Ç.–µ. 45 000 –ì–ë).\n",
    "- –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: 2048 —Ç–æ–∫–µ–Ω–æ–≤.\n",
    "\n",
    "GPT-3 –æ–±—É—á–µ–Ω–∞ –Ω–∞ –µ—â—ë –±–æ–ª—å—à–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –¥–∞–Ω–Ω—ã—Ö ‚Äî –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞, –≤—Å—è –∞–Ω–≥–ª–∏–π—Å–∫–∞—è –í–∏–∫–∏–ø–µ–¥–∏—è —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–æ–ª—å–∫–æ 0,6% –∏–∑ –Ω–∏—Ö.\n",
    "\n",
    "–ú–æ–¥–µ–ª—å –Ω–∞—É—á–∏–ª–∞—Å—å —Ä–µ—à–∞—Ç—å –º–Ω–æ–≥–æ NLP-–∑–∞–¥–∞—á –±–µ–∑ –¥–æ–æ–±—É—á–µ–Ω–∏—è."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <table >\n",
    "     <tr>\n",
    "       <td>\n",
    "       \n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L07/gpt3_gec.jpg\" width=\"440\"></center>\n",
    "\n",
    "<em>–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫</em>\n",
    "\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L07/gpt3_sa.jpg\" width=\"450\"></center>\n",
    "\n",
    "<em>–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏</em>\n",
    "\n",
    "\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L07/gpt3_mt.jpg\" width=\"500\"></center>\n",
    "\n",
    "<em>–ú–∞—à–∏–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥</em>\n",
    "\n",
    "\n",
    "</td>\n",
    "     </tr>\n",
    "    </table>\n",
    "    </div>\n",
    "\n",
    "<center><em>–ò—Å—Ç–æ—á–Ω–∏–∫: <a href=\"https://beta.openai.com/examples\">GPT-3 beta</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-3 —Ç–∞–∫–∂–µ —Å–ø–æ—Å–æ–±–Ω–∞ –ø–∏—Å–∞—Ç—å –∏ –ø–æ–Ω–∏–º–∞—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º–Ω—ã–π –∫–æ–¥."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <table >\n",
    "     <tr>\n",
    "       <td>\n",
    "       \n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L07/gpt3_code_promt.jpg\" width=\"300\"></center>\n",
    "\n",
    "<em>–ó–∞–ø—Ä–æ—Å</em>\n",
    "\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L07/gpt3_code_response.jpg\" width=\"400\"></center>\n",
    "\n",
    "<em>–ü—Ä–∏–º–µ—Ä –æ—Ç–≤–µ—Ç–∞</em>\n",
    "\n",
    "\n",
    "</td>\n",
    "     </tr>\n",
    "    </table>\n",
    "    </div>\n",
    "\n",
    "<center><em>–ò—Å—Ç–æ—á–Ω–∏–∫: <a href=\"https://beta.openai.com/examples\">GPT-3 beta</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°—Ç–∞—Ç—å—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç.\n",
    "\n",
    "- –ù–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø—Ä–æ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏.\n",
    "- –†–∞–±–æ—Ç–∞–µ—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –¥–æ 8192 —Ç–æ–∫–µ–Ω–æ–≤. –ï—Å—Ç—å –≤–µ—Ä—Å–∏—è, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∞—è 32 768 —Ç–æ–∫–µ–Ω–æ–≤ (50 —Å—Ç—Ä–∞–Ω–∏—Ü).\n",
    "\n",
    "–ü–æ–º–∏–º–æ —Ç–µ–∫—Å—Ç–æ–≤, —É–º–µ–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏, —è–≤–ª—è–µ—Ç—Å—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L07/gpt4.jpg\" width=\"550\"></center>\n",
    "\n",
    "<center><em>–ò—Å—Ç–æ—á–Ω–∏–∫: <a href=\"https://thecymes.com/article/the-game-changing-features-of-openais-chatgpt-4\">The game-changing features of OpenAI's GPT 4</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —Ä–∞–±–æ—Ç—É –º–æ–¥–µ–ª–∏ ruGPT-3 –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞. –û–Ω–∞ –¥–æ—Å—Ç—É–ø–Ω–∞ –≤ —Ç—Ä–µ—Ö –≤–∞—Ä–∏–∞–Ω—Ç–∞—Ö:\n",
    "- small: [ai-forever/rugpt3small_based_on_gpt2 üõ†Ô∏è[doc]](https://huggingface.co/ai-forever/rugpt3small_based_on_gpt2)\n",
    "- medium: [ai-forever/rugpt3medium_based_on_gpt2 üõ†Ô∏è[doc]](https://huggingface.co/ai-forever/rugpt3medium_based_on_gpt2)\n",
    "- large: [ai-forever/rugpt3large_based_on_gpt2 üõ†Ô∏è[doc]](https://huggingface.co/ai-forever/rugpt3large_based_on_gpt2)\n",
    "\n",
    "–î–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è –∫–ª–∞—Å—Å `AutoTokenizer` [üõ†Ô∏è[doc]](https://huggingface.co/docs/transformers/v4.45.2/en/model_doc/auto#transformers.AutoTokenizer), –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ ‚Äî –∫–ª–∞—Å—Å `AutoModelForCausalLM` [üõ†Ô∏è[doc]](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM).\n",
    "\n",
    "–í—ã–±–µ—Ä–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—É—é –º–æ–¥–µ–ª—å. API –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π, –¥–ª—è –ø–æ–¥–º–µ–Ω—ã –º–æ–¥–µ–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ `model_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"ai-forever/rugpt3large_based_on_gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ –º—ã —Ö–æ—Ç–∏–º –ø—Ä–∏ –ø–æ–º–æ—â–∏ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å: **¬´–°–∫–æ–ª—å–∫–æ –±—É–¥–µ—Ç 2+2?¬ª**, —Ç–æ –º–æ–∂–µ–º –ø–æ–¥–∞—Ç—å –Ω–∞ –≤—Ö–æ–¥ –º–æ–¥–µ–ª–∏ —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç:\\\n",
    "`¬´–í–æ–ø—Ä–æ—Å: –°–∫–æ–ª—å–∫–æ –±—É–¥–µ—Ç 2+2? –û—Ç–≤–µ—Ç: ‚Ä¶ ¬ª`\\\n",
    "–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ–º —Ç–∞–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –±—É–¥–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å, –ø–æ—ç—Ç–æ–º—É –º–æ–¥–µ–ª—å –¥–æ–ø–∏—à–µ—Ç `¬´4¬ª`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"–í–æ–ø—Ä–æ—Å: '–°–∫–æ–ª—å–∫–æ –±—É–¥–µ—Ç 2+2?'\\n–û—Ç–≤–µ—Ç:\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "out = model.generate(input_ids, do_sample=False, max_length=20, pad_token_id=20)\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–†–∞—Å—Å–º–æ—Ç—Ä–∏–º –ø—Ä–∏–º–µ—Ä —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏. –ö–∞–∫ –∏ BERT-like –º–æ–¥–µ–ª–∏, GPT –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –Ω–∞ –ø–æ–¥—Å–ª–æ–≤–∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"–¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–π –º–µ–Ω—è\" # raw text\n",
    "tokens = tokenizer.encode(text, add_special_tokens=False) # applying ruGPT-3 tokenizer\n",
    "decoded_tokens = [tokenizer.decode([token]) for token in tokens] # converting ids to tokens\n",
    "\n",
    "print(\"text:\", text)\n",
    "print(\"tokens: \", tokens)\n",
    "print(\"decoded tokens: \", decoded_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–Ø–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –≤—ã–¥–∞—ë—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞, —ç—Ç—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –º–æ–∂–Ω–æ –ø–æ-—Ä–∞–∑–Ω–æ–º—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞.\n",
    "\n",
    "–î–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏ –ø—Ä–∏–º–µ–Ω–∏–º –≤—Å–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è —Å–ª–µ–¥—É—é—â–µ–≥–æ: '–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: \"–Ø–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å\" - —ç—Ç–æ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "text = '–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: \"–Ø–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å\" ‚Äî —ç—Ç–æ'\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ñ–∞–¥–Ω—ã–π –ø–æ–∏—Å–∫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–∞–º—ã–π –ø—Ä–æ—Å—Ç–æ–π —Å–ø–æ—Å–æ–± ‚Äî –∂–∞–¥–Ω—ã–π –ø–æ–∏—Å–∫ (greedy search): –∫–∞–∂–¥—ã–π —Ä–∞–∑ –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è —Ç–æ–∫–µ–Ω, —É –∫–æ—Ç–æ—Ä–æ–≥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å.\n",
    "\n",
    "–ü—Ä–∏ —Ç–∞–∫–æ–º —Å–ø–æ—Å–æ–±–µ –º—ã –Ω–µ –ø–æ–ª—É—á–∏–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ –∑–∞–ø—Ä–æ—Å, –∏, —á—Ç–æ –µ—â—ë —Ö—É–∂–µ, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –º–æ–∂–µ—Ç –∑–∞—Å—Ç—Ä–µ–≤–∞—Ç—å –≤ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–∏–Ω–∏–º—É–º–∞—Ö –∏ –≤—ã–¥–∞–≤–∞—Ç—å –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã, –Ω–∞–ø—Ä–∏–º–µ—Ä `the the the the ...`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L07/greedy_seacrh.png\" width=\"500\"></center>\n",
    "\n",
    "<center><em>–ò—Å—Ç–æ—á–Ω–∏–∫: <a href=\"https://habr.com/ru/articles/589663/\">–ö—Ä–∞—Ç–∫–∏–π —ç–∫—Å–∫—É—Ä—Å –≤ ruGPT-3</a></em></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of greedy search\n",
    "out = model.generate(input_ids,\n",
    "                     do_sample=False,\n",
    "                     max_length=30)\n",
    "\n",
    "# Decoding\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –õ—É—á–µ–≤–æ–π –ø–æ–∏—Å–∫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "–ß—É—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–π –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Å–ø–æ—Å–æ–± ‚Äî —ç—Ç–æ –ª—É—á–µ–≤–æ–π –ø–æ–∏—Å–∫ (beam search). –ù–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è –Ω–µ —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —Å–∞–º—ã–π –≤–µ—Ä–æ—è—Ç–Ω—ã–π —Ç–æ–∫–µ–Ω, –∞ —Å—Ä–∞–∑—É –Ω–µ—Å–∫–æ–ª—å–∫–æ (–∑–∞ –∏—Ö –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—á–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä `num_beams`). –î–∞–ª—å—à–µ –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç—Å—è –ø–æ–∏—Å–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤. –ü—É—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–∞–∑–≤–µ—Ç–≤–ª—è—é—Ç—Å—è, —á—Ç–æ –¥–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.\n",
    "\n",
    "–û–±—ã—á–Ω–æ —ç—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –≤—ã—Å–æ–∫–æ–π —Å–≤—è–∑–Ω–æ—Å—Ç–∏ (–∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏), –Ω–æ –ø—Ä–∏ —ç—Ç–æ–º –∫ —Å—É—Ö–æ—Å—Ç–∏ –∏ —Å–∫—É—á–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞. –¢–∞–∫–∂–µ —ç—Ç–æ –Ω–µ —Ä–µ—à–∞–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—Ä–æ–±–ª–µ–º—É —Å –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è–º–∏ –∫—É—Å–æ—á–∫–æ–≤ —Ç–µ–∫—Å—Ç–∞."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L07/beam_search.png\" width=\"500\"></center>\n",
    "\n",
    "<center><em>–ò—Å—Ç–æ—á–Ω–∏–∫: <a href=\"https://habr.com/ru/articles/589663/\">–ö—Ä–∞—Ç–∫–∏–π —ç–∫—Å–∫—É—Ä—Å –≤ ruGPT-3</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–∞–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–±–ª–∞–¥–∞–µ—Ç —Ö–æ—Ä–æ—à–µ–π –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å—é (—Å–≤—è–∑–Ω–æ—Å—Ç—å—é), –Ω–æ –æ–±—ã—á–Ω–æ —É –Ω–∏—Ö –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç \"—á–µ–ª–æ–≤–µ—á–Ω–æ—Å—Ç–∏\", –æ–Ω–∏ –∫–∞–∂—É—Ç—Å—è —Å—É—Ö–∏–º–∏ –∏ —Å–∫—É—á–Ω—ã–º–∏. –¢–∞–∫–∂–µ —ç—Ç–æ –Ω–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of beam search\n",
    "out = model.generate(input_ids,\n",
    "                     do_sample=False,\n",
    "                     num_beams=5,\n",
    "                     max_length=40)\n",
    "\n",
    "# Decoding\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú–æ–∂–Ω–æ –≤—ã–≤–µ—Å—Ç–∏ –Ω–µ —Ç–æ–ª—å–∫–æ –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –Ω–æ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –¥—Ä—É–≥–∏–º –ª—É—á–∞–º."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set return_num_sequences > 1\n",
    "out = model.generate(input_ids,\n",
    "                     do_sample=False,\n",
    "                     num_beams=5,\n",
    "                     max_length=40,\n",
    "                     num_return_sequences=5)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(out):\n",
    "  print(\"{}: {}\\n\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–æ–≤ –º–æ–∂–Ω–æ —É–º–µ–Ω—å—à–∏—Ç—å, –µ—Å–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–∞–ø—Ä–µ—Ç –Ω–∞ –ø–æ–≤—Ç–æ—Ä *n*-–≥—Ä–∞–º–º."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can prohibit repeating bigrams\n",
    "out = model.generate(input_ids,\n",
    "                     do_sample=False,\n",
    "                     num_beams=5,\n",
    "                     max_length=40,\n",
    "                     num_return_sequences=5,\n",
    "                     no_repeat_ngram_size=2)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(out):\n",
    "  print(\"{}: {}\\n\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –°—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–æ–π"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ß—Ç–æ–±—ã –¥–æ–±–∞–≤–∏—Ç—å —Ç–µ–∫—Å—Ç—É –Ω–µ–ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ—Å—Ç–∏ –∏ —á–µ–ª–æ–≤–µ—á–Ω–æ—Å—Ç–∏, –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ–µ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–æ–π. –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –Ω–µ —Å–∞–º—ã–π –≤–µ—Ä–æ—è—Ç–Ω—ã–π —Ç–æ–∫–µ–Ω, –∞ —Å–ª—É—á–∞–π–Ω—ã–π, —Å —É—á—ë—Ç–æ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π.\n",
    "\n",
    "–ü–∞—Ä–∞–º–µ—Ç—Ä —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å —Å—Ç–µ–ø–µ–Ω—å —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏. –ü—Ä–∏ –Ω—É–ª–µ–≤–æ–π —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–µ –º–µ—Ç–æ–¥ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∂–∞–¥–Ω—ã–º —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ–º, –ø—Ä–∏  –±–æ–ª—å—à–æ–π —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–µ —Ç–æ–∫–µ–Ω—ã –±—É–¥—É—Ç –≤—ã–±–∏—Ä–∞—Ç—å—Å—è –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–ª—É—á–∞–π–Ω–æ. –û–±—ã—á–Ω–æ —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ `0.8‚Äì2.0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–§–æ—Ä–º—É–ª–∞ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–∞ –Ω–∞ —Ñ–æ—Ä–º—É–ª—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ë–æ–ª—å—Ü–º–∞–Ω–∞: —á–µ–º –≤—ã—à–µ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ —Å–∏—Å—Ç–µ–º—ã, —Ç–µ–º –±–æ–ª—å—à–µ \"—Ä–∞–∑–º–∞–∑—ã–≤–∞–µ—Ç—Å—è\" —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –µ—ë –≤–æ–∑–º–æ–∂–Ω—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π, –æ—Ç—Å—é–¥–∞ —Å–ª–æ–≤–æ \"—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞\".\n",
    "\n",
    "$$\\large p=\\text{softmax}(\\log(p)/t)$$\n",
    "\n",
    "–°—Ç–æ–∏—Ç –æ—Ç–º–µ—Ç–∏—Ç—å, —á—Ç–æ —Å–ª—É—á–∞–π–Ω–∞—è –ø—Ä–∏—Ä–æ–¥–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±—É–¥–µ—Ç –∏–Ω–æ–≥–¥–∞ –ø—Ä–∏–≤–æ–¥–∏—Ç—å –∫ –ø–æ–ª–Ω–æ—Å—Ç—å—é –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling eith temperature\n",
    "out = model.generate(input_ids, do_sample=True, temperature=1.3, max_length=30)\n",
    "\n",
    "# Decoding\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –°—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ top-k –∏ top-p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–ª—è –∑–∞–ø—Ä–µ—Ç–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–∞–ª–æ–≤–µ—Ä–æ—è—Ç–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤–≤–æ–¥—è—Ç `top-k` –∏–ª–∏ `top-p` –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è. –í —ç—Ç–æ–º —Å–ª—É—á–∞–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–æ–∂–µ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Å–ª—É—á–∞–π–Ω—ã–º –æ–±—Ä–∞–∑–æ–º, –Ω–æ –∑–∞—Ä–∞–Ω–µ–µ –æ—Ç—Å–µ–∫–∞—é—Ç—Å—è –≤—Å–µ –º–∞–ª–æ–≤–µ—Ä–æ—è—Ç–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í —Å–ª—É—á–∞–µ `top-k` –∑–∞–Ω—É–ª—è—é—Ç—Å—è –≤—Å–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∫—Ä–æ–º–µ `k` —Å–∞–º—ã—Ö –≤–µ—Ä–æ—è—Ç–Ω—ã—Ö.\n",
    "\n",
    "–£—Å—Ç–∞–Ω–æ–≤–∏–≤ $k=6$, –º—ã –±—É–¥–µ–º –≤—ã–±–∏—Ä–∞—Ç—å —Å–ª–µ–¥—É—é—â–µ–µ —Å–ª–æ–≤–æ —Ç–æ–ª—å–∫–æ –∏–∑ 6 —Å–ª–æ–≤ —Å –Ω–∞–∏–±–æ–ª—å—à–µ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é:\n",
    "- *nice, dog, car, woman, guy, man* –Ω–∞ 1-–º —à–∞–≥–µ\n",
    "- *drives, is, turns, stops, down, a* –Ω–∞ 2-–º —à–∞–≥–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L07/top-k.png\" width=\"800\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í —Å–ª—É—á–∞–µ `top-p` –æ—Å—Ç–∞–µ—Ç—Å—è –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä —Ç–æ–∫–µ–Ω–æ–≤, —á—Ç–æ–±—ã —Å—É–º–º–∞ –∏—Ö –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –±—ã–ª–∞ –Ω–µ –±–æ–ª—å—à–µ `p`. –°–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤, –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö –º—ã –≤—ã–±–∏—Ä–∞–µ–º, –º–æ–∂–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –º–µ–Ω—è—Ç—å—Å—è (—É–≤–µ–ª–∏—á–∏–≤–∞—Ç—å—Å—è –∏ —É–º–µ–Ω—å—à–∞—Ç—å—Å—è).\n",
    "\n",
    "–£—Å—Ç–∞–Ω–æ–≤–∏–º $p=0.92$ –∏ –±—É–¥–µ–º –≤—ã–±–∏—Ä–∞—Ç—å –∏–∑:\n",
    "- 9 —Å–ª–æ–≤ –Ω–∞ 1-–º —à–∞–≥–µ\n",
    "- 3 —Å–ª–æ–≤ –Ω–∞ 2-–º —à–∞–≥–µ\n",
    "\n",
    "–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ `top-p` –Ω–∞–∑—ã–≤–∞—é—Ç —è–¥–µ—Ä–Ω—ã–º —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ–º (nucleus sampling)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L07/top-p.png\" width=\"700\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random state\n",
    "torch.random.manual_seed(42)\n",
    "\n",
    "# Sampling with top-k and top-p restrictions\n",
    "out = model.generate(input_ids,\n",
    "                     do_sample=True,\n",
    "                     temperature=1.0,\n",
    "                     top_k=20,\n",
    "                     top_p=0.9,\n",
    "                     max_length=30,\n",
    "                    )\n",
    "# Decoding\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ó–∞—Ç—Ä–∞–≤–æ—á–Ω–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í –æ—Ç–ª–∏—á–∏–µ –æ—Ç BERT-like –º–æ–¥–µ–ª–µ–π, –¥–ª—è GPT —ç—Ç–∞–ø –¥–æ–æ–±—É—á–µ–Ω–∏—è –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω. –î–ª—è —Ç–æ–≥–æ —á—Ç–æ–±—ã –¥–æ–ø–∏—Å—ã–≤–∞—Ç—å —Ç–µ–∫—Å—Ç, –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –æ—á–µ–Ω—å —Ö–æ—Ä–æ—à–æ –ø–æ–Ω–∏–º–∞—Ç—å –µ–≥–æ —Å–º—ã—Å–ª –∏ –∏–º–µ—Ç—å –∑–Ω–∞–Ω–∏—è –æ —Ä–µ–∞–ª—å–Ω–æ–º –º–∏—Ä–µ. –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –∑–Ω–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –º–æ–∂–Ω–æ –ø–æ–ø—ã—Ç–∞—Ç—å—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å, –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É—è –ª–µ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ç–µ–∫—Å—Ç–∞. –ü–æ—Å–∫–æ–ª—å–∫—É GPT —Å–ø–æ—Å–æ–±–Ω–∞ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç, –º—ã –º–æ–∂–µ–º —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –ª—é–±—É—é NLP-–∑–∞–¥–∞—á—É –∫–∞–∫ –∑–∞–¥–∞—á—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è) —Ç–µ–∫—Å—Ç–∞. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ä–µ—à–∞—Ç—å –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∑–∞–¥–∞—á —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º –∏–ª–∏ –±–µ–∑ –Ω–µ–≥–æ.\n",
    "\n",
    "–ü–æ–¥–±–æ—Ä –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–π —Ç–µ–∫—Å—Ç–∞ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è \"–∑–∞—Ç—Ä–∞–≤–æ—á–Ω–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ\" (prompt engineering). –°—É—Ç—å –∑–∞—Ç—Ä–∞–≤–æ—á–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –≤ —Ç–æ–º, —á—Ç–æ–±—ã –ø–æ–¥–æ–±—Ä–∞—Ç—å —Ç–∞–∫–∏–µ –∑–∞–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∏ –Ω–∞—á–∞–ª–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, —á—Ç–æ–±—ã –¥–∞–ª—å—à–µ –º–æ–¥–µ–ª—å –≤—ã–¥–∞–≤–∞–ª–∞ —Ä–æ–≤–Ω–æ —Ç–æ, —á—Ç–æ –Ω–∞–¥–æ. –ü–æ–¥–±–∏—Ä–∞—è \"–∑–∞—Ç—Ä–∞–≤–∫–∏\" –∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ (—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞, `top-k`, `top-p`), –º–æ–∂–Ω–æ –¥–æ–±–∏—Ç—å—Å—è —Ö–æ—Ä–æ—à–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤ —Ç–æ–ª—å–∫–æ –∑–∞ —Å—á–µ—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.\n",
    "\n",
    "–°—É—â–µ—Å—Ç–≤—É–µ—Ç –¥–≤–∞ –ø–æ–¥—Ö–æ–¥–∞ –¥–ª—è –∑–∞—Ç—Ä–∞–≤–æ—á–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è:\n",
    "- zero-shot: —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞–¥–∞—á—É –∏ –ø–µ—Ä–µ–¥–∞—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é;\n",
    "- few-shot: —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞–¥–∞—á—É, –ø–æ–¥–∞—Ç—å –≤ –º–æ–¥–µ–ª—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –∏ –∑–∞—Ç–µ–º –ø–µ—Ä–µ–¥–∞—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"ai-forever/rugpt3large_based_on_gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero-shot learning\n",
    "text = \"–° –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –Ω–∞ —Ä—É—Å—Å–∫–∏–π 'cat' –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—Å—è –∫–∞–∫ '\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "out = model.generate(input_ids, do_sample=False,max_length=15)\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# few-shot learning\n",
    "text = \"–° –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –Ω–∞ —Ä—É—Å—Å–∫–∏–π 'cat' –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—Å—è –∫–∞–∫ '–∫–æ—à–∫–∞', 'dog' –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—Å—è –∫–∞–∫ '—Å–æ–±–∞–∫–∞', –∞ 'bird' –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—Å—è –∫–∞–∫ '\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "out = model.generate(input_ids, do_sample=False,max_length=35)\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–¥–µ–ª–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—É—é —Å–∏—Å—Ç–µ–º—É –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è: –±—É–¥–µ–º –ø–æ–¥–∞–≤–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –≤–∏–¥–µ —Ç–µ–∫—Å—Ç–∞ –∏ —Å–º–æ—Ç—Ä–µ—Ç—å, –∫–∞–∫ –º–æ–¥–µ–ª—å –ø—Ä–æ–¥–æ–ª–∂–∏—Ç –µ–≥–æ. –î–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –±—É–¥–µ–º –ø–æ–¥–∞–≤–∞—Ç—å —Ç–µ–∫—Å—Ç —Å –Ω–µ–∑–∞–∫—Ä—ã—Ç–æ–π –∫–∞–≤—ã—á–∫–æ–π, —á—Ç–æ–±—ã –≤—ã–Ω—É–¥–∏—Ç—å –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏–º–µ–Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(1)\n",
    "\n",
    "# Film recomendation\n",
    "text = \"–Ø –ª—é–±–ª—é —Å–æ–≤–µ—Ç—Å–∫–∏–µ –∫–æ–º–µ–¥–∏–∏: ‚Äú–ü—Ä–∏–∫–ª—é—á–µ–Ω–∏—è –®—É—Ä–∏–∫–∞‚Äù, ‚Äú\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "out = model.generate(input_ids,\n",
    "                     do_sample=True,\n",
    "                     temperature=1.4,\n",
    "                     top_k=20,\n",
    "                     top_p=0.8,\n",
    "                     max_length=46,\n",
    "                    )\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(1)\n",
    "\n",
    "# Music recomendation\n",
    "text = \"–¢–µ–º –ª—é–¥—è–º, –∫–æ–º—É –Ω—Ä–∞–≤–∏—Ç—Å—è ‚Äú–ê–ª–∏—Å–∞‚Äù, —Ç–∞–∫–∂–µ –ø–æ–Ω—Ä–∞–≤—è—Ç—Å—è –≥—Ä—É–ø–ø—ã ‚Äú\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "out = model.generate(input_ids,\n",
    "                     do_sample=True,\n",
    "                     temperature=1.1,\n",
    "                     top_k=10,\n",
    "                     top_p=0.7,\n",
    "                     max_length=35,\n",
    "                    )\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–†–∞—Å—Å–º–æ—Ç—Ä–∏–º –±–∏–Ω–∞—Ä–Ω—É—é –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –ø–æ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –±–µ–∑ –¥–æ–æ–±—É—á–µ–Ω–∏—è.\n",
    "\n",
    "–î–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ—Ä—É –ø–µ—Ä–ø–ª–µ–∫—Å–∏–∏. –í —Ç–µ–æ—Ä–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –ø–µ—Ä–ø–ª–µ–∫—Å–∏—è ‚Äì –º–µ—Ä–∞ —Ç–æ–≥–æ, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –≤—ã–±–æ—Ä–∫—É. –ì–æ–≤–æ—Ä—è –ø—Ä–æ—Å—Ç—ã–º —è–∑—ã–∫–æ–º, –ø–µ—Ä–ø–ª–µ–∫—Å–∏—è ‚Äì –º–µ—Ä–∞ \"—É–¥–∏–≤–ª—ë–Ω–Ω–æ—Å—Ç–∏\" –º–æ–¥–µ–ª–∏.\n",
    "\n",
    "–ù–∞ –≤—Ö–æ–¥ –º–æ–¥–µ–ª–∏ –º—ã –ø–æ–¥–∞—ë–º –∑–∞—Ç—Ä–∞–≤–∫—É –≤ —Ñ–æ—Ä–º–∞—Ç–µ <–º–µ—Ç–∫–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏> + <–æ—Ç–∑—ã–≤>. –î–∞–ª–µ–µ –º—ã —Å—á–∏—Ç–∞–µ–º –ø–µ—Ä–ø–ª–µ–∫—Å–∏—é –∫–∞–∂–¥–æ–≥–æ –∏–∑ –Ω–∏—Ö. –í—ã–±–∏—Ä–∞–µ–º –Ω–∞–∏–º–µ–Ω—å—à—É—é –ø–µ—Ä–ø–ª–µ–∫—Å–∏—é –∏–∑ –¥–≤—É—Ö –∏ –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º –º–µ—Ç–∫—É –æ—Ç–∑—ã–≤—É."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_perplexity(sentence, model, tokenizer):\n",
    "    # Add tone lable to sentence\n",
    "    sentence_positive = '–¥–æ–≤–æ–ª–µ–Ω:'+sentence\n",
    "    sentence_negative = '–Ω–µ–¥–æ–≤–æ–ª–µ–Ω:'+sentence\n",
    "    list_sent = [sentence_positive, sentence_negative]\n",
    "    ppl_values = []\n",
    "\n",
    "    for sentence in list_sent:\n",
    "      # Tokenize sentence\n",
    "      encodings = tokenizer(sentence, return_tensors='pt')\n",
    "      input_ids = encodings.input_ids.to(device)\n",
    "      # Apply model\n",
    "      with torch.no_grad():\n",
    "          outputs = model(input_ids=input_ids, labels=input_ids)\n",
    "      loss = outputs.loss\n",
    "      # Count perplexity\n",
    "      ppl = math.exp(loss.item() * input_ids.size(1))\n",
    "      ppl_values.append(ppl)\n",
    "\n",
    "    # Choose sentence with lower perplexity\n",
    "    if ppl_values[0] > ppl_values[1]:\n",
    "      return '–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π'\n",
    "    else:\n",
    "      return '–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = '—è —á—É—Ç—å –Ω–µ –∑–∞—Å–Ω—É–ª –≤–æ –≤—Ä–µ–º—è —Ñ–∏–ª—å–º–∞'\n",
    "print(f'–û—Ç–∑—ã–≤ \"{negative}\": {calculate_perplexity(negative, model, tokenizer)}')\n",
    "positive = '—Å—é–∂–µ—Ç –Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–æ –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–π'\n",
    "print(f'–û—Ç–∑—ã–≤ \"{positive}\": {calculate_perplexity(positive, model, tokenizer)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –§–∞–π–Ω—Ç—é–Ω–∏–Ω–≥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è –º–æ–¥–µ–ª—å—é –º–µ–Ω—å—à–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞, —á—Ç–æ–±—ã –æ–Ω–∞ –ø–æ–º–µ—Å—Ç–∏–ª–∞—Å—å –Ω–∞ GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"ai-forever/rugpt3small_based_on_gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–±—É—á–∞—é—â–∏–π —Ç–µ–∫—Å—Ç –Ω–∞—Ä–µ–∑–∞–µ—Ç—Å—è –Ω–∞ —Å–ª—É—á–∞–π–Ω—ã–µ –±–ª–æ–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ—Å—Ç–∞–≤–ª—è—é—Ç—Å—è –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–∑ 1024 (2048 —É GPT-3) —Ç–æ–∫–µ–Ω–æ–≤, —Ä–∞–∑–¥–µ–ª—è—è—Å—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º `<|endoftext|>` —Å–∏–º–≤–æ–ª–æ–º. –í–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å (–∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å) –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –æ–¥–∏–Ω –∑–∞ –¥—Ä—É–≥–∏–º –ø—Ä–∏ –ø–æ–º–æ—â–∏ Cross-Entropy Loss.\n",
    "\n",
    "–¢–∞–∫ –∫–∞–∫ –≤—Ö–æ–¥–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤—Å–µ–≥–¥–∞ –∑–∞–ø–æ–ª–Ω–µ–Ω–∞ –¥–æ –∫–æ–Ω—Ü–∞, padding –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è. –ù–æ –≤–æ –≤—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –¥–ª–∏–Ω–∞ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–π, –ø–æ—ç—Ç–æ–º—É –Ω–∞–¥–æ —è–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞—Ç—å, —á–µ–º –¥–æ–ø–æ–ª–Ω—è—Ç—å –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –ø–æ–∑–∏—Ü–∏–∏. –ü–æ –¥–µ—Ñ–æ–ª—Ç—É –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ—Ç –∂–µ `<|endoftext|>`.\n",
    "\n",
    "–í –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –≤–µ—Ä—Å–∏—è—Ö GPT –≤—ã—à–µ—Å–∫–∞–∑–∞–Ω–Ω–æ–µ –º–æ–∂–µ—Ç –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å—Å—è. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ ruGPT3 –≥–æ—Ä–∞–∑–¥–æ –±–æ–ª—å—à–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤: `<s\\>`, `<s>`, `<pad>`, `<unk>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–û–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ**\n",
    "\n",
    "–ë—É–¥–µ–º —É—á–∏—Ç—å GPT –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å—Ç–∏—Ö–∏ –ú–∞—è–∫–æ–≤—Å–∫–æ–≥–æ. –í –∫–∞—á–µ—Å—Ç–≤–µ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –≤–æ–∑—å–º—ë–º –≤—Å–µ–≥–æ –ª–∏—à—å –æ–¥–∏–Ω —Å—Ç–∏—Ö."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"–î—ã–º —Ç–∞–±–∞—á–Ω—ã–π –≤–æ–∑–¥—É—Ö –≤—ã–µ–ª.\n",
    "–ö–æ–º–Ω–∞—Ç–∞ ‚Äî\n",
    "–≥–ª–∞–≤–∞ –≤ –∫—Ä—É—á–µ–Ω—ã—Ö–æ–≤—Å–∫–æ–º –∞–¥–µ.\n",
    "–í—Å–ø–æ–º–Ω–∏ ‚Äî\n",
    "–∑–∞ —ç—Ç–∏–º –æ–∫–Ω–æ–º\n",
    "–≤–ø–µ—Ä–≤—ã–µ\n",
    "—Ä—É–∫–∏ —Ç–≤–æ–∏, –∏—Å—Å—Ç—É–ø–ª–µ–Ω–Ω—ã–π, –≥–ª–∞–¥–∏–ª.\n",
    "–°–µ–≥–æ–¥–Ω—è —Å–∏–¥–∏—à—å –≤–æ—Ç,\n",
    "—Å–µ—Ä–¥—Ü–µ –≤ –∂–µ–ª–µ–∑–µ.\n",
    "–î–µ–Ω—å –µ—â–µ ‚Äî\n",
    "–≤—ã–≥–æ–Ω–∏—à—å,\n",
    "–º–æ–∂–µ—Ç –±—ã—Ç—å, –∏–∑—Ä—É–≥–∞–≤.\n",
    "–í –º—É—Ç–Ω–æ–π –ø–µ—Ä–µ–¥–Ω–µ–π –¥–æ–ª–≥–æ –Ω–µ –≤–ª–µ–∑–µ—Ç\n",
    "—Å–ª–æ–º–∞–Ω–Ω–∞—è –¥—Ä–æ–∂—å—é —Ä—É–∫–∞ –≤ —Ä—É–∫–∞–≤.\n",
    "–í—ã–±–µ–≥—É,\n",
    "—Ç–µ–ª–æ –≤ —É–ª–∏—Ü—É –±—Ä–æ—à—É —è.\n",
    "–î–∏–∫–∏–π,\n",
    "–æ–±–µ–∑—É–º–ª—é—Å—å,\n",
    "–æ—Ç—á–∞—è–Ω—å–µ–º –∏—Å—Å–µ—á–∞ÃÅ—Å—å.\n",
    "–ù–µ –Ω–∞–¥–æ —ç—Ç–æ–≥–æ,\n",
    "–¥–æ—Ä–æ–≥–∞—è,\n",
    "—Ö–æ—Ä–æ—à–∞—è,\n",
    "–¥–∞–π –ø—Ä–æ—Å—Ç–∏–º—Å—è —Å–µ–π—á–∞—Å.\n",
    "–í—Å–µ —Ä–∞–≤–Ω–æ\n",
    "–ª—é–±–æ–≤—å –º–æ—è ‚Äî\n",
    "—Ç—è–∂–∫–∞—è –≥–∏—Ä—è –≤–µ–¥—å ‚Äî\n",
    "–≤–∏—Å–∏—Ç –Ω–∞ —Ç–µ–±–µ,\n",
    "–∫—É–¥–∞ –Ω–∏ –±–µ–∂–∞–ª–∞ –±.\n",
    "–î–∞–π –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–º –∫—Ä–∏–∫–µ –≤—ã—Ä–µ–≤–µ—Ç—å\n",
    "–≥–æ—Ä–µ—á—å –æ–±–∏–∂–µ–Ω–Ω—ã—Ö –∂–∞–ª–æ–±.\n",
    "–ï—Å–ª–∏ –±—ã–∫–∞ —Ç—Ä—É–¥–æ–º —É–º–æ—Ä—è—Ç ‚Äî\n",
    "–æ–Ω —É–π–¥–µ—Ç,\n",
    "—Ä–∞–∑–ª—è–∂–µ—Ç—Å—è –≤ —Ö–æ–ª–æ–¥–Ω—ã—Ö –≤–æ–¥–∞—Ö.\n",
    "–ö—Ä–æ–º–µ –ª—é–±–≤–∏ —Ç–≤–æ–µ–π,\n",
    "–º–Ω–µ\n",
    "–Ω–µ—Ç—É –º–æ—Ä—è,\n",
    "–∞ —É –ª—é–±–≤–∏ —Ç–≤–æ–µ–π –∏ –ø–ª–∞—á–µ–º –Ω–µ –≤—ã–º–æ–ª–∏—à—å –æ—Ç–¥—ã—Ö.\n",
    "–ó–∞—Ö–æ—á–µ—Ç –ø–æ–∫–æ—è —É—Å—Ç–∞–≤—à–∏–π —Å–ª–æ–Ω ‚Äî\n",
    "—Ü–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–π –ª—è–∂–µ—Ç –≤ –æ–ø–æ–∂–∞—Ä–µ–Ω–Ω–æ–º –ø–µ—Å–∫–µ.\n",
    "–ö—Ä–æ–º–µ –ª—é–±–≤–∏ —Ç–≤–æ–µ–π,\n",
    "–º–Ω–µ\n",
    "–Ω–µ—Ç—É —Å–æ–ª–Ω—Ü–∞,\n",
    "–∞ —è –∏ –Ω–µ –∑–Ω–∞—é, –≥–¥–µ —Ç—ã –∏ —Å –∫–µ–º.\n",
    "–ï—Å–ª–∏ –± —Ç–∞–∫ –ø–æ—ç—Ç–∞ –∏–∑–º—É—á–∏–ª–∞,\n",
    "–æ–Ω\n",
    "–ª—é–±–∏–º—É—é –Ω–∞ –¥–µ–Ω—å–≥–∏ –± –∏ —Å–ª–∞–≤—É –≤—ã–º–µ–Ω—è–ª,\n",
    "–∞ –º–Ω–µ\n",
    "–Ω–∏ –æ–¥–∏–Ω –Ω–µ —Ä–∞–¥–æ—Å—Ç–µ–Ω –∑–≤–æ–Ω,\n",
    "–∫—Ä–æ–º–µ –∑–≤–æ–Ω–∞ —Ç–≤–æ–µ–≥–æ –ª—é–±–∏–º–æ–≥–æ –∏–º–µ–Ω–∏.\n",
    "–ò –≤ –ø—Ä–æ–ª–µ—Ç –Ω–µ –±—Ä–æ—à—É—Å—å,\n",
    "–∏ –Ω–µ –≤—ã–ø—å—é —è–¥–∞,\n",
    "–∏ –∫—É—Ä–æ–∫ –Ω–µ —Å–º–æ–≥—É –Ω–∞–¥ –≤–∏—Å–∫–æ–º –Ω–∞–∂–∞—Ç—å.\n",
    "–ù–∞–¥–æ –º–Ω–æ—é,\n",
    "–∫—Ä–æ–º–µ —Ç–≤–æ–µ–≥–æ –≤–∑–≥–ª—è–¥–∞,\n",
    "–Ω–µ –≤–ª–∞—Å—Ç–Ω–æ –ª–µ–∑–≤–∏–µ –Ω–∏ –æ–¥–Ω–æ–≥–æ –Ω–æ–∂–∞.\n",
    "–ó–∞–≤—Ç—Ä–∞ –∑–∞–±—É–¥–µ—à—å,\n",
    "—á—Ç–æ —Ç–µ–±—è –∫–æ—Ä–æ–Ω–æ–≤–∞–ª,\n",
    "—á—Ç–æ –¥—É—à—É —Ü–≤–µ—Ç—É—â—É—é –ª—é–±–æ–≤—å—é –≤—ã–∂–µ–≥,\n",
    "–∏ —Å—ÉÃÅ–µ—Ç–Ω—ã—Ö –¥–Ω–µ–π –≤–∑–º–µ—Ç–µ–Ω–Ω—ã–π –∫–∞—Ä–Ω–∞–≤–∞–ª\n",
    "—Ä–∞—Å—Ç—Ä–µ–ø–ª–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã –º–æ–∏—Ö –∫–Ω–∏–∂–µ–∫‚Ä¶\n",
    "–°–ª–æ–≤ –º–æ–∏—Ö —Å—É—Ö–∏–µ –ª–∏—Å—Ç—å—è –ª–∏\n",
    "–∑–∞—Å—Ç–∞–≤—è—Ç –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è,\n",
    "–∂–∞–¥–Ω–æ –¥—ã—à–∞?\n",
    "–î–∞–π —Ö–æ—Ç—å\n",
    "–ø–æ—Å–ª–µ–¥–Ω–µ–π –Ω–µ–∂–Ω–æ—Å—Ç—å—é –≤—ã—Å—Ç–µ–ª–∏—Ç—å\n",
    "—Ç–≤–æ–π —É—Ö–æ–¥—è—â–∏–π —à–∞–≥..\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í –±–∏–±–ª–∏–æ—Ç–µ–∫–µ transformers –µ—Å—Ç—å –≥–æ—Ç–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ –∑–∞–≥—Ä—É–∑—á–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö. –ù–∞ –≤—Ö–æ–¥ –Ω—É–∂–µ–Ω –≤—Å–µ–≥–æ –ª–∏—à—å –æ–¥–∏–Ω `.txt` —Ñ–∞–π–ª —Å –æ–±—É—á–∞—é—â–∏–º —Ç–µ–∫—Å—Ç–æ–º."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save text train data as .txt file\n",
    "train_path = \"train_dataset.txt\"\n",
    "with open(train_path, mode=\"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], padding=\"max_length\", truncation=False, max_length=64\n",
    "    )\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"text\", data_files={\"train\": train_path})\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# –°rop the text into optimal length pieces\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–û–±—É—á–µ–Ω–∏–µ**\n",
    "\n",
    "–î–ª—è —Ñ–∞–π–Ω—Ç—é–Ω–∏–Ω–≥–∞ –Ω–∞–º –Ω–µ–æ–±—Ö–æ–¥–∏–º –æ–±—ä–µ–∫—Ç –∫–ª–∞—Å—Å–∞ Trainer, –∫–æ—Ç–æ—Ä—ã–π —Å–¥–µ–ª–∞–µ—Ç –≤—Å—é —Ä–∞–±–æ—Ç—É –∑–∞ –Ω–∞—Å. –î–∞–ª–µ–µ –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –≤—Å–µ–≥–æ –ª–∏—à—å –∑–∞–ø—É—Å—Ç–∏—Ç—å `trainer.train()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finetuned\",  # The output directory\n",
    "    overwrite_output_dir=True,  # overwrite the content of the output directory\n",
    "    num_train_epochs=200,  # number of training epochs\n",
    "    per_device_train_batch_size=32,  # batch size for training\n",
    "    per_device_eval_batch_size=32,  # batch size for evaluation\n",
    "    warmup_steps=10,  # number of warmup steps for learning rate scheduler\n",
    "    gradient_accumulation_steps=16,  # to make \"virtual\" batch size larger\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    optimizers=(\n",
    "        torch.optim.AdamW(model.parameters(), lr=1e-5),\n",
    "        None,\n",
    "    ),  # Optimizer and learnig rate scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–†–µ–∑—É–ª—å—Ç–∞—Ç —Ñ–∞–π–Ω—Ç—é–Ω–∏–Ω–≥–∞**\n",
    "\n",
    "–ì–æ—Ç–æ–≤–æ! –¢–µ–ø–µ—Ä—å –¥–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º, —á—Ç–æ –∂–µ —Å–æ—á–∏–Ω–∏—Ç GPT –≤ —Å—Ç–∏–ª–µ –ú–∞—è–∫–æ–≤—Å–∫–æ–≥–æ, –µ—Å–ª–∏ –Ω–∞ –≤—Ö–æ–¥ –ø–æ–¥–∞—Ç—å —Ç–∞–∫—É—é —Å—Ç—Ä–æ—á–∫—É:\n",
    "\n",
    "\"–ö–∞–∫ –∂–µ —Å–ª–æ–∂–Ω–æ —É—á–∏—Ç—å –º–∞—Ç–∞–Ω–∞–ª–∏–∑!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability sampling with limit example\n",
    "text = \"–ö–∞–∫ –∂–µ —Å–ª–æ–∂–Ω–æ —É—á–∏—Ç—å –º–∞—Ç–∞–Ω–∞–ª–∏–∑!\\n\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True,\n",
    "        num_beams=3,\n",
    "        temperature=1.9,\n",
    "        top_p=0.9,\n",
    "        max_length=200,\n",
    "        pad_token_id=512,\n",
    "    )\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –û–¥–Ω–∞ –º–æ–¥–µ–ª—å –¥–ª—è –≤—Å–µ—Ö –∑–∞–¥–∞—á"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–¥–∏–Ω –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –±—ã—Å—Ç—Ä–æ —Ä–∞–∑–≤–∏–≤–∞—é—â–∏—Ö—Å—è —Ç—Ä–µ–Ω–¥–æ–≤ –≤ NLP ‚Äî —Ä–µ—à–∞—Ç—å –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∑–∞–¥–∞—á –æ–¥–Ω–æ–π –æ–±—â–µ–π –º–æ–¥–µ–ª—å—é.\n",
    "\n",
    "- –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –±–æ–ª—å—à–æ–º –æ–±—ä–µ–º–µ –Ω–µ—Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∑–∞–∫–ª–∞–¥—ã–≤–∞–µ—Ç —à–∏—Ä–æ–∫–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —è–∑—ã–∫–∞\n",
    "- –î–æ–æ–±—É—á–µ–Ω–∏–µ –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –∑–∞–¥–∞—á—É —Ç—Ä–µ–±—É–µ—Ç –Ω–∞–ª–∏—á–∏—è —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –∫–æ—Ä–ø—É—Å–æ–≤ –≥–æ—Ä–∞–∑–¥–æ –º–µ–Ω—å—à–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞\n",
    "- –ü—Ä–∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞–≤–∫–∞—Ö (–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è—Ö) –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç —Ä–µ—à–∞—Ç—å –∑–∞–¥–∞—á–∏, –∫–æ—Ç–æ—Ä—ã–µ –æ–Ω–∏ –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –≤–∏–¥–µ–ª–∏\n",
    "- –í—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ NLP –∑–∞–¥–∞—á–∏ –º–æ–∂–Ω–æ —Å–≤–µ—Å—Ç–∏ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–µ—Ä–µ–≤–æ–¥–∞ –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–∞ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ–ø–µ—á–∞—Ç–æ–∫, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–º–µ–Ω–æ–≤–∞–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π. –ì–ª–∞–≤–Ω–æ–µ ‚Äî –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞—Ç—Ä–∞–≤–∫—É –∏–ª–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –î–æ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è—Ö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò–¥–µ—è: —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –º–Ω–æ–≥–æ –∑–∞–¥–∞—á –∫–∞–∫ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏ –¥–æ–æ–±—É—á–∏—Ç—å –Ω–∞ –≤—Å–µ—Ö –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ. –û–Ω–∞ –±—ã–ª–∞ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ –¥–ª—è –º–æ–¥–µ–ª–∏ FLAN –∏ –æ–ø–∏—Å–∞–Ω–∞ –≤ —Å—Ç–∞—Ç—å–µ [[paper] üéì Finetuned Language Models Are Zero-Shot Learners (2021)](https://arxiv.org/abs/2109.01652). 62 –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö NLP –∑–∞–¥–∞—á –±—ã–ª–∏ –ø–µ—Ä–µ–¥–µ–ª–∞–Ω—ã –≤ –æ–¥–∏–Ω –±–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π, —á—Ç–æ–±—ã –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å —Ä–µ—à–∞—Ç—å —Å—Ä–∞–∑—É –≤—Å—ë.\n",
    "–ü—Ä–∏–º–µ—Ä—ã –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π:\n",
    "- Please translate this sentence to French: 'The dog\n",
    "runs.'\n",
    "- What is the sentiment of this text? Options: Negative, Positive, Neutral.\n",
    "\n",
    "–ë—ã–ª–æ –∑–∞–º–µ—á–µ–Ω–æ, —á—Ç–æ –º–æ–¥–µ–ª—å –æ–±–ª–∞–¥–∞–µ—Ç –æ–±–æ–±—â–∞—é—â–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å—é –∏ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã—Ö –æ–Ω–∞ –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –≤–∏–¥–µ–ª–∞. –¢–∞–∫ –∫–∞–∫ –º–æ–¥–µ–ª—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∞ –Ω–∞ –±–æ–ª—å—à–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ —Ç–µ–∫—Å—Ç–æ–≤, –æ–Ω–∞ —É–∂–µ —Ö–æ—Ä–æ—à–æ –ø–æ–Ω–∏–º–∞–µ—Ç —è–∑—ã–∫ –∏ —ç–∫—Å—Ç—Ä–∞–ø–æ–ª–∏—Ä—É–µ—Ç –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏–∑ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏, –∏—Å–ø–æ–ª—å–∑—É—è —Å–≤–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —è–∑—ã–∫–∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'google/flan-t5-small'\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=512)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_instruction(instruction, text, model):\n",
    "\n",
    "    inputs = tokenizer([instruction.format(text)],\n",
    "                        return_tensors=\"pt\", padding=True)\n",
    "    output_sequences = model.generate(\n",
    "        num_beams=5,\n",
    "        max_length=100,\n",
    "        no_repeat_ngram_size=3,\n",
    "        early_stopping=True,\n",
    "        num_return_sequences= 1,\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        do_sample=False,\n",
    "    )\n",
    "    summaries = tokenizer.batch_decode(output_sequences, skip_special_tokens=True)\n",
    "    return summaries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Lomonosov Moscow State University is a public research university in Moscow, Russia, and the most prestigious university in the country. \\\n",
    "The university includes 15 research institutes, 43 faculties, more than 300 departments, and six branches (including five foreign ones in the Commonwealth of Independent States countries). \\\n",
    "Alumni of the university include past leaders of the Soviet Union and other governments. As of 2019, 13 Nobel laureates, six Fields Medal winners, and one Turing Award winner had been affiliated with the university. \\\n",
    "Ivan Shuvalov and Mikhail Lomonosov promoted the idea of a university in Moscow, and Russian Empress Elizabeth decreed its establishment on 23 January 1755. \\\n",
    "Since 1953, most of the faculties have been situated on Sparrow Hills, in southwest Moscow.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Suggest a headline for this text: {}\"\n",
    "print(f\"Instruction: {instruction}\")\n",
    "print(f\"Output: {predict_for_instruction(instruction, text, model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"When was Lomonosov Moscow State University founded? {}\"\n",
    "print(f\"Instruction: {instruction}\")\n",
    "print(f\"Output: {predict_for_instruction(instruction, text, model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Give a summary of this text: {}\"\n",
    "print(f\"Instruction: {instruction}\")\n",
    "print(f\"Output: {predict_for_instruction(instruction, text, model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑—å—é –æ—Ç —á–µ–ª–æ–≤–µ–∫–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " –û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑—å—é –æ—Ç —á–µ–ª–æ–≤–µ–∫–∞ (Reinforcement Learning from Human Feedback) ‚Äî –ø–æ–¥—Ö–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª–∏–ª –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è—Ö. –û–Ω –±—ã–ª –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ InstructGPT –∏ –æ–ø–∏—Å–∞–Ω –≤ —Å—Ç–∞—Ç—å–µ  [[paper] üéì Training language models to follow instructions with human feedback (2022)](https://arxiv.org/abs/2203.02155).\n",
    "\n",
    "\n",
    "\n",
    " –ú–æ–¥–µ–ª—å InstructGPT ‚Äî \"–º–ª–∞–¥—à–∏–π –±—Ä–∞—Ç\" ChatGPT. –û–Ω–∞ –Ω–µ –ø—Ä–∏–≤–ª–µ–∫–ª–∞ —Å—Ç–æ–ª—å–∫–æ –≤–Ω–∏–º–∞–Ω–∏—è –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏, –Ω–æ –±–æ–ª–µ–µ –ø–æ–ª–µ–∑–Ω–∞ –Ω–∞–º –∫–∞–∫ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è–º, –ø–æ—Å–∫–æ–ª—å–∫—É –ø—Ä–æ—Ü–µ—Å—Å –µ–µ –æ–±—É—á–µ–Ω–∏—è –æ–ø–∏—Å–∞–Ω –≤ —Å—Ç–∞—Ç—å–µ. –û–¥–Ω–∞–∫–æ —Å–∞–º–∞ –∏–¥–µ—è –æ–±—É—á–µ–Ω–∏—è –ø—Ä–∏–º–µ–Ω–∏–º–∞ –∏ –∫ ChatGPT.\n",
    "\n",
    " –ò—Ç–∞–∫, –∫–∞–∫ –ø—Ä–µ–≤—Ä–∞—Ç–∏—Ç—å (—É–∂–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —É–º–Ω—É—é) GPT-3 –≤ (–µ—â–µ –±–æ–ª–µ–µ —É–º–Ω—É—é) InstructGPT?\n",
    "\n",
    "- –®–∞–≥ 1: —Å–æ–±—Ä–∞—Ç—å –Ω–∞–±–æ—Ä –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –∏ –¥–æ–æ–±—É—á–∏—Ç—å –Ω–∞ –Ω–µ–º GPT-3.\n",
    "\n",
    "–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–æ–ª–∂–Ω—ã –≤–∫–ª—é—á–∞—Ç—å –Ω–µ —Ç–æ–ª—å–∫–æ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ NLP-–∑–∞–¥–∞—á–∏, –Ω–æ –∏ –±–æ–ª–µ–µ \"—Ç–≤–æ—Ä—á–µ—Å–∫–∏–µ\": –ø—Ä–∏–¥—É–º–∞—Ç—å –∏—Å—Ç–æ—Ä–∏—é, –Ω–∞–ø–∏—Å–∞—Ç—å —Å–ø–∏—Å–æ–∫ —á–µ–≥–æ-–Ω–∏–±—É–¥—å, –ø–æ—Å–æ–≤–µ—Ç–æ–≤–∞—Ç—å —á—Ç–æ-—Ç–æ. –ö–∞—á–µ—Å—Ç–≤–æ –≤–∞–∂–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞: –±–æ–ª—å—à–∞—è –¥–æ–ª—è —Ä—É—á–Ω–æ–≥–æ —Ç—Ä—É–¥–∞, –≤—ã—Å–æ–∫–æ–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ –∫ –∫–∞—á–µ—Å—Ç–≤—É –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π.\n",
    "\n",
    "–ü–æ—Å–ª–µ –¥–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ GPT-3 –ø–æ–ª—É—á–∏–º SFT-–º–æ–¥–µ–ª—å (supervised fine-tuned).\n",
    "\n",
    "- –®–∞–≥ 2: –æ—Ü–µ–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–æ–¥–µ–ª–∏ SFT –∏ –æ–±—É—á–∏—Ç—å reward-–º–æ–¥–µ–ª—å.\n",
    "\n",
    "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã SFT-–º–æ–¥–µ–ª–∏ –æ—Ü–µ–Ω–∏–≤–∞—é—Ç—Å—è —Å –ø–æ–º–æ—â—å—é —Ä–∞–∑–º–µ—Ç—á–∏–∫–æ–≤. –ë–µ—Ä–µ—Ç—Å—è —Ç–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, —Å –ø–æ–º–æ—â—å—é SFT-–º–æ–¥–µ–ª–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –æ—Ç–≤–µ—Ç–∞. –î–∞–ª–µ–µ —Ä–∞–∑–º–µ—Ç—á–∏–∫–∏ —Ä–∞–Ω–∂–∏—Ä—É—é—Ç –æ—Ç–≤–µ—Ç—ã –º–æ–¥–µ–ª–∏ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É: –ø–æ–ø–∞—Ä–Ω–æ —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç —ç—Ç–∏ –æ—Ç–≤–µ—Ç—ã –∏ –≥–æ–≤–æ—Ä—è—Ç, –∫–∞–∫–æ–π –∏–∑ –Ω–∏—Ö –ª—É—á—à–µ.\n",
    "\n",
    "–û—Ç–≤–µ—Ç—ã —Ä–∞–∑–º–µ—Ç—á–∏–∫–æ–≤ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è reward-–º–æ–¥–µ–ª–∏. –û–Ω–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Å–ø–æ—Å–æ–±–Ω–∞ –∏–º–∏—Ç–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç —Ä–∞–∑–º–µ—Ç—á–∏–∫–∞ –∏ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç—ã SFT-–º–æ–¥–µ–ª–∏.\n",
    "\n",
    "- –®–∞–≥ 3: –¥–æ–æ–±—É—á–∏—Ç—å SFT-–º–æ–¥–µ–ª—å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º reward-–º–æ–¥–µ–ª–∏\n",
    "\n",
    "SFT-–º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç—ã-–∫–∞–Ω–¥–∏–¥–∞—Ç—ã. –û–Ω–∏ –æ—Ü–µ–Ω–∏–≤–∞—é—Ç—Å—è —Å –ø–æ–º–æ—â—å—é reward-–º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω–æ–π –Ω–∞ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –æ—Ç –ª—é–¥–µ–π. –û—Ü–µ–Ω–∫–∞ reward-–º–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤–µ—Å–æ–≤ SFT-–º–æ–¥–µ–ª–∏. –û–Ω–∞ –¥–æ–æ–±—É—á–∞–µ—Ç—Å—è —Ç–∞–∫, —á—Ç–æ–±—ã –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –ø–æ–ª—É—á–∞–ª–∏ –≤—ã—Å–æ–∫–∏–µ –æ—Ü–µ–Ω–∫–∏.\n",
    "\n",
    "–î–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–µ—Ç–æ–¥ Proximal Policy Optimization (PPO). –ú–æ–¥–µ–ª—å –Ω–µ –ø—Ä–æ—Å—Ç–æ —É—á–∏—Ç—Å—è –º–∞–∫—Å–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –æ—Ü–µ–Ω–∫—É –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤, –Ω–æ –µ—â–µ —Å—Ç–∞—Ä–∞–µ—Ç—Å—è –º–∏–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L07/rlhf_steps.png\" width=\"800\"></center>\n",
    "\n",
    "<center><em>–ò—Å—Ç–æ—á–Ω–∏–∫: <a href=\"https://arxiv.org/abs/2203.02155\">Training language models to follow instructions with human feedback </a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RLHF –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø—Ä–∞–≤–∏—Ç—å —Ç–æ, —á—Ç–æ –±—ã–ª–æ –∑–∞–ª–æ–∂–µ–Ω–æ –∫–∞–∫ –≤–æ –≤—Ä–µ–º—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è, —Ç–∞–∫ –∏ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –º–µ—Ç–æ–¥–æ–º SFT. –≠—Ç–æ –º–æ–≥—É—Ç –±—ã—Ç—å –æ—à–∏–±–∫–∏ –∏–ª–∏ –Ω–µ–∂–µ—Ç–∞–ª—å–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L07/prediction_vs_reward_model.png\" width=\"800\"></center>\n",
    "\n",
    "<center><em>–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å—Ç—É–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞ –∏ –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è</em></center>\n",
    "\n",
    "<center><em>–ò—Å—Ç–æ—á–Ω–∏–∫: <a href=\"https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up#footnote-anchor-9-135824233\"> CAMERON R. WOLFE, \"LLaMA-2 from the Ground Up\"</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∏–º–µ–µ—Ç —Ç—É –∂–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –∏ –≤–µ—Å–∞, —á—Ç–æ –∏ –æ—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å. –†–∞–∑–Ω–∏—Ü–∞ –≤ —Ç–æ–º, —á—Ç–æ —Å–ª–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞) –∑–∞–º–µ–Ω—ë–Ω –Ω–∞ —Å–ª–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏.\n",
    "\n",
    "–ú–æ–¥–µ–ª—å —à—Ç—Ä–∞—Ñ—É–µ—Ç –∑–∞ —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—ã–º —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –æ—Ç–≤–µ—Ç–æ–º –∏ –∏–º–µ—é—â–∏–º—Å—è –º–∞—à–∏–Ω–Ω—ã–º. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –≤ —ç—Ç–æ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è —Å–º–µ—â–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑—É–µ—Ç—Å—è –≤—ã–±—Ä–∞–Ω–Ω–æ–π —á–µ–ª–æ–≤–µ–∫–æ–º –æ—Ü–µ–Ω–∫–æ–π –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–∞.\n",
    "\n",
    "$$\\large L_{\\text{ranking}} = - \\log (œÉ(r_{Œ∏}(x, y_c))-œÉ(r_{Œ∏}(x, y_r))-m(r))$$\n",
    "\n",
    "–°—Ç–æ–∏—Ç —É—á–µ—Å—Ç—å, —á—Ç–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –æ–±—É—á–∞–µ—Ç—Å—è –º–Ω–æ–∂–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –æ—Ç–≤–µ—á–∞—é—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º –æ—Ç–≤–µ—Ç–æ–≤ –º–æ–¥–µ–ª–∏: –æ—Ç –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –¥–æ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
