{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">–ú–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–µ–∫–æ–¥–µ—Ä–∞ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —ç–Ω–∫–æ–¥–µ—Ä–∞ –∏ –¥–µ–∫–æ–¥–µ—Ä–∞.\n",
    "\n",
    "–ù–∞ –æ—Å–Ω–æ–≤–µ —ç–Ω–∫–æ–¥–µ—Ä–∞ –±—ã–ª–∞ —Å–æ–∑–¥–∞–Ω–∞ –º–æ–¥–µ–ª—å BERT –∏ –ø–æ–¥–æ–±–Ω—ã–µ –µ–π (RoBERTa, ALBERT). –û–Ω–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —Å–ª–æ–≤ –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π. –ó–∞—Ç–µ–º –º–æ–¥–µ–ª–∏ –¥–æ–æ–±—É—á–∞—é—Ç—Å—è –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–¥–∞—á–∏, —á—Ç–æ —Ç—Ä–µ–±—É–µ—Ç –Ω–∞–ª–∏—á–∏—è —Ä–∞–∑–º–µ—á–µ–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö.\n",
    "\n",
    "–ù–∞ –æ—Å–Ω–æ–≤–µ —ç–Ω–∫–æ–¥–µ—Ä–∞ –±—ã–ª–∞ —Å–æ–∑–¥–∞–Ω–∞ –º–æ–¥–µ–ª—å GPT ‚Äî Generative Pretrained Transformer. –ú–æ–¥–µ–ª—å GPT –±—ã–ª–∞ –æ–±—É—á–µ–Ω–∞ –Ω–∞ –∑–∞–¥–∞—á–µ —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è, —Ç–æ –µ—Å—Ç—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏) —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–ª–æ–≤–∞ –≤ —Ç–µ–∫—Å—Ç–µ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ß–µ—Ç—ã—Ä–µ –ø–æ–∫–æ–ª–µ–Ω–∏—è GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é GPT –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Å–ª–µ–¥—É—é—â–µ–µ:\n",
    "\n",
    "1. –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç—Å—è –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —á–∏—Å–µ–ª (—Ç–æ–∫–µ–Ω–æ–≤).\n",
    "2. –°–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤ –ø—Ä–æ—Ö–æ–¥–∏—Ç —á–µ—Ä–µ–∑ Embedding layer (–ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π) –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç—Å—è –≤ —Å–ø–∏—Å–æ–∫ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.\n",
    "3. –ö –∫–∞–∂–¥–æ–º—É —ç–º–±–µ–¥–¥–∏–Ω–≥—É –ø—Ä–∏–±–∞–≤–ª—è–µ—Ç—Å—è **positional embedding**.\n",
    "4. –°–ø–∏—Å–æ–∫ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –ø—Ä–æ—Ö–æ–¥–∏—Ç —á–µ—Ä–µ–∑ –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö –±–ª–æ–∫–æ–≤ (Transformer Decoder Block).\n",
    "5. –ü–æ—Å–ª–µ —Ç–æ–≥–æ, –∫–∞–∫ —Å–ø–∏—Å–æ–∫ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –ø—Ä–æ–π–¥—ë—Ç —á–µ—Ä–µ–∑ –ø–æ—Å–ª–µ–¥–Ω–∏–π –±–ª–æ–∫, —ç–º–±–µ–¥–¥–∏–Ω–≥, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –ø–æ—Å–ª–µ–¥–Ω–µ–º—É —Ç–æ–∫–µ–Ω—É, –º–∞—Ç—Ä–∏—á–Ω–æ —É–º–Ω–æ–∂–∞–µ—Ç—Å—è –Ω–∞ –≤—Å—ë —Ç–æ—Ç –∂–µ –≤—Ö–æ–¥–Ω–æ–π, –Ω–æ —É–∂–µ —Ç—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π Embedding Layer, –∏ –ø–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è SoftMax –ø–æ–ª—É—á–∞–µ—Ç—Å—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞.\n",
    "6. –ò–∑ —ç—Ç–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è —Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å –ø–æ–º–æ—â—å—é argmax).\n",
    "7. –ü–æ–ª—É—á–µ–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –∫ –≤—Ö–æ–¥–Ω–æ–º—É —Å–ø–∏—Å–∫—É —Ç–æ–∫–µ–Ω–æ–≤, —à–∞–≥–∏ 1‚Äì6 –ø–æ–≤—Ç–æ—Ä—è—é—Ç—Å—è."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L06/gpt3.gif\" width=\"800\"></center>\n",
    "\n",
    "<center><em>–ò—Å—Ç–æ—á–Ω–∏–∫: <a href=\"https://jalammar.github.io/how-gpt3-works-visualizations-animations/\">How GPT3 Works ‚Äî Visualizations and Animations</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[paper] üéì Improving Language Understanding by Generative Pre-Training (2018)](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)\n",
    "\n",
    "- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: 117 –º–∏–ª–ª–∏–æ–Ω–æ–≤.\n",
    "- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤: 12.\n",
    "- –†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö: 5 –ì–ë.\n",
    "- –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: 512 —Ç–æ–∫–µ–Ω–æ–≤.\n",
    "\n",
    "–ü–µ—Ä–≤–∞—è –≤–µ—Ä—Å–∏—è —ç—Ç–æ–π –º–æ–¥–µ–ª–∏ –±—ã–ª–∞ –æ–±—É—á–µ–Ω–∞ –Ω–∞ 7000 –∫–Ω–∏–≥.\n",
    "\n",
    "–ö–∞–∫ —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –æ–Ω–∞ —Ä–∞–±–æ—Ç–∞–ª–∞ –Ω–µ –æ—á–µ–Ω—å —Ö–æ—Ä–æ—à–æ, –Ω–æ –ø—Ä–∏ –¥–æ–æ–±—É—á–µ–Ω–∏–∏ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –ø–æ–∫–∞–∑—ã–≤–∞–ª–∞ –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ.\n",
    "\n",
    "–û–¥–Ω–∞–∫–æ SOTA —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã GPT-1 –¥–µ—Ä–∂–∞–ª–∏—Å—å –Ω–µ–¥–æ–ª–≥–æ, —Ç–∞–∫ –∫–∞–∫ –ø–æ—è–≤–∏–ª—Å—è BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L06/gpt1.jpg\" width=\"800\"></center>\n",
    "\n",
    "<center><em>–ò—Å—Ç–æ—á–Ω–∏–∫: <a href=\"https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf/\">Improving Language Understanding by Generative Pre-Training</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[paper] üéì Language Models are Unsupervised Multitask Learners (2019)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "\n",
    "- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: 1.5 –º–∏–ª–ª–∏–∞—Ä–¥–∞.\n",
    "- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤: 48.\n",
    "- –†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö: 40 –ì–ë.\n",
    "- –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: 1024 —Ç–æ–∫–µ–Ω–æ–≤.\n",
    "\n",
    "\n",
    "–ù–æ–≤–∞—è GPT-2 –Ω–µ —Å–æ–¥–µ—Ä–∂–∞–ª–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã—Ö –Ω–æ–≤–æ–≤–≤–µ–¥–µ–Ω–∏–π, –æ—Å–Ω–æ–≤–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –¥–æ—Å—Ç–∏–≥–∞–ª–∏—Å—å –∑–∞ —Å—á–µ—Ç —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö (–∫ –∫–Ω–∏–≥–∞–º –¥–æ–±–∞–≤–∏–ª–∏ 8 –º–∏–ª–ª–∏–æ–Ω–æ–≤ —Å–∞–π—Ç–æ–≤).\n",
    "\n",
    "GPT-2 –Ω–∞—É—á–∏–ª–∞—Å—å –ø–∏—Å–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ —Å–≤—è–∑–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L06/gpt2.png\" width=\"800\"></center>\n",
    "\n",
    "<center><em>–ò—Å—Ç–æ—á–Ω–∏–∫: <a href=\"https://en.wikipedia.org/wiki/GPT-2\">GPT-2</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–∞–∫–∂–µ GPT-2 —É–∂–µ –º–æ–≥–ª–∞ –±–µ–∑ —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≤—ã–ø–æ–ª–Ω—è—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∑–∞–¥–∞—á–∏, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ –±—ã–ª–æ —è–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ù–∞–ø—Ä–∏–º–µ—Ä, –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–æ–ø–∏—Å–∞—Ç—å `TL;DR` –ø–æ—Å–ª–µ —Ç–µ–∫—Å—Ç–∞."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i.postimg.cc/nL19wfYL/tldr.jpg\" width=\"850\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[paper] üéì Language Models are Few-Shot Learners (2020)](https://arxiv.org/pdf/2005.14165.pdf)\n",
    "\n",
    "- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: 175 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤.\n",
    "- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤: 96.\n",
    "- –†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö: 45 –¢–ë (—Ç.–µ. 45 000 –ì–ë).\n",
    "- –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: 2048 —Ç–æ–∫–µ–Ω–æ–≤.\n",
    "\n",
    "GPT-3 –æ–±—É—á–µ–Ω–∞ –Ω–∞ –µ—â—ë –±–æ–ª—å—à–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –¥–∞–Ω–Ω—ã—Ö ‚Äî –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞, –≤—Å—è –∞–Ω–≥–ª–∏–π—Å–∫–∞—è –í–∏–∫–∏–ø–µ–¥–∏—è —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–æ–ª—å–∫–æ 0,6% –∏–∑ –Ω–∏—Ö.\n",
    "\n",
    "–ú–æ–¥–µ–ª—å –Ω–∞—É—á–∏–ª–∞—Å—å —Ä–µ—à–∞—Ç—å –º–Ω–æ–≥–æ NLP-–∑–∞–¥–∞—á –±–µ–∑ –¥–æ–æ–±—É—á–µ–Ω–∏—è."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <table >\n",
    "     <tr>\n",
    "       <td>\n",
    "       \n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L06/gpt3_gec.jpg\" width=\"440\"></center>\n",
    "\n",
    "<em>–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫</em>\n",
    "\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L06/gpt3_sa.jpg\" width=\"450\"></center>\n",
    "\n",
    "<em>–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏</em>\n",
    "\n",
    "\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L06/gpt3_mt.jpg\" width=\"500\"></center>\n",
    "\n",
    "<em>–ú–∞—à–∏–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥</em>\n",
    "\n",
    "\n",
    "</td>\n",
    "     </tr>\n",
    "    </table>\n",
    "    </div>\n",
    "\n",
    "<center><em>–ò—Å—Ç–æ—á–Ω–∏–∫: <a href=\"https://beta.openai.com/examples\">GPT-3 beta</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-3 —Ç–∞–∫–∂–µ —Å–ø–æ—Å–æ–±–Ω–∞ –ø–∏—Å–∞—Ç—å –∏ –ø–æ–Ω–∏–º–∞—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º–Ω—ã–π –∫–æ–¥."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <table >\n",
    "     <tr>\n",
    "       <td>\n",
    "       \n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L06/gpt3_code_promt.jpg\" width=\"300\"></center>\n",
    "\n",
    "<em>–ó–∞–ø—Ä–æ—Å</em>\n",
    "\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L06/gpt3_code_response.jpg\" width=\"400\"></center>\n",
    "\n",
    "<em>–ü—Ä–∏–º–µ—Ä –æ—Ç–≤–µ—Ç–∞</em>\n",
    "\n",
    "\n",
    "</td>\n",
    "     </tr>\n",
    "    </table>\n",
    "    </div>\n",
    "\n",
    "<center><em>–ò—Å—Ç–æ—á–Ω–∏–∫: <a href=\"https://beta.openai.com/examples\">GPT-3 beta</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°—Ç–∞—Ç—å—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç.\n",
    "\n",
    "- –ù–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø—Ä–æ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏.\n",
    "- –†–∞–±–æ—Ç–∞–µ—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –¥–æ 8192 —Ç–æ–∫–µ–Ω–æ–≤. –ï—Å—Ç—å –≤–µ—Ä—Å–∏—è, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∞—è 32 768 —Ç–æ–∫–µ–Ω–æ–≤ (50 —Å—Ç—Ä–∞–Ω–∏—Ü).\n",
    "\n",
    "–ü–æ–º–∏–º–æ —Ç–µ–∫—Å—Ç–æ–≤, —É–º–µ–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏, —è–≤–ª—è–µ—Ç—Å—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L06/gpt4.jpg\" width=\"550\"></center>\n",
    "\n",
    "<center><em>–ò—Å—Ç–æ—á–Ω–∏–∫: <a href=\"https://thecymes.com/article/the-game-changing-features-of-openais-chatgpt-4\">The game-changing features of OpenAI's GPT 4</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —Ä–∞–±–æ—Ç—É –º–æ–¥–µ–ª–∏ ruGPT-3 –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞. –û–Ω–∞ –¥–æ—Å—Ç—É–ø–Ω–∞ –≤ —Ç—Ä–µ—Ö –≤–∞—Ä–∏–∞–Ω—Ç–∞—Ö:\n",
    "- small: [ai-forever/rugpt3small_based_on_gpt2 üõ†Ô∏è[doc]](https://huggingface.co/ai-forever/rugpt3small_based_on_gpt2)\n",
    "- medium: [ai-forever/rugpt3medium_based_on_gpt2 üõ†Ô∏è[doc]](https://huggingface.co/ai-forever/rugpt3medium_based_on_gpt2)\n",
    "- large: [ai-forever/rugpt3large_based_on_gpt2 üõ†Ô∏è[doc]](https://huggingface.co/ai-forever/rugpt3large_based_on_gpt2)\n",
    "\n",
    "–î–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è –∫–ª–∞—Å—Å `AutoTokenizer` [üõ†Ô∏è[doc]](https://huggingface.co/docs/transformers/v4.45.2/en/model_doc/auto#transformers.AutoTokenizer), –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ ‚Äî –∫–ª–∞—Å—Å `AutoModelForCausalLM` [üõ†Ô∏è[doc]](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM).\n",
    "\n",
    "–í—ã–±–µ—Ä–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—É—é –º–æ–¥–µ–ª—å. API –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π, –¥–ª—è –ø–æ–¥–º–µ–Ω—ã –º–æ–¥–µ–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ `model_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"ai-forever/rugpt3large_based_on_gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ –º—ã —Ö–æ—Ç–∏–º –ø—Ä–∏ –ø–æ–º–æ—â–∏ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å: **¬´–°–∫–æ–ª—å–∫–æ –±—É–¥–µ—Ç 2+2?¬ª**, —Ç–æ –º–æ–∂–µ–º –ø–æ–¥–∞—Ç—å –Ω–∞ –≤—Ö–æ–¥ –º–æ–¥–µ–ª–∏ —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç:\\\n",
    "`¬´–í–æ–ø—Ä–æ—Å: –°–∫–æ–ª—å–∫–æ –±—É–¥–µ—Ç 2+2? –û—Ç–≤–µ—Ç: ‚Ä¶ ¬ª`\\\n",
    "–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ–º —Ç–∞–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –±—É–¥–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å, –ø–æ—ç—Ç–æ–º—É –º–æ–¥–µ–ª—å –¥–æ–ø–∏—à–µ—Ç `¬´4¬ª`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"–í–æ–ø—Ä–æ—Å: '–°–∫–æ–ª—å–∫–æ –±—É–¥–µ—Ç 2+2?'\\n–û—Ç–≤–µ—Ç:\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "out = model.generate(input_ids, do_sample=False, max_length=20, pad_token_id=20)\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–†–∞—Å—Å–º–æ—Ç—Ä–∏–º –ø—Ä–∏–º–µ—Ä —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏. –ö–∞–∫ –∏ BERT-like –º–æ–¥–µ–ª–∏, GPT –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –Ω–∞ –ø–æ–¥—Å–ª–æ–≤–∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"–¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–π –º–µ–Ω—è\" # raw text\n",
    "tokens = tokenizer.encode(text, add_special_tokens=False) # applying ruGPT-3 tokenizer\n",
    "decoded_tokens = [tokenizer.decode([token]) for token in tokens] # converting ids to tokens\n",
    "\n",
    "print(\"text:\", text)\n",
    "print(\"tokens: \", tokens)\n",
    "print(\"decoded tokens: \", decoded_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–Ø–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –≤—ã–¥–∞—ë—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞, —ç—Ç—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –º–æ–∂–Ω–æ –ø–æ-—Ä–∞–∑–Ω–æ–º—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞.\n",
    "\n",
    "–î–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏ –ø—Ä–∏–º–µ–Ω–∏–º –≤—Å–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è —Å–ª–µ–¥—É—é—â–µ–≥–æ: '–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: \"–Ø–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å\" - —ç—Ç–æ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "text = '–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: \"–Ø–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å\" ‚Äî —ç—Ç–æ'\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ñ–∞–¥–Ω—ã–π –ø–æ–∏—Å–∫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–∞–º—ã–π –ø—Ä–æ—Å—Ç–æ–π —Å–ø–æ—Å–æ–± ‚Äî –∂–∞–¥–Ω—ã–π –ø–æ–∏—Å–∫ (greedy search): –∫–∞–∂–¥—ã–π —Ä–∞–∑ –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è —Ç–æ–∫–µ–Ω, —É –∫–æ—Ç–æ—Ä–æ–≥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å.\n",
    "\n",
    "–ü—Ä–∏ —Ç–∞–∫–æ–º —Å–ø–æ—Å–æ–±–µ –º—ã –Ω–µ –ø–æ–ª—É—á–∏–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ –∑–∞–ø—Ä–æ—Å, –∏, —á—Ç–æ –µ—â—ë —Ö—É–∂–µ, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –º–æ–∂–µ—Ç –∑–∞—Å—Ç—Ä–µ–≤–∞—Ç—å –≤ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–∏–Ω–∏–º—É–º–∞—Ö –∏ –≤—ã–¥–∞–≤–∞—Ç—å –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã, –Ω–∞–ø—Ä–∏–º–µ—Ä `the the the the ...`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L06/greedy_seacrh.png\" width=\"500\"></center>\n",
    "\n",
    "<center><em>–ò—Å—Ç–æ—á–Ω–∏–∫: <a href=\"https://habr.com/ru/articles/589663/\">–ö—Ä–∞—Ç–∫–∏–π —ç–∫—Å–∫—É—Ä—Å –≤ ruGPT-3</a></em></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of greedy search\n",
    "out = model.generate(input_ids,\n",
    "                     do_sample=False,\n",
    "                     max_length=30)\n",
    "\n",
    "# Decoding\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –õ—É—á–µ–≤–æ–π –ø–æ–∏—Å–∫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "–ß—É—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–π –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Å–ø–æ—Å–æ–± ‚Äî —ç—Ç–æ –ª—É—á–µ–≤–æ–π –ø–æ–∏—Å–∫ (beam search). –ù–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è –Ω–µ —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —Å–∞–º—ã–π –≤–µ—Ä–æ—è—Ç–Ω—ã–π —Ç–æ–∫–µ–Ω, –∞ —Å—Ä–∞–∑—É –Ω–µ—Å–∫–æ–ª—å–∫–æ (–∑–∞ –∏—Ö –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—á–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä `num_beams`). –î–∞–ª—å—à–µ –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç—Å—è –ø–æ–∏—Å–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤. –ü—É—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–∞–∑–≤–µ—Ç–≤–ª—è—é—Ç—Å—è, —á—Ç–æ –¥–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.\n",
    "\n",
    "–û–±—ã—á–Ω–æ —ç—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –≤—ã—Å–æ–∫–æ–π —Å–≤—è–∑–Ω–æ—Å—Ç–∏ (–∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏), –Ω–æ –ø—Ä–∏ —ç—Ç–æ–º –∫ —Å—É—Ö–æ—Å—Ç–∏ –∏ —Å–∫—É—á–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞. –¢–∞–∫–∂–µ —ç—Ç–æ –Ω–µ —Ä–µ—à–∞–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—Ä–æ–±–ª–µ–º—É —Å –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è–º–∏ –∫—É—Å–æ—á–∫–æ–≤ —Ç–µ–∫—Å—Ç–∞."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L06/beam_search.png\" width=\"500\"></center>\n",
    "\n",
    "<center><em>–ò—Å—Ç–æ—á–Ω–∏–∫: <a href=\"https://habr.com/ru/articles/589663/\">–ö—Ä–∞—Ç–∫–∏–π —ç–∫—Å–∫—É—Ä—Å –≤ ruGPT-3</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–∞–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–±–ª–∞–¥–∞–µ—Ç —Ö–æ—Ä–æ—à–µ–π –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å—é (—Å–≤—è–∑–Ω–æ—Å—Ç—å—é), –Ω–æ –æ–±—ã—á–Ω–æ —É –Ω–∏—Ö –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç \"—á–µ–ª–æ–≤–µ—á–Ω–æ—Å—Ç–∏\", –æ–Ω–∏ –∫–∞–∂—É—Ç—Å—è —Å—É—Ö–∏–º–∏ –∏ —Å–∫—É—á–Ω—ã–º–∏. –¢–∞–∫–∂–µ —ç—Ç–æ –Ω–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of beam search\n",
    "out = model.generate(input_ids,\n",
    "                     do_sample=False,\n",
    "                     num_beams=5,\n",
    "                     max_length=40)\n",
    "\n",
    "# Decoding\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú–æ–∂–Ω–æ –≤—ã–≤–µ—Å—Ç–∏ –Ω–µ —Ç–æ–ª—å–∫–æ –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –Ω–æ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –¥—Ä—É–≥–∏–º –ª—É—á–∞–º."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set return_num_sequences > 1\n",
    "out = model.generate(input_ids,\n",
    "                     do_sample=False,\n",
    "                     num_beams=5,\n",
    "                     max_length=40,\n",
    "                     num_return_sequences=5)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(out):\n",
    "  print(\"{}: {}\\n\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–æ–≤ –º–æ–∂–Ω–æ —É–º–µ–Ω—å—à–∏—Ç—å, –µ—Å–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–∞–ø—Ä–µ—Ç –Ω–∞ –ø–æ–≤—Ç–æ—Ä *n*-–≥—Ä–∞–º–º."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can prohibit repeating bigrams\n",
    "out = model.generate(input_ids,\n",
    "                     do_sample=False,\n",
    "                     num_beams=5,\n",
    "                     max_length=40,\n",
    "                     num_return_sequences=5,\n",
    "                     no_repeat_ngram_size=2)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(out):\n",
    "  print(\"{}: {}\\n\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –°—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–æ–π"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ß—Ç–æ–±—ã –¥–æ–±–∞–≤–∏—Ç—å —Ç–µ–∫—Å—Ç—É –Ω–µ–ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ—Å—Ç–∏ –∏ —á–µ–ª–æ–≤–µ—á–Ω–æ—Å—Ç–∏, –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ–µ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–æ–π. –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –Ω–µ —Å–∞–º—ã–π –≤–µ—Ä–æ—è—Ç–Ω—ã–π —Ç–æ–∫–µ–Ω, –∞ —Å–ª—É—á–∞–π–Ω—ã–π, —Å —É—á—ë—Ç–æ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π.\n",
    "\n",
    "–ü–∞—Ä–∞–º–µ—Ç—Ä —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å —Å—Ç–µ–ø–µ–Ω—å —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏. –ü—Ä–∏ –Ω—É–ª–µ–≤–æ–π —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–µ –º–µ—Ç–æ–¥ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∂–∞–¥–Ω—ã–º —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ–º, –ø—Ä–∏  –±–æ–ª—å—à–æ–π —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–µ —Ç–æ–∫–µ–Ω—ã –±—É–¥—É—Ç –≤—ã–±–∏—Ä–∞—Ç—å—Å—è –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–ª—É—á–∞–π–Ω–æ. –û–±—ã—á–Ω–æ —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ `0.8‚Äì2.0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–§–æ—Ä–º—É–ª–∞ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–∞ –Ω–∞ —Ñ–æ—Ä–º—É–ª—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ë–æ–ª—å—Ü–º–∞–Ω–∞: —á–µ–º –≤—ã—à–µ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ —Å–∏—Å—Ç–µ–º—ã, —Ç–µ–º –±–æ–ª—å—à–µ \"—Ä–∞–∑–º–∞–∑—ã–≤–∞–µ—Ç—Å—è\" —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –µ—ë –≤–æ–∑–º–æ–∂–Ω—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π, –æ—Ç—Å—é–¥–∞ —Å–ª–æ–≤–æ \"—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞\".\n",
    "\n",
    "$$\\large p=\\text{softmax}(\\log(p)/t)$$\n",
    "\n",
    "–°—Ç–æ–∏—Ç –æ—Ç–º–µ—Ç–∏—Ç—å, —á—Ç–æ —Å–ª—É—á–∞–π–Ω–∞—è –ø—Ä–∏—Ä–æ–¥–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±—É–¥–µ—Ç –∏–Ω–æ–≥–¥–∞ –ø—Ä–∏–≤–æ–¥–∏—Ç—å –∫ –ø–æ–ª–Ω–æ—Å—Ç—å—é –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling eith temperature\n",
    "out = model.generate(input_ids, do_sample=True, temperature=1.3, max_length=30)\n",
    "\n",
    "# Decoding\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –°—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ top-k –∏ top-p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–ª—è –∑–∞–ø—Ä–µ—Ç–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–∞–ª–æ–≤–µ—Ä–æ—è—Ç–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤–≤–æ–¥—è—Ç `top-k` –∏–ª–∏ `top-p` –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è. –í —ç—Ç–æ–º —Å–ª—É—á–∞–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–æ–∂–µ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Å–ª—É—á–∞–π–Ω—ã–º –æ–±—Ä–∞–∑–æ–º, –Ω–æ –∑–∞—Ä–∞–Ω–µ–µ –æ—Ç—Å–µ–∫–∞—é—Ç—Å—è –≤—Å–µ –º–∞–ª–æ–≤–µ—Ä–æ—è—Ç–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í —Å–ª—É—á–∞–µ `top-k` –∑–∞–Ω—É–ª—è—é—Ç—Å—è –≤—Å–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∫—Ä–æ–º–µ `k` —Å–∞–º—ã—Ö –≤–µ—Ä–æ—è—Ç–Ω—ã—Ö.\n",
    "\n",
    "–£—Å—Ç–∞–Ω–æ–≤–∏–≤ $k=6$, –º—ã –±—É–¥–µ–º –≤—ã–±–∏—Ä–∞—Ç—å —Å–ª–µ–¥—É—é—â–µ–µ —Å–ª–æ–≤–æ —Ç–æ–ª—å–∫–æ –∏–∑ 6 —Å–ª–æ–≤ —Å –Ω–∞–∏–±–æ–ª—å—à–µ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é:\n",
    "- *nice, dog, car, woman, guy, man* –Ω–∞ 1-–º —à–∞–≥–µ\n",
    "- *drives, is, turns, stops, down, a* –Ω–∞ 2-–º —à–∞–≥–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L06/top-k.png\" width=\"800\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í —Å–ª—É—á–∞–µ `top-p` –æ—Å—Ç–∞–µ—Ç—Å—è –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä —Ç–æ–∫–µ–Ω–æ–≤, —á—Ç–æ–±—ã —Å—É–º–º–∞ –∏—Ö –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –±—ã–ª–∞ –Ω–µ –±–æ–ª—å—à–µ `p`. –°–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤, –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö –º—ã –≤—ã–±–∏—Ä–∞–µ–º, –º–æ–∂–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –º–µ–Ω—è—Ç—å—Å—è (—É–≤–µ–ª–∏—á–∏–≤–∞—Ç—å—Å—è –∏ —É–º–µ–Ω—å—à–∞—Ç—å—Å—è).\n",
    "\n",
    "–£—Å—Ç–∞–Ω–æ–≤–∏–º $p=0.92$ –∏ –±—É–¥–µ–º –≤—ã–±–∏—Ä–∞—Ç—å –∏–∑:\n",
    "- 9 —Å–ª–æ–≤ –Ω–∞ 1-–º —à–∞–≥–µ\n",
    "- 3 —Å–ª–æ–≤ –Ω–∞ 2-–º —à–∞–≥–µ\n",
    "\n",
    "–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ `top-p` –Ω–∞–∑—ã–≤–∞—é—Ç —è–¥–µ—Ä–Ω—ã–º —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ–º (nucleus sampling)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L06/top-p.png\" width=\"700\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random state\n",
    "torch.random.manual_seed(42)\n",
    "\n",
    "# Sampling with top-k and top-p restrictions\n",
    "out = model.generate(input_ids,\n",
    "                     do_sample=True,\n",
    "                     temperature=1.0,\n",
    "                     top_k=20,\n",
    "                     top_p=0.9,\n",
    "                     max_length=30,\n",
    "                    )\n",
    "# Decoding\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ó–∞—Ç—Ä–∞–≤–æ—á–Ω–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í –æ—Ç–ª–∏—á–∏–µ –æ—Ç BERT-like –º–æ–¥–µ–ª–µ–π, –¥–ª—è GPT —ç—Ç–∞–ø –¥–æ–æ–±—É—á–µ–Ω–∏—è –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω. –î–ª—è —Ç–æ–≥–æ —á—Ç–æ–±—ã –¥–æ–ø–∏—Å—ã–≤–∞—Ç—å —Ç–µ–∫—Å—Ç, –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –æ—á–µ–Ω—å —Ö–æ—Ä–æ—à–æ –ø–æ–Ω–∏–º–∞—Ç—å –µ–≥–æ —Å–º—ã—Å–ª –∏ –∏–º–µ—Ç—å –∑–Ω–∞–Ω–∏—è –æ —Ä–µ–∞–ª—å–Ω–æ–º –º–∏—Ä–µ. –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –∑–Ω–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –º–æ–∂–Ω–æ –ø–æ–ø—ã—Ç–∞—Ç—å—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å, –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É—è –ª–µ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ç–µ–∫—Å—Ç–∞. –ü–æ—Å–∫–æ–ª—å–∫—É GPT —Å–ø–æ—Å–æ–±–Ω–∞ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç, –º—ã –º–æ–∂–µ–º —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –ª—é–±—É—é NLP-–∑–∞–¥–∞—á—É –∫–∞–∫ –∑–∞–¥–∞—á—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è) —Ç–µ–∫—Å—Ç–∞. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ä–µ—à–∞—Ç—å –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∑–∞–¥–∞—á —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º –∏–ª–∏ –±–µ–∑ –Ω–µ–≥–æ.\n",
    "\n",
    "–ü–æ–¥–±–æ—Ä –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–π —Ç–µ–∫—Å—Ç–∞ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è \"–∑–∞—Ç—Ä–∞–≤–æ—á–Ω–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ\" (prompt engineering). –°—É—Ç—å –∑–∞—Ç—Ä–∞–≤–æ—á–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –≤ —Ç–æ–º, —á—Ç–æ–±—ã –ø–æ–¥–æ–±—Ä–∞—Ç—å —Ç–∞–∫–∏–µ –∑–∞–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∏ –Ω–∞—á–∞–ª–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, —á—Ç–æ–±—ã –¥–∞–ª—å—à–µ –º–æ–¥–µ–ª—å –≤—ã–¥–∞–≤–∞–ª–∞ —Ä–æ–≤–Ω–æ —Ç–æ, —á—Ç–æ –Ω–∞–¥–æ. –ü–æ–¥–±–∏—Ä–∞—è \"–∑–∞—Ç—Ä–∞–≤–∫–∏\" –∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ (—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞, `top-k`, `top-p`), –º–æ–∂–Ω–æ –¥–æ–±–∏—Ç—å—Å—è —Ö–æ—Ä–æ—à–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤ —Ç–æ–ª—å–∫–æ –∑–∞ —Å—á–µ—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.\n",
    "\n",
    "–°—É—â–µ—Å—Ç–≤—É–µ—Ç –¥–≤–∞ –ø–æ–¥—Ö–æ–¥–∞ –¥–ª—è –∑–∞—Ç—Ä–∞–≤–æ—á–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è:\n",
    "- zero-shot: —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞–¥–∞—á—É –∏ –ø–µ—Ä–µ–¥–∞—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é;\n",
    "- few-shot: —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞–¥–∞—á—É, –ø–æ–¥–∞—Ç—å –≤ –º–æ–¥–µ–ª—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –∏ –∑–∞—Ç–µ–º –ø–µ—Ä–µ–¥–∞—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero-shot learning\n",
    "text = \"–° –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –Ω–∞ —Ä—É—Å—Å–∫–∏–π 'cat' –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—Å—è –∫–∞–∫ '\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "out = model.generate(input_ids, do_sample=False,max_length=15)\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# few-shot learning\n",
    "text = \"–° –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –Ω–∞ —Ä—É—Å—Å–∫–∏–π 'cat' –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—Å—è –∫–∞–∫ '–∫–æ—à–∫–∞', 'dog' –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—Å—è –∫–∞–∫ '—Å–æ–±–∞–∫–∞', –∞ 'bird' –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—Å—è –∫–∞–∫ '\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "out = model.generate(input_ids, do_sample=False,max_length=35)\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–¥–µ–ª–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—É—é —Å–∏—Å—Ç–µ–º—É –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è: –±—É–¥–µ–º –ø–æ–¥–∞–≤–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –≤–∏–¥–µ —Ç–µ–∫—Å—Ç–∞ –∏ —Å–º–æ—Ç—Ä–µ—Ç—å, –∫–∞–∫ –º–æ–¥–µ–ª—å –ø—Ä–æ–¥–æ–ª–∂–∏—Ç –µ–≥–æ. –î–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –±—É–¥–µ–º –ø–æ–¥–∞–≤–∞—Ç—å —Ç–µ–∫—Å—Ç —Å –Ω–µ–∑–∞–∫—Ä—ã—Ç–æ–π –∫–∞–≤—ã—á–∫–æ–π, —á—Ç–æ–±—ã –≤—ã–Ω—É–¥–∏—Ç—å –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏–º–µ–Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(1)\n",
    "\n",
    "# Film recomendation\n",
    "text = \"–Ø –ª—é–±–ª—é —Å–æ–≤–µ—Ç—Å–∫–∏–µ –∫–æ–º–µ–¥–∏–∏: ‚Äú–ü—Ä–∏–∫–ª—é—á–µ–Ω–∏—è –®—É—Ä–∏–∫–∞‚Äù, ‚Äú\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)\n",
    "out = model.generate(input_ids,\n",
    "                     do_sample=True,\n",
    "                     temperature=1.4,\n",
    "                     top_k=20,\n",
    "                     top_p=0.8,\n",
    "                     max_length=46,\n",
    "                    )\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(1)\n",
    "\n",
    "# Music recomendation\n",
    "text = \"–¢–µ–º –ª—é–¥—è–º, –∫–æ–º—É –Ω—Ä–∞–≤–∏—Ç—Å—è ‚Äú–ê–ª–∏—Å–∞‚Äù, —Ç–∞–∫–∂–µ –ø–æ–Ω—Ä–∞–≤—è—Ç—Å—è –≥—Ä—É–ø–ø—ã ‚Äú\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)\n",
    "out = model.generate(input_ids,\n",
    "                     do_sample=True,\n",
    "                     temperature=1.1,\n",
    "                     top_k=10,\n",
    "                     top_p=0.7,\n",
    "                     max_length=35,\n",
    "                    )\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–†–∞—Å—Å–º–æ—Ç—Ä–∏–º –±–∏–Ω–∞—Ä–Ω—É—é –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –ø–æ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –±–µ–∑ –¥–æ–æ–±—É—á–µ–Ω–∏—è.\n",
    "\n",
    "–î–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ—Ä—É –ø–µ—Ä–ø–ª–µ–∫—Å–∏–∏. –í —Ç–µ–æ—Ä–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –ø–µ—Ä–ø–ª–µ–∫—Å–∏—è ‚Äì –º–µ—Ä–∞ —Ç–æ–≥–æ, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –≤—ã–±–æ—Ä–∫—É. –ì–æ–≤–æ—Ä—è –ø—Ä–æ—Å—Ç—ã–º —è–∑—ã–∫–æ–º, –ø–µ—Ä–ø–ª–µ–∫—Å–∏—è ‚Äì –º–µ—Ä–∞ \"—É–¥–∏–≤–ª—ë–Ω–Ω–æ—Å—Ç–∏\" –º–æ–¥–µ–ª–∏.\n",
    "\n",
    "–ù–∞ –≤—Ö–æ–¥ –º–æ–¥–µ–ª–∏ –º—ã –ø–æ–¥–∞—ë–º –∑–∞—Ç—Ä–∞–≤–∫—É –≤ —Ñ–æ—Ä–º–∞—Ç–µ <–º–µ—Ç–∫–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏> + <–æ—Ç–∑—ã–≤>. –î–∞–ª–µ–µ –º—ã —Å—á–∏—Ç–∞–µ–º –ø–µ—Ä–ø–ª–µ–∫—Å–∏—é –∫–∞–∂–¥–æ–≥–æ –∏–∑ –Ω–∏—Ö. –í—ã–±–∏—Ä–∞–µ–º –Ω–∞–∏–º–µ–Ω—å—à—É—é –ø–µ—Ä–ø–ª–µ–∫—Å–∏—é –∏–∑ –¥–≤—É—Ö –∏ –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º –º–µ—Ç–∫—É –æ—Ç–∑—ã–≤—É."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_perplexity(sentence, model, tokenizer):\n",
    "    # Add tone lable to sentence\n",
    "    sentence_positive = '–¥–æ–≤–æ–ª–µ–Ω:'+sentence\n",
    "    sentence_negative = '–Ω–µ–¥–æ–≤–æ–ª–µ–Ω:'+sentence\n",
    "    list_sent = [sentence_positive, sentence_negative]\n",
    "    ppl_values = []\n",
    "\n",
    "    for sentence in list_sent:\n",
    "      # Tokenize sentence\n",
    "      encodings = tokenizer(sentence, return_tensors='pt')\n",
    "      input_ids = encodings.input_ids.to(device)\n",
    "      # Apply model\n",
    "      with torch.no_grad():\n",
    "          outputs = model(input_ids=input_ids, labels=input_ids)\n",
    "      loss = outputs.loss\n",
    "      # Count perplexity\n",
    "      ppl = math.exp(loss.item() * input_ids.size(1))\n",
    "      ppl_values.append(ppl)\n",
    "\n",
    "    # Choose sentence with lower perplexity\n",
    "    if ppl_values[0] > ppl_values[1]:\n",
    "      return '–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π'\n",
    "    else:\n",
    "      return '–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = '—è —á—É—Ç—å –Ω–µ –∑–∞—Å–Ω—É–ª –≤–æ –≤—Ä–µ–º—è —Ñ–∏–ª—å–º–∞'\n",
    "print(f'–û—Ç–∑—ã–≤ \"{negative}\": {calculate_perplexity(negative, model, tokenizer)}')\n",
    "positive = '—Å—é–∂–µ—Ç –Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–æ –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–π'\n",
    "print(f'–û—Ç–∑—ã–≤ \"{positive}\": {calculate_perplexity(positive, model, tokenizer)}')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
