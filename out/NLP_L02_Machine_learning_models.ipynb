{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Модели машинного обучения</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Необходимость методов классического машинного обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L03/out/ml_dl_learning_plateau.png\" alt=\"alttext\" width=\"600\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует ряд причин, почему важно знать и уметь применять методы классического машинного обучения:\n",
    "\n",
    "* **Недостаточный объем данных для нейронных сетей.** При малом объеме данных методы классического машинного обучения показывают более высокую эффективность.\n",
    "* **Табличные данные.** Решения на основе классического машинного обучения часто показывают сопоставимое или более высокое качество на табличных данных.\n",
    "* **Время получения решения задачи.** Скорость экспериментов и получения хорошего бейзлайна выше, чем у решения с помощью нейронных сетей.\n",
    "*  **Интерпретируемость предсказаний.** Из методов классического машинного обучения легче получить важность каждого отдельно взятого признака.\n",
    "* **Комбинирование подходов.** В современных задачах часто используются комбинации нейронных сетей и алгоритмов классического машинного обучения. Например, можно использовать нейронные сети в качестве генераторов признаков для методов классического обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зачастую ваши задачи могут быть полностью и частично сведены к использованию методов машинного обучения. На пришлой лекции мы познакомились с основами, сегодня мы познакомимся с более мощными моделями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее познакомимся с логистической регрессией, но для лучшего понимания  вначале объясним общий подход на примере линейной регрессии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Линейная регрессия\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Регрессия** — это одна из трех базовых задач машинного обучения (классификация, регрессия, кластеризация).\n",
    "\n",
    "В задаче **регрессии** мы используем входные **признаки**, чтобы предсказать **целевые значения**. **Линейная регрессия** сводится к тому, чтобы провести “**линию наилучшего соответствия**” через набор точек данных.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Модель и ее параметры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим, у нас есть набор точек $\\{(x_i, y_i)\\}$.\n",
    "\n",
    "Цель линейной регрессии — **поиск линии, которая наилучшим образом соответствует заданным точкам**. Напомним, что общее уравнение прямой:\n",
    "\n",
    "$$\\large f(x) = w⋅x + b,$$\n",
    "\n",
    "где $w$ — характеризует наклон линии (в будущем мы будем называть значения $w$ весом, weight) а $b$ — её сдвиг по оси $y$ (bias). Таким образом, решение линейной регрессии определяет значения для $w$ и $b$ так, что $f(x)$ приближается как можно ближе к $y(x)$. Здесь $w$ и $b$ — **параметры модели**.\n",
    "\n",
    "Отобразим на графике случайные точки, расположенные в окрестности $y(x) = 3⋅x + 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "x = np.random.rand(100, 1)\n",
    "y = 2 + 3 * x + (np.random.rand(100, 1) - 0.5)\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.scatter(x, y, s=10)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим, что нам неизвестны параметры наклона и сдвига $w$ и $b$. Для их определения мы бы могли рассмотреть все возможные прямые вида $f(x) = w⋅x + b$ и выбрать среди семейства прямых такую, которая лучше всего приближает имеющиеся данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "plt.scatter(x, y, s=10)\n",
    "for w in [3.1, 2.8]:\n",
    "    for b in [1.9, 2.3]:\n",
    "        y_predicted = w * x + b\n",
    "        plt.plot(x, y_predicted, color=\"r\", alpha=0.3)\n",
    "plt.plot(x, 2 + 3 * x, color=\"g\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функция потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как выбрать параметры?\n",
    "\n",
    "**Функция потерь** позволяет вычислить меру количества ошибок. Для задачи **регрессии** такой мерой может быть **расстояние** между предсказанным значением $f(x)$ и его фактическим значением. Распространенной функцией потерь является **средняя квадратичная ошибка** (MSE). Чтобы вычислить MSE, мы просто берем все значения ошибок, считаем квадраты их длин и усредняем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть мы определяем ошибку модели на одном объекте как квадрат расстояния между предсказанием и истинным значением, а общая функция потерь будет задана выражением:\n",
    "\n",
    "$$l_i =|y_i - f(x_i)| $$\n",
    "\n",
    "$$ \\text{Loss} = \\sum l_i^2 = \\frac{1}{N} \\sum (y_i - f(x_i))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для прямой с параметрами $w=4$, $b = 2$ и $w=3$, $b = 2$ (верные значения):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_delta_line(ax, x, y, w, b, color=\"r\"):\n",
    "    y_predicted = w * x + b\n",
    "    # line\n",
    "    ax.plot(x, y_predicted, color=color, alpha=0.5, label=f\"f(x)={w}x+{b}\")\n",
    "    # delta\n",
    "    for x_i, y_i, f_x in zip(x, y, y_predicted):\n",
    "        ax.vlines(x=x_i, ymin=min(f_x, y_i), ymax=max(f_x, y_i), ls=\"--\", alpha=0.3)\n",
    "    # MSE\n",
    "    loss = np.sum((y - (w * x + b)) ** 2) / (len(x))\n",
    "    ax.set_title(f\"MSE = {loss:.3f}\")\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(11, 4))\n",
    "\n",
    "# plot x_i y_i (dots)\n",
    "for ax in axs:\n",
    "    ax.scatter(x, y, s=10)\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([2, 6])\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "\n",
    "plot_delta_line(axs[0], x, y, w=4, b=2, color=\"r\")\n",
    "plot_delta_line(axs[1], x, y, w=3, b=2, color=\"g\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача **поиска оптимальных параметров** модели сводится к задаче **поиска минимума функции потерь**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Поиск локального минимума"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как будет выглядеть ландшафт функции потерь для нашей задачи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.arange(-10, 30, 1)\n",
    "b = np.arange(-10, 10, 1)\n",
    "\n",
    "w, b = np.meshgrid(w, b)\n",
    "\n",
    "loss = np.zeros_like(w)\n",
    "for i in range(w.shape[0]):\n",
    "    for j in range(w.shape[1]):\n",
    "        loss[i, j] = np.sum((y - (w[i, j] * x + b[i, j])) ** 2) / (len(x))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "surf = ax.plot_surface(w, b, loss, cmap=plt.cm.RdYlGn_r, alpha=0.5)\n",
    "\n",
    "ax.contourf(w, b, loss, zdir=\"z\", offset=-1, cmap=\"RdYlGn_r\", alpha=0.5)\n",
    "ax.set_zlim(0, 20)\n",
    "\n",
    "ax.set_xlabel(\"w\")\n",
    "ax.set_ylabel(\"b\")\n",
    "ax.set_title(\"MSE\")\n",
    "\n",
    "fig.colorbar(surf, location=\"left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимым (но недостаточным) условием локального минимума дифференцируемой функции является равенство нулю частных производных:\n",
    "\n",
    "$$\t\\begin{equation*}\n",
    " \\begin{cases}\n",
    "   \\displaystyle\\frac{\\partial \\text{Loss}}{\\partial w}=0,\n",
    "   \\\\\n",
    "   \\displaystyle\\frac{\\partial \\text{Loss}}{\\partial b}=0.\n",
    " \\end{cases}\n",
    "\\end{equation*} $$\n",
    "\n",
    "Т.к. MSE для линейной регрессии — полином второй степени относительно $w$ и $b$, а полином второй степени не может иметь больше одного экстремума, то локальный минимум будет глобальным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переходя к задаче множества переменных $x_1,x_2,...,x_n$ (признаков), получаем:\n",
    "\n",
    "$$f(x) = w_1x_1 + w_2x_2 + ... + w_nx_n$$\n",
    "\n",
    "Переменная $f$ может принимать значения в любом диапазоне. Чтобы  решать задачу бинарной классификации, т.е. получать вероятность отнесения объекта к классу 0 или 1, нужно привести $f(x)$ в диапазон от 0 до 1.\n",
    "\n",
    "Если классов более двух, задачу можно свести  к бинарной, если для каждого класса формулировать её как \"один против всех\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Переход к вероятности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае бинарной классификации используется сигмоида. Если получившееся значение больше 0.5, то объект относится к классу 1 (положительному), иначе — к классу 0 (отрицательному).\n",
    "\n",
    "$$ σ(z(x))=\\frac{1}{1+e^{-z(x)}}$$\n",
    "\n",
    "Запишем формулу сигмоиды, используя методы из библиотеки NumPy:\n",
    "\n",
    "- `np.exp(x)`  для подсчета $e^x$\n",
    "- `np.arange()` для указания диапазона возможных значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "x = np.arange(-10, 10, 0.5)\n",
    "z = 1/(1 + np.exp(-x))\n",
    "\n",
    "plt.plot(x, z)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Sigmoid(X)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для многоклассовой классификации используется функция активации softmax.  Вероятность $i$-го класса при наличии $K$ классов рассчитывается следующим образом:\n",
    "\n",
    "$$\\text{softmax}(z_{i}) = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}$$\n",
    "$$i=1,...,K$$\n",
    "\n",
    "Для каждого объекта выбирается класс с наибольшей вероятностью.\n",
    "\n",
    "Запишем формулу для функции софтмакс. Для подсчета суммы используем метод `np.sum()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "x = np.array([2.6, 3.2, 0.5])\n",
    "print(softmax(x))\n",
    "print(np.sum(softmax(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо определить оптимальные параметры $w_1,...,w_n$, при которых различие между предсказанными и истинными значениями будет минимально. Рассчитать величину ошибки позволяет **функция потерь**, которую необходимо минимизировать.\n",
    "\n",
    "Мы бы хотели такую функцию, которая очень сильно штрафует за большие отступления от истинного ответа и не сильно за небольшие ошибки.\n",
    "\n",
    "<center><img src =\"https://i.ibb.co/FKsSgWL/cross-entropy.png\" width=\"500\" ></center>\n",
    "\n",
    "Этим задача отвечает функция кросс-энтропии ($L_{CE}$ — *Cross Entropy Loss*):\n",
    "\n",
    "$$L_{CE}(\\hat y,y) = -\\sum^{K}_{i=1}y_i \\cdot log(\\hat y_i),$$\n",
    "\n",
    "где $i$ — номер класса, $y$ — истинный ответ, $\\hat y$ — предсказанный ответ. Если истинный ответ 0, а предсказывается 1 — loss уходит в бесконечность. Однако, чем ближе мы приближаемся к 0 — тем существенно меньше штраф. В принципе, мы могли бы изобрести и другую функцию.\n",
    "\n",
    "В случае бинарной классификации имеет $K=2$, $y=0$ или $y=1$ ($L_{BCE}$ — *Binary Cross Entropy Loss*):\n",
    "\n",
    "$$L_{BCE}(\\hat y,y) = -(y \\cdot log(\\hat y)+(1-y) \\cdot log(1-\\hat y))$$\n",
    "\n",
    "\n",
    "В случае многоклассовой классификации $K>2$. Истинный ответ $y$ — вектор длины $K$, где элемент вектора $y_c=1$, если $c$ — истинный класс, остальные элементы равны $0$.\n",
    "\n",
    "Например, $K=3$ (классы $0,1,2$). Объект относится к классу $2$. Тогда $y = (0, 0, 1)$, $c=2$, $y_2 = 1$.\n",
    "\n",
    "Модель предсказывает вектор $\\hat y$ длины $K$. Функция кросс-энтропии имеет вид:\n",
    "\n",
    "$$L_{CE}(\\hat y,y) = -\\log \\hat y_c$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод градиентного спуска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача поиска оптимальных параметров модели сводится к задаче **поиска минимума функции потерь**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Точки минимума и максимума функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как найти минимум функции в простом случае?\n",
    "\n",
    "- Найти производную функции.\n",
    "- Найти значения, при которых производная равна нулю.\n",
    "- Определить знаки производной. Когда функция возрастает, то производная положительна. Когда функция убывает, то производная отрицательна.\n",
    "- Определить точки минимума и максимума. Если функция возрастала и в определенной точке начала убывать, то это точка максимума. Если функция убывала и в некоторой точке начала возрастать, то это точка минимума."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L01/function.png\" width=\"700\" ></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Таблица производных:}$$\n",
    "\\begin{array}{|с|c|c|} \\hline\n",
    " f(x) \\text{ (функция)} & f'(x)  \\text{ (производная)}\\\\ \\hline\n",
    "с \\text{ (константа)} & 0 \\\\ \\hline\n",
    "cx & c \\\\ \\hline\n",
    "x^c & cx^{c-1} \\\\ \\hline\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим на примере функции $f(x)=x^2 + x + 3$.\n",
    "\n",
    "Она зависит от одной переменной $x$.\n",
    "\n",
    "Найдем производную $f'(x^2 - x + 3)$.\n",
    "- $f'(x^2 - x + 3) = 2x - 1$\n",
    "- $f'(x^2 - x + 3) = 0$ при $x =0.5$\n",
    "-  $f'(x^2 - x + 3) < 0 $ при $x < 0.5$, $f'(x^2 - x + 3) > 0 $ при $x > 0.5$\n",
    "- $f(x)$ убывает при $x < 0.5$, $f(x)$ возрастает при $x > 0.5$\n",
    "- $x = 0.5$ — точка минимума"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.linspace(-2, 3, 100)\n",
    "y = x**2 - x + 3\n",
    "plt.figure()\n",
    "plt.title(\"$x^2 - x + 3$\")\n",
    "plt.plot(x, y)\n",
    "plt.xticks(np.arange(-2, 3.5, step=0.5))\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.plot(0.5,2.75, marker='.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Частная производная и градиент функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":Если функция зависит от нескольких переменных, можно говорить о частной производной, когда все остальные переменные, кроме интересующей нас, становятся константами. Производная функции $f$ от переменной $x_1$: $\\large\\frac{\\partial f}{\\partial x_1}$.\n",
    "\n",
    "**Градиент** в математическом анализе — это вектор, указывающий на направление максимального роста функции в заданной точке. Вектор-градиент состоит из частных производных функции от каждой из ее переменных $(x_{1},...,x_{n})$:\n",
    "\n",
    "$$ \\nabla f(\\vec{x}) = \\begin{bmatrix}\n",
    "\\displaystyle\\frac{\\partial f}{\\partial x_1}\\\\\n",
    "\\displaystyle\\frac{\\partial f}{\\partial x_2}\\\\\n",
    "...\\\\\n",
    "\\displaystyle\\frac{\\partial f}{\\partial x_n}\\\\\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим функцию $f(x,y,z)=2xy^3+3z^2$. Найдем градиент функции для переменных $x,y,z$.\n",
    "\n",
    "$$\n",
    "\\nabla f(x, y, z)=\\begin{bmatrix}\n",
    "\\displaystyle\\frac{\\partial f}{\\partial x}\\\\\n",
    "\\displaystyle\\frac{\\partial f}{\\partial y}\\\\\n",
    "\\displaystyle\\frac{\\partial f}{\\partial z}\\\\\n",
    "\\end{bmatrix}\n",
    "=\\begin{bmatrix}\n",
    "2y^3\\\\\n",
    "\\\\\n",
    "6xy^2\\\\\n",
    "\\\\\n",
    "6z\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для минимизации ошибки нужен антиградиент, который показывает направление скорейшего убывания функции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Градиентный спуск и скорость обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Градиентный спуск** — это некоторый алгоритм, который помогает нам итеративно шагами перемещаться по функции и в конце концов оказаться в ее минимуме. Мы не напрямую ищем точку, где производная равна нулю, а некоторыми манипуляциями передвигаемся вниз по функции. Градиентный спуск выбирает случайную точку, находит направление самого быстрого убывания функции и двигается до ближайшего минимума вдоль этого направления.\n",
    "\n",
    "Важно настроить размер шага или **скорость обучения** $η$ — некоторый коэффициент, как правило, небольшой, который не позволяет нам двигаться слишком быстро. У нас есть точка, в которую мы хотим попасть. Если мы сделаем слишком большой шаг, то мы ее перескочим (график справа), поэтому надо подобрать шаг, который не позволит ее перескочить, но в то же время такой, чтобы тот же процесс не шел слишком медленно (как на графике слева)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet_NLP-content/L01/out/learning_rate_optimal_value.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если на каком-то этапе разность между старой точкой (до шага) и новой снижается ниже предела, считается, что минимум найден, алгоритм завершен."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомним, что функция потерь $\\mathcal{L}(y, \\hat{y})$ зависит от нескольких переменных: весов $\\overline{w} = (w_1, \\cdots, w_m)$ и сдвига $b$.\n",
    "\n",
    "Следовательно, мы будем использовать несколько частных производных:\n",
    "\n",
    "- частные производные функции потерь от весов $w_1,\\cdots,w_m$: $\\large\\frac{\\partial \\mathcal{L}(y, \\hat{y})}{\\partial w_1}, \\cdots, \\frac{\\partial \\mathcal{L}(y, \\hat{y})}{\\partial w_m}$\n",
    "- частная производная функции потерь от сдвига $b$: $\\large\\frac{\\partial \\mathcal{L}(y, \\hat{y})}{\\partial b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После подсчета частных производных нам необходимо обновить значения весов и сдвига.\n",
    "\n",
    "$$w_i = w_i-η\\frac{\\partial \\mathcal{L}(y, \\hat{y})}{\\partial w_i}$$\n",
    "\n",
    "$$b = b-η\\frac{\\partial \\mathcal{L}(y, \\hat{y})}{\\partial b}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Распознавание эмоций"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим задачу многоклассовой классификации на примере распознавания эмоций.\n",
    "\n",
    "Распознавание эмоций ≠ анализ тональности. Анализ тональности — выявление оценки авторов по отношению к объектам, речь о которых идёт в тексте. Она может быть позитивной, негативной или нейтральной. Эмоции могут включать не только радость или грусть, но ужас, гнев, удивление, отвращение и т.п."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L01/sentiment_emotion.png\" width=\"700\" ></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чаще всего при обработке текстов стоит задача распознать 6 базовых эмоций: счастье (happiness), печаль (sadness), страх (fear), гнев (anger), отвращение (disgust), удивление (surprise). Данная классификация введена Полом Экманом в книге [[book] 📚 Basic emotions](https://www.paulekman.com/wp-content/uploads/2013/07/Basic-Emotions.pdf). Утверждается, что люди всех культур испытывают их и могут распознавать в других людях. Для каждой базовой эмоции есть соответствующее, безошибочно опознаваемое выражение лица."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L01/basic_emotions.png\" width=\"1100\" ></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка и подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать русскоязычный набор данных, представленный в работе [[paper] 🎓 Emotion classification in Russian: feature engineering and analysis](https://link.springer.com/chapter/10.1007/978-3-030-72610-2_10).\n",
    "\n",
    "Он размечен по **5 классам эмоций**:\n",
    "- радость (joy),\n",
    "- печаль (sadness),\n",
    "- злость (anger),\n",
    "- неуверенность (uncertainty),\n",
    "- нейтральность (neutrality).\n",
    "\n",
    "Из классификации Экмана удалена категория отвращения из-за отсутствия соответствующих данных. Категории страха и удивления  объединены в одну категорию неопределенности из-за сходства способов, которыми они выражаются.\n",
    "\n",
    "Загрузим набор данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/datasets/Emotion_Classification_Russian.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "emo = pd.read_csv('Emotion_Classification_Russian.csv')\n",
    "emo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://i.ibb.co/Jr5SprX/cleandata.png\" width=\"700\" ></center>\n",
    "\n",
    "Вначале всех текстовых задач идёт очистка данных от \"мусора\": номеро встраниц и названий журналов, преобразование или удаление таблиц и картинок и прочее приведение текста в \"чистый\" вид.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо метки класса (label) и текста (text), присутствует столбец с лемматизированными предложениями (lemmatized). Лемматизация — процесс приведения словоформы к лемме, то есть её нормальной (словарной) форме.\n",
    "\n",
    "<center><img src =\"https://i.ibb.co/wB19yJX/lemma.webp\" width=\"500\" ></center>\n",
    "\n",
    "В русском языке это следующие морфологические формы:\n",
    "\n",
    "- для существительных — именительный падеж, единственное число\n",
    "  - кошками → кошка;\n",
    "- для прилагательных — именительный падеж, единственное число, мужской род\n",
    "  - красивых → красивый;\n",
    "- для глаголов, причастий, деепричастий — глагол в инфинитиве (неопределённой форме) несовершенного вида\n",
    "  - бежал → бежать.\n",
    "\n",
    "Лемматизация необходима для языков с богатой морфологией, чтобы не расширять размер словаря за счет слов, у которых одинаковый смысл, но разные морфологические формы.\n",
    "\n",
    "В рассматриваемом наборе данных лемматизация проведена с помощью морфологического анализатора [[git] 🐾 RNNmorph](https://github.com/IlyaGusev/rnnmorph). Он использует реккурентную нейронную сеть для определения части речи и морфологических признаков слова с учетом контекста.\n",
    "\n",
    "Рассмотрим пример его применения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/IlyaGusev/rnnmorph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnnmorph.predictor import RNNMorphPredictor\n",
    "predictor = RNNMorphPredictor(language=\"ru\")\n",
    "\n",
    "forms = predictor.predict([\"мама\", \"поймала\", \"мышь\"])\n",
    "print(f'Часть речи слова \"мама\": {forms[0].pos}')\n",
    "print(f'Лемма слова \"поймала\": {forms[1].normal_form}')\n",
    "print(f'Морфологический анализ слова \"мышь\": {forms[2].tag}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для удобства заменим словесные обозначения классов в датасете на числовые. Метки присваиваются в алфавитном порядке:\n",
    "- **a**nger → 0\n",
    "- **j**oy → 1\n",
    "- **n**eutrality →  2\n",
    "- **s**adness → 3\n",
    "- **u**ncertainty → 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo['num_label'] = emo['label'].astype('category').cat.codes\n",
    "emo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Узнаем количество данных в датасете и их распределение по классам эмоций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.pie(emo['label'].value_counts(), labels=emo['label'].unique(), autopct='%.1f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведем уже знакомые операции для подготовки данных:\n",
    "- запишем в отдельные переменные лемматизированные тексты `X` и метки классов `y`;\n",
    "- разделим данные на обучающую и тестовую выборку;\n",
    "- осуществим векторизацию обучающей и тестовой выборок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X, y = emo['lemmatized'], emo['num_label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "vect = TfidfVectorizer(min_df=0.0002, max_df=0.2)\n",
    "X_train_vect = vect.fit_transform(X_train)\n",
    "X_test_vect = vect.transform(X_test)\n",
    "X_train_vect, X_test_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение и анализ модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель логистической регрессии (при дефолтном параметре `max_iter=100` модель не сходится; чтобы добиться сходимости алгоритма, установим большее количество итераций)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(random_state=42, max_iter=300) # model initialization\n",
    "logreg.fit(X_train_vect, y_train) # trainig\n",
    "y_logreg = logreg.predict(X_test_vect) # predicting labels\n",
    "y_logreg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты классификации можно отразить в виде матрицы ошибок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def show_confusion_matrix(confusion_matrix):\n",
    "  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Greens\")\n",
    "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
    "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
    "  plt.ylabel('True emotion')\n",
    "  plt.xlabel('Predicted emotion')\n",
    "\n",
    "class_names = ['anger', 'joy', 'neutrality', 'sadness', 'uncertainty']\n",
    "cm = confusion_matrix(y_test, y_logreg)\n",
    "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "show_confusion_matrix(df_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метрики для каждого класса, а также усредненные метрики представлены в отчете о классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['anger', 'joy', 'neutrality', 'sadness', 'uncertainty']\n",
    "print(classification_report(y_test, y_logreg, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого класса были посчитаны свои коэффициенты регрессии. Они представляют матрицу $K \\times n$, где $K$ — количество классов, $n$ — количество признаков (слов)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficient_matrix = logreg.coef_\n",
    "print(coefficient_matrix)\n",
    "print(coefficient_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Каждый из коэффициентов регрессии описывает размер вклада соответствующего признака.\n",
    " - Положительный коэффициент — признак повышает вероятность принадлежности к классу, отрицательный коэффициент — признак уменьшает вероятность.\n",
    " - Большой коэффициент — признак существенно влияет на вероятность принадлежности к классу, почти нулевой коэффициент — признак имеет небольшое влияние на вероятность результата.\n",
    "\n",
    "Выведем признаки с самыми большими коэффициентами для каждого класса и получим наиболее характерные слова для каждой эмоции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Злость', 'Радость', 'Нейтральность', 'Печаль', 'Неуверенность']\n",
    "coefficient_matrix = logreg.coef_\n",
    "\n",
    "for i in range(coefficient_matrix.shape[0]):\n",
    "\n",
    "    print(f\"\\n{labels[i]}:\")\n",
    "\n",
    "    feature_names = vect.get_feature_names_out() # word features\n",
    "    order = coefficient_matrix[i].argsort() # ascending order for coefficients\n",
    "    class_coefficients = coefficient_matrix[i][order][::-1][:5] # sort and extract top-5 coefficients\n",
    "    feature_names = feature_names[order][::-1][:5] # words with the highest coefficients\n",
    "\n",
    "    for feature, coefficient in zip(feature_names, class_coefficients):\n",
    "      print(feature, coefficient.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Литература</font>\n",
    "\n",
    "<font size=\"5\">Инструменты:</font>\n",
    "\n",
    "- [[doc] 🛠️ Scikit-learn](https://scikit-learn.org/stable/) — ML алгоритмы\n",
    "- [[doc] 🛠️ NumPy](https://numpy.org/) — массивы и математические функции\n",
    "- [[doc] 🛠️ Pandas](https://pandas.pydata.org/) — табличные данные\n",
    "- [[doc] 🛠️ Matplotlib](https://matplotlib.org/) — визуализация\n",
    "- [[doc] 🛠️ NLTK](https://www.nltk.org/) — токенизация, словари стоп-слов\n",
    "- [[git] 🐾 RNNmorph](https://github.com/IlyaGusev/rnnmorph) — морфологический парсер для русского и английского языков\n",
    "- [[doc] 🛠️ UDPipe](https://ufal.mff.cuni.cz/udpipe) — морфологический и синтаксический парсер для различных языков\n",
    "\n",
    "<font size=\"5\">Методы и алгоритмы:</font>\n",
    "\n",
    "- [[wiki] 📚 Мешок слов](https://ru.wikipedia.org/wiki/Мешок_слов)\n",
    "- [[wiki] 📚 TF-IDF](https://ru.wikipedia.org/wiki/TF-IDF)\n",
    "- [[wiki] 📚 Наивный байесовский классификатор](https://ru.wikipedia.org/wiki/Наивный_байесовский_классификатор)\n",
    "- [[wiki] 📚 Логистическая регрессия](https://ru.wikipedia.org/wiki/Логистическая_регрессия)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
